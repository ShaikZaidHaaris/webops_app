Back to 0,,Story of how my company almost died 6 times before things working out a little bitOh! I actually redirected this entire blog to our Company's blog traffic to drive more SEO traffic there üß†Cuz, technicallyyyyyyy, I am the company and the company is me.Enjoy the full blog here üòÅ - https://recontact.substack.com/p/back-to-0
How I learnt to do sales, starting out as a coder,Breaking some personal barriers to actually be good at telling the world about the problem we're solving and the product we're building,Checkout the blog on our company website - link
Baking customer obsession into everyday decisions,Simple ways you can turn your project into a customer-centric product.,I used to just get so confused for understanding how do I actually incorporate a culture of powerful customer obsession that big companies like Amazon, Airbnb, Stripe speak so strongly about.What does it mean? How does that change your workflow? What small optimizations can you do that can ensure you become such a company?Yeah, you don't need a dedicated team for that - I want to list simple things you can do as a solopreneur in a garage as well.Here's 4 learnings that I've had, from my current and previous startup, on how to build a reflex muscle for customer obsession, from 120+ overall interviews I've done.0] an intro to the topicCustomer-centricity means you just know so much about the customer, that when you take decisions about product, you can tell exactly how your customers will react to it.It's important because it builds very great relationship with them and they will stand by your team and product even when things don't work out that well.1] 5hrs of talktime per weekIf you're having less than 5hrs of talktime with customers, that means you either know everything about them or you need to find a different set of people to talk to, who're willing to spend time to discuss the problem. Easy ways to get this:Start by just asking your friends for people in their companies or families for a call and ask them to connect further. If you don't know anyone, just cold reachout your competitors' customers on LinkedinJust ask for 25min calls. They seem shorter than 30 for some reason so I've seen people say yes and then give you the extra 5.Ask them to not refer to "anyone", but "1 name" that they can think of - this is fast, and has 100% success rate.Setup a communication channel on a platform they're most comfortable on. (Whatsapp, Telegram, Discord etc.) where you can just keep talking to them as friends2] Less than a day of turnaround timeWhen you're a young company, you can provide near instanteneous customer support and that goes a long way in turning early adopters into evangelists. This is simple - just have them as priority notifications and swear you will reply something the moment you see a popup. Direct lines with founders who ping back quick feel very nice. 3] Organizing their feedbackThis is the most important part. I'm sharing my company's template for this. It's just colorful stickies you can move around, but captures certain key snippets across multiple customers really well.Just pick key sentences they say, tag and color them well - really helps a lot. Here's an exampleAt our company Recontact, we're helping people be amazing at building long-term relationships in professional and personal settingsUsing this strategy, we figured out (check video) that "Maintenance" (i.e. keeping in touch with people) - was the most talked about point in our problem statement - because it had more stickies. And that realization saved us from spending 2 weeks worth of time in making "Search" in our app more efficient - because that's not what our customers cared about.So yeah, that coloring and legend-making you did as a kid - well that's one of the key components in making a customer-centric business.4] Building a muscle for the infoRevise, revise, revise. This, I learned from Apple CEO Tim Cook - who starts his day with reading random customer emails. Well, you can always read these stickies. Look into a simple analytics dashboard you can create for your app. - template linkYou won't see it instantly, but, over time, a decision-making muscle gets built (very much like driving, music, fashion-sense) - that is very grounded in qualitative and quantitative analysis of customer feedback. Listen to even the smallest details - you might not remember instantly, but then you'll be surprised how often you'll randomly remember a customer fact, when making a decision related to it.In closingYou can be crazy passionate about this too - write customer snippets on your wall, listen to customer interviews at the gym, be thoughtful with delighting gifts, send personalized notes when their kids graduate etc. etc. etc. Bottom line - Be close to the customer, even though the product will not work out, your company will. Business runs on trust and mutual respect.
Social Capital,Superfast read on 6 ways to get more people to like you and make them strong connections.,"Social Capital" is simply the "network of relationships and acquantainces" that a person has developed over life".We all hear fancy ass quotes about "making strong professional relations", "staying in touch with friends" and "your network is your net-worth". Here's a quick rundown of the best-selling way to build such social capital, since the past 80 years and beyond! Basically, become more likeable.These are my key pointers from the book "How to win friends and influence people" by Dale Carnegie. Each topic is its own tweetstorm - so kick back and read these wisdom snippets to help you build a strong people network.With that let's dive in by understanding 6 ways to be a person who's good at making friends.6 ways to make people like youüí°1. Become genuinely interested in other peopleA dog makes his living by giving you nothing but loveYou can make more friends in 2 months by becoming interested in other people than you can, in 2 years, by trying to get other people interested in youWhen you see a group photograph that you're in, whose picture do you look for first ?It is the individual who is not interested in his/her fellow men/women who has the greatest difficulties in life and provides the greatest injury to others. It is from such individuals that all human failures springYou have to be interested in people if you wanna be a successful writer of storiesCan win the attention and time and cooperation of even the most sought-after people by becoming genuinely interested in themeg : To call someone for an interview, write a letter first enclosing a list of questions that you except them to answer about themselves and their method of workTo find out someone's birthday ‚áí Begin by asking whether he/she believes the date of anyone's birth has anything to do with character or dispositionA show of interest, like every other human emotion, must be sincereüí°2. SmileA smile says - "I like you. You make me happy. I am glad to see you". That is why dogs make such a hitA real smile, a heartwarming smile, a smile that comes from within, the kind of smile that will bring a good price in the marketplacePeople who smile, tend to manage, teach and sell more effectivelySmile comes through in your voiceHard work alone is the magic ¬†key that will unlock the door to our desires. People have succeeded because they had a rip-roaring good time conducting their business. As the fun becomes work, people change ‚áí lose joy in it, leading business to grow dull and failHappiness doesn't depend on outward conditions. It depends on inner conditionsIt isn't what you have or who you are or where you are or what you are doing that makes you happy or unhappy. It is what you think about it. Different mental attitudeAction seems to follow feeling but really, action and feeling go together and by regulating the action, which is under the more direct control of the will, we can indirectly regulate the feeling which is notThus, the sovereign voluntary path to cheerfulness is to sit up cheerfully and act and speak as if cheerfulness were already thereMost folks are about as happy as they make up our minds to bePicture in your mind, the able, earnest, useful person you desire to be and the thought you hold is hourly transforming you into that particular individualA man/woman without a smiling face must not open a shop. It costs nothing but creates muchüí°3. A person's name is to that person, the sweetest and most important sound in any languageThe average person is more interested in his or her own name in all the other names on earth put togetherRemember that name and call it easily, and you have paid a subtle and very effective compliment. But forget it or misspell it and you have placed yourself at a sharp disadvantageThe executive who can't remember names can't remember a significant part of his/ her business and is operating on quicksandTake the time and energy necessary to concentrate and repeat and fix names indelibly in their mindsMost important ways of gaining good will is by remembering names and making people feel importantTo recall a voter's name is statesmanship. To forget it is oblivionTo repeat the name several times during conversation and try to associate it with the person's features, expression and general appearanceGood manners are made up of petty sacrificesMagic contained in a name should be realized as the single item, wholly and completely owned by the person with whom we are dealing and nobody else.The name sets the individual apart; it makes him/ her unique among all othersThe info we are imparting or the request we are making takes on special importance when we approach the situation with the name of the individual.From the waitress to the senior executive herself, the name will work magic as we deal with others.üí°4. Be a good listener. Encourage others to talk about themselvesFew human beings are proof against the implied flattery of rapt attentionDale Carnegie on listening and responding ‚áí Hearty in my approbation and lavish in my praiseGood conversationalists are merely good listeners who encourage a person to talk"Yes, and" - pay attention and show indications of it from time to time with approval or disapproval.Important personages crave a good listener but ordinary folk do too."Many a person call a doctor when all they want is an audience"If you wanna know how to PREVENT making people shun you and laugh at you behind your back and even despise you, here is the recipe :Always listen to anyone for longDon't talk incessantly about yourselfIf you have an idea while the other person is talking, wait for him/ her to finishPeople who talk of themselves think only of themselvesThose people who think of themselves are hopelessly uneducated, no matter how instructed they may bePeople you might be talking to are 100 times more interested in themselves and their wants and problems than they are in you and your problemsTherefore, if you aspire to be a good conversationalist, be an attentive listenerüí°5. Talk in terms of the other person's interestsTheodore Roosevelt used to sit up late the night before, reading up on the subject in which he knew the guest is particularly interested inRoad to a person's heart is to talk about the things he/ she treasures mostThe reward is an enlargement of life each time you speak to someoneüí°6. Make the other person feel important ‚Äî and do it sincerelyIf we are contemptibly selfish that we can't radiate a little happiness and pass on a bit of honest appreciation without trying to get somethin out of the other person in return ‚Äî we shall meet with the failure that we so richly deserveAttain the feeling that you have done something for him/ her without his/ her being able to do anything in return for youThe deepest principle in human nature is the craving to be appreciated ~ William JamesDo unto others and you would have others do unto youLittle phrases such as "I'm sorry to trouble you", "Would you be so kind as to ‚Äî?", "Won't you please ?", "Would you mind", "Thank you ‚Äî" little courtesies like these, oil the cogs of the monotonous grind of everyday and, incidentally, they are the hallmark of good breedingThe power of honest appreciation is stupendous as everyone considers themselves impEvery person I meet is my superior in some way ¬†~EmersonTalk to people about themselves and they will listen for hoursIn conclusionBe interested, smile, remember their name, listen more, talk about their interests and make them feel important. You nail that and you find yourself as a likeable, good friend who is connected to a lot of different kinds of people very well.
Interactive VR app 101,Building a real-world interviewer who gives good feedback,This is going to be a super fast guide for developers to leverage platform development tools with VR and build cool apps. Starting with just a laptop and a Meta Quest in your hand.Here's what the final thing looks like:CodeHere's the GitHub Repo containing all the code - https://github.com/CMU-SV-Ronith/sem_3piIntroduction to VR and AI-Generated NPCsVirtual Reality (VR) is transforming how we interact with digital environments, offering immersive experiences that blur the lines between the virtual and real world. One of the most exciting advancements in VR is the integration of Artificial Intelligence (AI), especially in creating Non-Player Characters (NPCs) that can interact with users in more human-like ways. This blog post delves into the creation of an interactive AI interviewer app in VR, highlighting the technological synergy of VR and AI.Choosing the Right EngineWhen embarking on VR app development, selecting the right engine is crucial. Two prominent engines are Unity and Unreal. After extensive research and testing, Unity emerged as the preferable choice for several reasons:Ease of Learning and Accessibility: Unity uses C#, known for its simpler syntax and automatic memory management. This choice makes Unity accessible, especially for beginners and indie developers, unlike Unreal Engine's complex C++.Asset Store and Community Support: Unity boasts an expansive Asset Store, providing numerous resources that speed up development. Coupled with an active community, developers have access to extensive support and shared knowledge.Versatility in 2D and 3D Development: Unity's proficiency in both 2D and 3D development offers flexibility, allowing developers to work on various project types without switching engines.Platform Support and Integration: Unity's broad platform support, including PCs, consoles, mobile devices, and VR systems, is vital for developers targeting diverse audiences.Strong Documentation and Educational Resources: Unity‚Äôs comprehensive documentation and learning resources are invaluable for both beginners and experienced developers.Making API Calls in C# with Code-WalkthroughWe code in C# in Unity. Here's a simple video on integrating Unity App with C# code that you write - https://www.youtube.com/watch?v=lgUIx75fJ_EThis was used for the Speech Analysis and Text Analysis functions.To integrate AI into our VR app, we need to make API calls. This is achieved through UnityEngine.Networking, setting up coroutines for HTTP(s) communications. The process involves getting and posting information, waiting for responses, and updating the UI to reflect changes. We utilize AWS for backend services and the Dolby Media Speech Analytics API for speech analysis.The AnalyzeText coroutine sends a user's transcript to our backend for processing, while GetAnalysisResults retrieves the analysis. The AnalyseSpeech coroutine sends audio files for speech analysis, and UploadFileToS3 handles the uploading of files to AWS S3.References of setting up AWS in app - link, video linkAPI connectors in the appprivate IEnumerator AnalyzeText(string transcript)
        {
            var requestJson = new
            {
                prompt = transcript
            };

            var request = new UnityWebRequest(baseUrl + "/analyzeText", "POST")
            {
                uploadHandler = new UploadHandlerRaw(System.Text.Encoding.UTF8.GetBytes(JsonUtility.ToJson(requestJson))),
                downloadHandler = new DownloadHandlerBuffer()
            };

            request.SetRequestHeader("Content-Type", "application/json");

            yield return request.SendWebRequest();

            if (request.isNetworkError || request.isHttpError)
            {
                Debug.LogError("Error: " + request.error);
            }
            else
            {
                string responseText = request.downloadHandler.text;
                // Display responseText in the UI
            }
        }
        private IEnumerator GetAnalysisResults()
        {
            var request = UnityWebRequest.Get(baseUrl + "/getResults");

            yield return request.SendWebRequest();

            if (request.isNetworkError || request.isHttpError)
            {
                Debug.LogError("Error: " + request.error);
            }
            else
            {
                var responseJson = JsonUtility.FromJson<AnalysisResponse>(request.downloadHandler.text);
                var analysisObject = new
                {
                    Loudness = responseJson.processed_region.audio.speech.details[0].loudness.measured,
                    Confidence = responseJson.processed_region.audio.speech.details[0].sections[0].confidence,
                    Quality = responseJson.processed_region.audio.speech.details[0].quality_score,
                    LongestMonologue = responseJson.processed_region.audio.speech.details[0].longest_monologue
                };
                // Display analysisObject values in the UI
            }
        }
        private IEnumerator AnalyseSpeech(string fileUrl)
        {
            var requestJson = new
            {
                input = fileUrl
            };

            var request = new UnityWebRequest(baseUrl + "/analyseSpeech", "POST")
            {
                uploadHandler = new UploadHandlerRaw(System.Text.Encoding.UTF8.GetBytes(JsonUtility.ToJson(requestJson))),
                downloadHandler = new DownloadHandlerBuffer()
            };

            request.SetRequestHeader("Content-Type", "application/json");
            request.SetRequestHeader("x-api-key", "YOUR_API_KEY"); // Replace with your actual API key

            yield return request.SendWebRequest();

            if (request.isNetworkError || request.isHttpError)
            {
                Debug.LogError("Error: " + request.error);
            }
            else
            {
                string jobId = request.downloadHandler.text;
                // Store jobId for later use or handle it as needed
            }
        }



        private async Task<string> UploadFileToS3(string filePath, string bucketName)
        {
            // Hardcoded credentials (not recommended for production)
            string awsAccessKeyId = "AK";
            string awsSecretAccessKey = "f";
            AWSCredentials credentials = new BasicAWSCredentials(awsAccessKeyId, awsSecretAccessKey);
            AmazonS3Client s3Client = new AmazonS3Client(credentials, Amazon.RegionEndpoint.USEast1); // Initialize with your AWS credentials

            try
            {
                // Create a PutObject request
                PutObjectRequest putRequest = new PutObjectRequest
                {
                    BucketName = bucketName,
                    FilePath = filePath,
                    Key = Path.GetFileName(filePath),
                    CannedACL = S3CannedACL.PublicRead // Set the file to be publicly accessible
                };

                PutObjectResponse response = await s3Client.PutObjectAsync(putRequest);

                if (response.HttpStatusCode == System.Net.HttpStatusCode.OK)
                {
                    string fileUrl = $"https://{bucketName}.s3.amazonaws.com/{Path.GetFileName(filePath)}";
                    Debug.Log("File uploaded successfully. URL: " + fileUrl);
                    return fileUrl; // Return the URL
                }
                else
                {
                    Debug.LogError("Failed to upload file. HTTP Status Code: " + response.HttpStatusCode);
                    return null; // Return null if upload failed
                }
            }
            catch (AmazonS3Exception e)
            {
                Debug.LogError("Error encountered on server. Message:'" + e.Message + "'");
                return null; // Return null on exception
            }
            catch (Exception e)
            {
                Debug.LogError("Unknown encountered on server. Message:'" + e.Message + "'");
                return null; // Return null on exception
            }
        }

    }Function for stringing all API calls togetherprivate IEnumerator UploadAndAnalyze(string localPath)
        {
            // Upload file and wait for the result
            Task<string> uploadTask = UploadFileToS3(localPath, "recontact-temp-recording-bucket");
            yield return new WaitUntil(() => uploadTask.IsCompleted);

            if (uploadTask.Exception != null)
            {
                Debug.LogError("Upload failed: " + uploadTask.Exception);
                yield break;
            }

            string fileUrl = uploadTask.Result;
            StartCoroutine(AnalyseSpeech(fileUrl));

            // Wait before getting results
            yield return new WaitForSeconds(20);

            // Get analysis results
            StartCoroutine(GetAnalysisResults());

            // Get transcription and analyze text
            var req = new CreateAudioTranscriptionsRequest
            {
                FileData = new FileData() { Data = File.ReadAllBytes(localPath), Name = "audio.wav" },
                Model = "whisper-1",
                Language = "en"
            };

            var transcriptionTask = openai.CreateAudioTranscription(req);
            yield return new WaitUntil(() => transcriptionTask.IsCompleted);

            if (transcriptionTask.Exception != null)
            {
                Debug.LogError("Transcription failed: " + transcriptionTask.Exception);
                yield break;
            }

            var res = transcriptionTask.Result;
            StartCoroutine(AnalyzeText(res.Text));
        }Creating NPCs with Inworld.ai and Trigger WordsNPCs are given life using tools like Inworld.ai, which allows for the creation of characters with specific goals and triggers. For instance, an NPC can be programmed to give a task when a user says a particular phrase. This adds an interactive layer to the VR experience, making it more engaging and realistic.The code snippet for NPC interaction demonstrates how to set up these interactions. It includes setting up goals, defining trigger phrases, and programming responses and actions based on user interaction.Reference video - https://www.youtube.com/watch?v=FVDoMnkw4rYComplete Code and Overall WalkthroughThe complete code combines all these elements into a cohesive VR app. It demonstrates the integration of various APIs, handling of voice and text inputs, and interaction with NPCs. The code is structured to ensure seamless interaction within the VR environment, enabling users to experience a realistic and interactive AI interview.This guide offers a window into the intricate process of developing an interactive AI interviewer app in VR. By combining Unity's versatile development environment with powerful APIs and AI-driven NPCs, developers can create engaging and immersive VR experiences. The future of VR and AI in app development is undoubtedly bright, offering endless possibilities for innovation and user engagement.Photo Credits: Dalle3, by OpenAI
The state of sales in 2023,The current approach, tools and terminology, key insights, challenges and opportunities available in the sales domain,This article is a refresher to anyone looking to build in sales - the current approach, terminology, key insights, challenges and opportunities available in this domain. What is a sales funnel?A sales funnel is the marketing term for the journey potential customers go through on the way to purchase. There are several steps to a sales funnel, usually known as the top, middle, and bottom of the funnel, although these steps may vary depending on a company's sales model.A basic sales funnel can be described as consisting of 4 levels. Marketers can take inspiration from this basic structure to design a sales funnel that suits the needs of their organisation.Awareness: At the very top of the sales funnel is the awareness stage that is populated by the largest number of people. These people, not quite ready to be prospects yet, have just had their first interactions with your company and its offerings. They don‚Äôt know much about your brand at this stage, but are aware that it exists.Interest: The first interactions will hook some of these newly-aware people and draw them slightly deeper into the funnel. With their interest piqued, these people will spend some time getting to know more about your company and your offerings. They might browse your website or catalogue, read your blogs, or peruse reviews from past customers.Consideration: Armed with knowledge gathered during the interest stage, your prospects will double down on their efforts to know your company and offerings better. They may reach out to your customer service team with specific questions, or fill out a form to access more information. Remember, by this stage they may have already compared your offerings to those of your competitors. So, it is important to clearly answer their questions and help them understand how your offerings can solve their problems or needs.Negotiation and decision: The prospect has now almost decided to purchase your product or service. Depending on the nature of your offerings, they might begin a negotiation over the price, terms of purchase, or both. But it‚Äôs fair to assume that they have a purchase intention at this stage. At the end of this stage ‚Äîthe prospect and seller have negotiated the terms of the sale to their mutual satisfaction, and the prospect pays the seller to officially become a buyer.Extra - Renewal or repurchase: The sale stage is not the end of the sales funnel. Soon a time will come when the sales contract is up for renewal. The customer must now decide if they want to continue with the same seller. If so, there might be a fresh round of negotiations over price and purchase terms, followed by a renewal or repurchase.Here's an image of the currently most widely used Sales tools and the stage of the funnel they're utilized in:Why is a sales funnel imp?That‚Äôs the value of knowing a sales funnel: Successful sellers understand the journey a prospect is likely to take before they buy, and can make sure they‚Äôre ready to move on to each stage. They‚Äôre focused on the buyer, meeting their needs in the moment and building trust that helps move the deal forward.So what if you ditched the sales funnel? You‚Äôd run the risk of focusing too much on the close to hit quota, pushing products or services when prospects aren‚Äôt ready. Win rates would drop because prospects wouldn‚Äôt feel comfortable with the solutions they‚Äôre offered, and trust would be lost. No long-term relationships, no cross-sells, or upsells.What challenges does sales have in 2023?Challenge 1: Standing Out From the CompetitionTo differentiate from others, salespeople should exceed prospects' expectations by being well-versed in their CRM and sales software, and staying updated with industry trends and news.Today, SaaS companies offer personalised email marketing, educational content, and in-depth exploratory calls with experienced representatives. Therefore, if you want to stand out from the crowd, you must be one step ahead of the already high standards of helpful, inbound selling.Gather sales enablement materials like industry specific demos, case studies, reviews, testimonials, and analytics tools or reports.Keep up with industry news, trends, and reports to help prospects guide strategy and show the potential ROI to their supervisors or other decision makers.Set up alerts and automation to help you follow up with prospects at the right time and provide valuable resources.Existing tools - Apollo.io, Outreach.ioOpportunity: This presents an opportunity to innovate in personalized and inbound selling.Challenge 2: Meeting QuotasA sales representative's best strategy for achieving quotas and converting leads is to make phone calls - whether they are selling remotely or not.2. Two thirds of salespeople also use discounts and promotions, and more than half report that they increase sales. The most popular promotion among sellers is bundling a suite of products, followed by free trials.Challenge 3: Getting in Direct Contact with Decision-MakersWith remote work becoming prevalent, establishing a strong social media presence, particularly on LinkedIn, is crucial for connecting with decision-makers.In 2023, 67% of sales representatives worked under a hybrid or fully remote basis, and globally, almost 70% of professionals worked remotely. Many leaders and decision makers have physically moved to new locations farther away from their headquarters as industry events have largely shifted to virtual and hybrid formats.It's crucial that salespeople establish their own presence on social media if they haven't already done so. LinkedIn was rated as the most effective platform for prospect research by nearly three quarters of sales professionals.Challenge 4: Lack of High-Quality LeadsThe first place to look when dealing with lead quality issues is sales and marketing alignment. Having a shared understanding of the ideal customer profile or buyer persona for sales and marketing teams is essential.Assess the lifetime value of customers and which ones turn into promoters in order to develop and update these assets.Using tools like LinkedIn Sales Navigator, HubSpot Sales Hub, LeadIQ, and others, sales teams find and engage with prospects at the right time after identifying target customers.Challenge 5: Keeping Prospects Engaged Throughout the Sales ProcessIt is important to understand your buyer in terms of the bigger picture context so you can keep their interest throughout the sales process, which can be lengthy depending on the software package or service you are selling.It is common for sales representatives to communicate with prospects two to seven times during B2B and B2C sales processes, which adds up when considering that most work with dozens of prospects at once.The use of automated sales management tools and CRMs with lead intelligence and logging of communications with prospects (such as phone calls, emails, and actions) can help sales teams manage hundreds of data points to successfully acquire new customers.Challenge 6: Difficulty Getting Meetings With ProspectsTiming is everything. Reach out at the wrong time, you‚Äôre a nuisance. Reach out at the right time, and you‚Äôre the answer to a prayer. It‚Äôs well known in the industry that the best salespeople spend most of their time on calls with prospects listening.In order to secure meetings with qualified prospects, the best strategy is to listen. And to make it easy to meet with you. When listening for signals that a prospect is ready to meet, use these questions as a guide:Social selling requires a combination of social media savvy and sales tools that can automatically flag key readiness indicators and send automatic alerts.Key insights from 30+ interviews with sales and growth teamsOver the past 3 weeks, with conversations over 30 sales people, here are the key insights that stood out from my learnings in the current approach:Efficiency in Data Management: Many use tools like Excel and Salesforce for data organization, but there's a challenge in maintaining data hygiene and integrating personal notes effectively.Personalized Customer Engagement: Sales reps emphasize personalized interactions, like remembering personal details about clients and customizing outreach. This includes tactics like social media engagement, client lunches, and gift-giving.Utilization of Digital Tools: Sales teams heavily rely on digital tools like CRM systems, LinkedIn Sales Navigator, and calendar reminders for tracking client interactions and industry news.Challenges with Remote and Hybrid Sales: Adapting to remote and hybrid sales environments is a notable challenge, affecting how sales reps connect with decision-makers and clients.Importance of Relationship Building: There's a strong emphasis on building and maintaining relationships, seen as crucial in high-value sales.Adapting to Market and Client Dynamics: Sales reps must adapt to changing client personnel and market conditions.Efficient Use of Time and Resources: Balancing time spent on tasks like data entry and client engagement is crucial, especially in cold outreach and event networking.ConclusionIn summary, the sales funnel remains a vital concept in the sales process, guiding potential customers from awareness to purchase. In 2023, sales teams face challenges like standing out from the competition, meeting quotas, and effectively engaging with decision-makers, particularly in remote and hybrid work environments. Utilizing digital tools like CRMs and LinkedIn, focusing on personalized customer engagement, and maintaining efficient data management are essential strategies. Building strong relationships and adapting to market dynamics are key to navigating these challenges.Sales teams must balance innovative selling techniques with the efficient use of time and resources to succeed in the evolving sales landscape.Referenceshttps://www.linkedin.com/pulse/hubspot-report-global-sales-trends-look-2023-iv-leadhttps://keap.com/product/sales-funnel
Maximizing LLM performance,Short excerpt on how LLMs can be pushed from prototype to performance in a scalable way,üí°This blog is from notes from an amazing talk at the OpenAI DevDay conference by Colin Jarvis and John Allard. Here's the YouTube video (https://www.youtube.com/watch?v=ahnGLM-RC1Y)In the realm of natural language processing, optimizing large language models (LLMs) is a formidable challenge. Striking the delicate balance between performance and efficiency is akin to finding a needle in a haystack of computational complexity. The performance of LLMs, often abstract and difficult to measure, requires a nuanced approach to optimization. In this blog, we‚Äôll explore a mental model of options for LLM optimization and develop an appreciation for which option to use and when.You will leave with a foundational understanding of the two axes of optimization‚Äîcontext and LLM behavior‚Äîas well as a practical guide to the nuanced world of model refinement.The Twin Axes of Optimization1. Context OptimizationAt its core, context optimization is about fine-tuning what the model needs to know. Here‚Äôs how you can approach it:Prompt Engineering: As the lower-left quadrant in our optimization matrix, prompt engineering is your fast lane to setting a baseline. Start by refining the prompts you feed into the LLM and observe the performance changes.Retrieval Augmented Generation (RAG): Positioned in the upper-left, RAG introduces more context. Begin with simple retrieval mechanisms, and consider fine-tuning for a more nuanced approach.2. LLM Behavior OptimizationLLM behavior optimization delves into how the model should act. The two main strategies are:Fine-tuning: The lower-right quadrant represents fine-tuning, which customizes the LLM‚Äôs behavior for specific tasks.Comprehensive Approach: Sometimes, a combination of all methods is required to reach the desired performance level. An example would be integrating HyDE retrieval with a fact-checking step.The key is to start somewhere, evaluate, and then iterate with another method.Strategies ExploredPrompt Engineering: The Starting BlockStarting with clear, concise instructions and breaking down complex tasks into simpler subtasks is paramount. Giving models "time to think" and testing changes systematically can yield surprising improvements. Extending this by providing reference text and using external tools can further enhance the results.When It Shines:Quick testing and learningEstablishing a baseline for further optimizationIts Limitations:Introducing new informationReplicating complex styles or methodologiesRetrieval Augmented Generation: Expanding KnowledgeBy giving LLMs access to domain-specific content, RAG helps update the model's knowledge and control the content it generates to reduce inaccuracies.When It Shines:Introducing new, specific informationControlling content to reduce errorsIts Limitations:Broad domain understandingLearning new languages, formats, or stylesEnhancing RAG:Employ retrieval with cosine similarity and HyDEExperiment with FT embeddings and chunkingImplement reranking and classification stepsEvaluating RAG:On the LLM side, consider the faithfulness and relevance of the answers.On the content side, assess the precision and recall of the context retrieved. More data does not necessarily equate to higher accuracy.Fine-tuning: The Custom TailorWhen prompt engineering doesn't cut it, fine-tuning may be the right path. Continuing the training process with domain-specific data can optimize model performance and efficiency. For example, Canva leveraged fine-tuned GPT-3.5 to produce structured output, showcasing the power of high-quality training data.When It Shines:Emphasizing existing knowledgeCustomizing response structure or toneIts Limitations:Injecting new knowledge into the modelQuick iterations on new use-casesSteps to Fine-tune:Data preparationHyperparameter selection and loss function understanding during trainingEvaluation with relevant test sets and expert opinionFine-tuning Best Practices:Start with prompt engineeringEstablish a clear baselinePrioritize quality over quantity in training dataThe Combined Approach: Fine-tuning + RAGSometimes, a blend of fine-tuning and RAG yields the best results. This method allows the model to understand complex instructions with minimal tokens, creating more space for retrieved context and leading to a more robust performance.Practical ApplicationThe real-world application of these strategies can be as creative as generating hypothetical answers to enhance similarity searches. For example, initial baselines might yield a 69% performance rate, while a RAG with a well-crafted answer could increase that to 84%, comparable to fine-tuning. Collaborations, such as the one between Scale AI and OpenAI, demonstrate how a combined approach can optimize models to new heights of efficiency and effectiveness.Careful what you fine-tune onIn conclusion, maximizing LLM performance isn‚Äôt a one-size-fits-all endeavor. It requires a toolkit of strategies‚Äîfrom prompt engineering to fine-tuning and beyond‚Äîeach with its own set of benefits and best-use scenarios.
BuildSchool - bet on student projects,How to inspire students to take a bet on themselves and build something 0 to 1,there needs to be a part of education that allows students to dream and tinker and take risks. Something that makes them laugh and cry and hope and bond and stay up late in teams and chase self-assigned deadlines and ship something they would have never thought they could.that was the ideal of buildschool.we wanted to give students "an ecosystem of believer-builders who built things to solve problems because they wanted to". and having friends around always helps.but the issue was, we were targeting a student base who had just come back from remote learning, who was not entirely immersed into this tinkering culture and was more driven by definite career paths.in April 2022, Technical Society (TechSoc) started with minimal funding, full motivation and targeted growth hacks, to grow a strong community. I think the best takebacks from this journey are perseverence, dealing with ambiguity, and faith in the dreamers of tomorrow.and our approach at TechSoc has always been - invest in the team, invest in the participants and invest in the future. here's 3 stages of how we built a community from scratch and run it well, to this day.üí°This journey was only possible due to the amazing contribution by Anouksha Hemanth, Prabhat Bedida, Subham Sagar, Yashaswi, Piyush Agrawal, Nikshay Jain, Vishwajeet, GK Harish and several others who made this vision real.¬†building a teamfirst, we had to make team buildschool believe that the mission they were solving was important because they would be enablers of a movement much bigger than themselves. so Anouksha and me used YCombinator's startup school curriculum and actually had each manager at buildschool study startup from a perspective of product strategy, history, revenue and future steps. this helped them understand how products are built, gain confidence in talking more about these ideas and also be better thinkers in general. over 2.5 months we did multiple startup teardowns, had these people practice cold emailing founders and speaking comfortably with them. here are some of their early day case studies.once our little army was ready, it was time to go to war. boy, was it going to be brutal.inspiring a communitythe early days were very random and we had barely any clue of processes and contacts and pretty much anything. we were just very clear on what we had to solve - get 10 student prototypes, not powerpoints, out by the next 13 weeks. to begin with, the issue was, how do you even get people to care to solve problems? it was always cooler to do fancy PoRs that add sure-shot resume points as opposed to taking a bet on whether you and your buddies could build something or notand even if people do care, how do you get them to actually learn this process of product building - right from design, to shipping to release to iteration and beyond? well, the answer actually lies in inspiring people.as Simon Sinek says, if you connect with the "why" of people, you can truly move them to do unbelievable things. and we did all sorts of crazy things to get the message across. here's a video from the early days. just a whiteboard, an ai generated thumbnail and some students talking about a crazy vision of 10 teams, 10 weeks, 10 prototypes.we did speak to people and touch upon a nerve.to further this, we revived the history of the "fun in building" through our alumnus Kedar Kulkarni - who started off with very realistic stories of moving up the quality ladder by building computer vision fun-tools with his friends and then moving up to govt partnerships and funded projects. looking back, i think that may have struck a really strong chord - even in me. something to remind students of where the foundation of innovation liessustaining the experience10 teams selected. basic application on their plan for 10 weeks. we were just looking for grit. to refine our teams, buildschool organized really amazing talks right from product research and UI designing, all the way up to engineering and growth post launch. we got CXOs and industry veterans to give talks for free - how did we do that? well, we just connected with the "why" of these disruptors through a cold email as simple as:the thing that sold best was the thought of inspiring students to build moreboom! 7 high quality lectures in various domains - all conducted at zero cost. well, except for the food. it does not have to be complex - ¬†growth hack - give people free food and they show up the 1st time, give them a memorable experience and they will keep showing up again and again.give them pizza and they'll do amazing thingsand it was hard. students want to do all sorts of things at all times. so we used accountability partners called "build buds" for each team to navigate through getting the right resources, connected to the right folks, and even actually finishing right before deadlines.guess what - at a mini demo day amongst the teams - 9 of them had a functioning product! here's a random picture from the mini demo day at the 8 week mark.investing in the futureby the time the 10th week ended, teams had transformed beyond what we had thought. people who didn't know coding had built apps, people who were shy on stage were presenting, people who were unsure about their passions had picked up something new and built something meaningful out of it. well, and the buildschool team - they had gone through the extreme rollercoaster of 0 attendee events (that really happened, it was really stressful) to having a great turnout of 9 products built in those 10 weeks and then we came for a repeat in the upcoming semesters with even more student interest. what makes me smile the most, is that people actually now think of this as a thing they can do full time. end of buildsprint #1, november 2022fun fact, one team even got incubated in the startup accelerator at IIT Madras. i look forward to more such stories.‚≠êÔ∏è here's my takeaway - atleast once in life, i feel, every person must go through the pain and the fulfilment of building something from 0 to 1. because once you're on the other side of the storm - you gain the self-confidence to do anything you want. that's what our vision at techsoc always has been. cheers and happy building!
HuggingSage,Using Amazon Sagemaker to deploy whichever Hugging Face models you want, along with API inference,Amazon SageMaker Jumpstart is a fantastic tool to deploy machine learning models. In this blog, we'll walk you through the process of using Jumpstart to deploy a model named "Llama2" and expose it as an API endpoint using AWS Lambda functions.Step 1: Set Up SageMaker Domain and UserBegin by navigating to the Amazon SageMaker service in the AWS Console. Here, create a domain and user profile. This will allow you to manage access and track your experiments.Step 2a: Deploy the Llama2 ModelAfter setting up your domain and user, head over to the SageMaker console. Look for the "Sagemaker Jumpstart" section, and from there, choose to deploy the "llama 7b model." This process usually takes about seven minutes, and at the end of it, you'll be provided with an endpoint name. Make a note of this, as it will be essential for the next steps.Step 2b: Integrating any Huggingface Model with Amazon SagemakerInstead of Amazon Sagemaker Jumpstart model options, you can also bring any HuggingFace model too and have it setup. Here's how1. Setup development environment
We are going to use the sagemaker python SDK to deploy Llama 2 to Amazon SageMaker. We need to make sure to have an AWS account configured and the sagemaker python SDK installed.
import sagemaker
import boto3
sess = sagemaker.Session()
# sagemaker session bucket -> used for uploading data, models and logs
# sagemaker will automatically create this bucket if it not exists
sagemaker_session_bucket=None
if sagemaker_session_bucket is None and sess is not None:
    # set to default bucket if a bucket name is not given
    sagemaker_session_bucket = sess.default_bucket()

    
    
def setup_sagemaker_session(default_bucket=None):
    """
    Params:
    - default_bucket: Default bucket name to use for the session

    Returns:
    - session: SageMaker session object
    - role_arn: ARN of the IAM execution role
    """
    global sagemaker_execution_role
    session = sagemaker.Session(default_bucket=default_bucket)

    try:
        sagemaker_execution_role = sagemaker.get_execution_role()
    except ValueError:
        iam = boto3.client('iam')
        sagemaker_execution_role = iam.get_role(RoleName="sagemaker_execution_role")['Role']['Arn']

    return session, sagemaker_execution_role

def mask_account_id(account_id):
    return '*' * len(account_id)

def main():
    sagemaker_session_bucket = None

    session, sagemaker_execution_role = setup_sagemaker_session(default_bucket=sagemaker_session_bucket)

    # Mask it
    account_id = sagemaker_execution_role.split(':')[4]
    masked_account_id = mask_account_id(account_id)
    masked_role = sagemaker_execution_role.replace(account_id, masked_account_id)

    print(f"SageMaker role ARN: {masked_role}")
    print(f"SageMaker session region: {session.boto_region_name}")

if __name__ == "__main__":
    main()

2. Retrieve the new Hugging Face LLM DLC
Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our HuggingFaceModel model class with a image_uri pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the get_huggingface_llm_image_uri method provided by the sagemaker SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified backend, session, region, and version. You can find the available versions here
from sagemaker.huggingface import get_huggingface_llm_image_uri


## Fetch docker image URI for the Hugging Face DLC:
# 1. backend name
# 2. Hugging face LDC version


# retrieve the llm image uri
llm_image = get_huggingface_llm_image_uri(
  "huggingface",
  version="0.9.3"
)

# print ecr image uri
print(f"llm image uri: {llm_image}")


Hardware Requirements
Llama 2 comes in 3 different sizes - 7B, 13B & 70B parameters. The hardware requirements will vary based on the model size deployed to SageMaker. Below is a set up minimum requirements for each model size we tested.
Note: We haven't tested GPTQ models yet.



Model
Instance Type
Quantization
# of GPUs per replica




Llama 7B
(ml.)g5.2xlarge
-
1


Llama 13B
(ml.)g5.12xlarge
-
4


Llama 70B
(ml.)g5.48xlarge
bitsandbytes
8


Llama 70B
(ml.)p4d.24xlarge
-
8




Note: Amazon SageMaker currently doesn't support instance slicing meaning, e.g. for Llama 70B you cannot run multiple replica on a single instance.

These are the minimum setups we have validated for 7B, 13B and 70B LLaMA 2 models to work on SageMaker. In the coming weeks, we plan to run detailed benchmarking covering latency and throughput numbers across different hardware configurations. We are currently not recommending deploying Llama 70B to g5.48xlarge instances, since long request can timeout due to the 60s request timeout limit for SageMaker. Use p4d instances for deploying Llama 70B it.
It might be possible to run Llama 70B on g5.48xlarge instances without quantization by reducing the MAX_TOTAL_TOKENS and MAX_BATCH_TOTAL_TOKENS parameters. We haven't tested this yet.
# confirm requirements met for kernel
import json

def get_instance_type_from_metadata():
    with open('/opt/ml/metadata/resource-metadata.json') as f:
        metadata = json.load(f)
        resource_name = metadata.get('ResourceName', '')
    return resource_name

def main():
    resource_name = get_instance_type_from_metadata()

    # List valid instance types
    valid_instance_types = ['ml.g5.12xlarge', 'ml.g5-48xlarge']

    if any(instance_type in resource_name for instance_type in valid_instance_types):
        print("Instance configured correctly")
    else:
        print("Need to upgrade to at least 'ml.g5.12xlarge' instance")

if __name__ == "__main__":
    main()

4. Deploy Llama2 to Amazon Sagemaker
To deploy meta-llama/Llama-2-13b-chat-hf to Amazon SageMaker we create a HuggingFaceModel model class and define our endpoint configuration including the hf_model_id, instance_type etc. We will use a g5.12xlarge instance type, which has 4 NVIDIA A10G GPUs and 96GB of GPU memory.
Note: This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.
import json
import getpass
from sagemaker.huggingface import HuggingFaceModel


def get_sagemaker_config():
    # sagemaker config
    instance_type = "ml.p4d.24xlarge"
    number_of_gpu = 8
    health_check_timeout = 300

    # Define Model and Endpoint configuration parameter
    config = {
      'HF_MODEL_ID': "meta-llama/Llama-2-13b-chat-hf", # model_id from hf.co/models
      'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica
      'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text
      'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)
      'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation
      'HUGGING_FACE_HUB_TOKEN': getpass.getpass("Enter your Hugging Face hub token")
      # ,'HF_MODEL_QUANTIZE': "bitsandbytes", # comment in to quantize
    }
    return instance_type, health_check_timeout, config

def create_huggingface_model(instance_type, config, role, image_uri):
    # check if token is set
    assert config['HUGGING_FACE_HUB_TOKEN'] != "", "Please set your Hugging Face Hub token"

    # create HuggingFaceModel with the image uri
    llm_model = HuggingFaceModel(
      role=role,
      image_uri=llm_image,
      env=config
    )
    return llm_model


def main():
    instance_type, health_check_timeout, config = get_sagemaker_config()
    
    # Set role and image_uri
    role = sagemaker_execution_role
    llm_image_to_ref = llm_image
    
    # Declare llm model with create_hugging_face_model module
    llm_model = create_huggingface_model(instance_type, config, role, llm_image_to_ref)
    
    # Deploy model to the endpoint if llm_model is available
    if llm_model:
            llm = llm_model.deploy(
                initial_instance_count = 1,
                instance_type=instance_type,
                container_startup_health_check_timeout=health_check_timeout
            )
            
if __name__ == "__main__":
    main()

5. Run inference and chat with the model
5. Run inference and chat with the model
After our endpoint is deployed we can run inference on it. We will use the predict method from the predictor to run inference on our endpoint. We can inference with different parameters to impact the generation. Parameters can be defined as in the parameters attribute of the payload. As of today the TGI supports the following parameters:

temperature: Controls randomness in the model. Lower values will make the model more deterministic and higher values will make the model more random. Default value is 1.0.
max_new_tokens: The maximum number of tokens to generate. Default value is 20, max value is 512.
repetition_penalty: Controls the likelihood of repetition, defaults to null.
seed: The seed to use for random generation, default is null.
stop: A list of tokens to stop the generation. The generation will stop when one of the tokens is generated.
top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Default value is null, which disables top-k-filtering.
top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling, default to null.
do_sample: Whether or not to use sampling ; use greedy decoding otherwise. Default value is false.
best_of: Generate best_of sequences and return the one if the highest token logprobs, default to null.
details: Whether or not to return details about the generation. Default value is false.
return_full_text: Whether or not to return the full text or only the generated part. Default value is false.
truncate: Whether or not to truncate the input to the maximum length of the model. Default value is true.
typical_p: The typical probability of a token. Default value is null.
watermark: The watermark to use for the generation. Default value is false.

You can find the open api specification of the TGI in the swagger documentation
The meta-llama/Llama-2-13b-chat-hf is a conversational chat model meaning we can chat with it using the following prompt:
[INST] <> {{ system_prompt }} <>
{{ user_msg_1 }} [/INST] {{ model_answer_1 }} [INST] {{ user_msg_2 }} [/INST]
We create a small helper method build_llama2_prompt, which converts a List of "messages" into the prompt format. We also define a system_prompt which is used to start the conversation. We will use the system_prompt to ask the model about some cool ideas to do in the summer.
def build_llama2_prompt(messages):
    startPrompt = "<s>[INST] "
    endPrompt = " [/INST]"
    conversation = []
    for index, message in enumerate(messages):
        if message["role"] == "system" and index == 0:
            conversation.append(f"<<SYS>>\n{message['content']}\n<</SYS>>\n\n")
        elif message["role"] == "user":
            conversation.append(message["content"].strip())
        else:
            conversation.append(f" [/INST] {message['content'].strip()} </s><s>[INST] ")

    return startPrompt + "".join(conversation) + endPrompt
  
messages = [
  { "role": "system","content": "You are a friendly and knowledgeable vacation planning assistant named Clara. Your goal is to have natural conversations with users to help them plan their perfect vacation. "}
]

# define question and add to messages
instruction = "What are some cool ideas to do in the summer?"
messages.append({"role": "user", "content": instruction})
prompt = build_llama2_prompt(messages)

chat = llm.predict({"inputs":prompt})

print(chat[0]["generated_text"][len(prompt):])

Now we will run inference with different parameters to impact the generation. Parameters can be defined as in the parameters attribute of the payload.
# hyperparameters for llm
payload = {
  "inputs":  prompt,
  "parameters": {
    "do_sample": True,
    "top_p": 0.6,
    "temperature": 0.9,
    "top_k": 50,
    "max_new_tokens": 512,
    "repetition_penalty": 1.03,
    "stop": ["</s>"]
  }
}


# send request to endpoint
response = llm.predict(payload)

print(response[0]["generated_text"][len(prompt):])

Step 3: Integrate with AWS LambdaWith your model deployed and the endpoint name at hand, navigate to AWS Lambda. Here, create a function that calls this endpoint. Below is the Python code for the Lambda function that invokes the SageMaker endpoint:import json
import boto3

ENDPOINT_NAME="jumpstart-dft-meta-textgeneration-llama-2-7b"
runtime= boto3.client('runtime.sagemaker')

def lambda_handler(event, context):
    data = event["body"]
    print(data)
    
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
        ContentType='application/json',
        Body=data,
        CustomAttributes="accept_eula=true"
    )
    
    response_content = response['Body'].read().decode()
    result = json.loads(response_content)
    
    return {
        'statusCode': 200,
        'body': json.dumps(result)
    }
Step 4: Grant Necessary PermissionsFor your Lambda function to work correctly, it requires specific permissions to invoke the SageMaker endpoint. To grant these permissions, create an inline policy in AWS Identity and Access Management (IAM) with the following JSON:{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "VisualEditor0",
      "Effect": "Allow",
      "Action": "sagemaker:InvokeEndpoint",
      "Resource": "*"
    }
  ]
}
Step 5: Expose Lambda Function as an APIOnce your function is ready and permissions are in place, expose this Lambda function as a URL. This makes it accessible over the web and can be called using common web tools.Step 6: Testing Your DeploymentTo test the setup, use a curl command, like the one below:curl --location 'https://khhg37edhrmibhj4ahsxqnqlha0qoqqc.lambda-url.us-east-1.on.aws' \
--header 'Content-Type: application/json' \
--data '{
    "inputs":"I believe the meaning of life is",
    "parameters":{
        "max_new_tokens":256,
        "top_p":0.9,
        "temperature":0.6
    }
}'
This command sends a POST request to your Lambda function, which in turn invokes the SageMaker endpoint and returns the model's prediction.In ConclusionSetting up a machine learning model and exposing it as an API using Amazon SageMaker Jumpstart and AWS Lambda is a straightforward process. It offers scalability and ease of integration, making it suitable for a range of applications. For more detailed steps and best practices, you can refer to this official AWS blog.
Can VR revolutionize education?,A case study on how simulated environments in VR can be used to track the implicit learning experiences,üí°This case study was carried out by our super talented team - Ronith Reddy, Katherine (Jiayuew) Wang, Harpreet Vishnoi, as part of a hackathonCheckout our super-fast slide deck on this - hereTable of Contents
Executive Summary
Problem Selection
1.1 Statement
1.2 Understanding the Potential
1.2 On Bodyswaps and Meta
1.2 Reflection
Understanding the data
2.1 Insights from current data
2.2 Where is VR most effective
2.3 What Student Demographics?
2.4 What are Attitudes Towards VR for Soft-Skill Development?
2.5 Reflection
3.1 Useful metrics to capture
3.2 Creating insightful indices
3.3 Reflection
Further Research Objectives
4.1 Future Goals
4.2 Proposed Research Design
4.3 Reflection
References
Executive SummaryVirtual Reality (VR) is poised to revolutionize education by offering immersive learning experiences. Mel Slater's 2016 research emphasizes VR's potential for implicit learning, making abstract concepts tangible and enhancing understanding. Platforms like Bodyswaps, combined with Oculus VR headsets, promise innovative soft skill training. The booming VR market, projected to reach $165.91 billion by 2030, testifies to its growing influence. Comprehensive research objectives have been outlined, focusing on long-term effectiveness, personalization, inclusivity, cost implications, and device efficacy. The research aims to ensure VR's effectiveness and relevance in the rapidly evolving professional world.Problem Selection1.1 Statement Virtual Reality (VR) offers a transformative approach to education, expanding the boundaries of traditional learning. Mel Slater's 2016 paper, "Implicit Learning through Embodiment in Immersive Virtual Reality," elucidates this potential by delving into the concept of implicit learning, where individuals acquire knowledge without conscious awareness (Slater, 2016). VR's immersive nature is largely attributed to two illusions: Place Illusion (PI) and Plausibility Illusion (Psi). PI gives users the sensation of being in a virtual environment, leveraging the body's natural perception methods, while Psi makes events in VR feel authentic (Slater, 2016). The sense of embodiment, where one's physical body is substituted by a virtual counterpart, enhances this immersion.1.2 Understanding the PotentialThe educational potential of VR is highlighted by its ability to transform abstract concepts into tangible experiences. Instead of mere observation, students can actively participate, visualizing and manipulating realities that would be otherwise challenging or impossible in the physical world. Such hands-on experiences advance explanations and deepen understanding (Slater, 2016). The incorporation of VR in education also shifts the pedagogical paradigm, placing students at the focal point of instructional attention.Another study examining the efficacy of VR in K-12 and higher education found VR-based instruction significantly impacts various learning outcomes (Effectiveness of virtual reality-based instruction). Factors like learning outcome measures, type of learning tasks, and feedback mechanisms moderate the effectiveness of VR instruction, suggesting a tailored approach to VR integration can yield optimal results.The expanding VR market, projected to grow from $25.11 billion in 2023 to $165.91 billion by 2030, underscores its burgeoning influence (TAM). Specifically, the VR education segment is expected to reach a valuation of $46.14B by 2027 (SAM). Such growth is indicative of VR's increasing integration in diverse sectors, including education.Moreover, the unique data potential of VR, as seen in a 2023 study where motion data allowed researchers to identify users with over 94% accuracy, promises personalized learning experiences, enhancing user engagement and facilitating predictive analytics. Such data-driven insights can quantify skill development, bolstering stakeholder confidence and paving the way for novel monetization strategies.1.2 On Bodyswaps and MetaBodyswaps, a cutting-edge VR training platform, specializes in honing soft skills through immersive simulations, enabling learners to practice, receive feedback, and even view their interactions from another's perspective. This innovative approach to learning facilitates deep introspection and skill enhancement. When integrated with Meta's Reality Labs' Oculus VR headsets, users can experience Bodyswaps' transformative training in a seamlessly immersive environment. The synergy between Bodyswaps and Oculus could redefine soft skill training, making it more accessible, effective, and in tune with the digital age's demands. This partnership promises a future where learning transcends traditional boundaries, merging technology with human-centric education.1.2 ReflectionVirtual Reality (VR) is revolutionizing the educational landscape by transcending traditional learning boundaries. Slater's 2016 research highlighted the immersive potential of VR, emphasizing implicit learning and embodiment. The integration of VR into education promotes experiential learning, making abstract concepts tangible. The booming VR market, expected to reach $165.91 billion by 2030, emphasizes its growing significance. Moreover, platforms like Bodyswaps, when combined with Oculus VR headsets, promise innovative soft skill training, blending technological advancements with human-centric educational approaches. The future of learning seems to be at the cusp of a VR-driven transformation.Understanding the data2.1 Insights from current dataThe current data offers a multitude of insights. Personalization is at the forefront, with data distinguishing between identified and anonymous users, potentially facilitating targeted feedback. Device-specific data can elucidate how different hardware affects user experiences. Furthermore, module-specific IDs enable segmentation based on the soft skill being trained. The diverse range of event verbs and time-stamped data illuminates user interactions and pace of learning, respectively. The metadata's complexity offers an opportunity for detailed event analysis, while the "context" field provides both structured and intuitive data analysis. The grading system in the 'Scored' event can provide immediate feedback, and the data under "conversed" and "said" events can be pivotal in analyzing user decision-making patterns. The presence of the "segmented" event also hints at the platform's capability to conduct A/B testing, fostering continuous module improvement.2.2 Where is VR most effectiveVR's efficacy shines in scenarios demanding an immersive experience, such as public speaking or job interviews, where environment and ambiance play a pivotal role. It excels in behavioral training, especially in modules like "Active Listening" or "Inclusive Leadership," enabling users to practice interpersonal interactions in a controlled setting. Additionally, VR proves highly beneficial in feedback-driven scenarios. For instance, modules like "Navigating Microaggressions" can benefit from VR's ability to simulate intricate social situations and deliver instantaneous, data-driven feedback.2.3 What Student Demographics?Young Adults/Recent Graduates:Beneficial for those entering the workforce.Modules such as "enteringTheWorkforce" and "jobInterviewSkills" are particularly relevant.Mid-Career Professionals:Modules like "Inclusive Leadership" and "Navigating Microaggressions" cater to those aiming for leadership roles or professional growth.Diverse Backgrounds:The system captures demographic data, including gender and racial group.Opportunities exist to examine training effectiveness across various demographic segments.2.4 What are Attitudes Towards VR for Soft-Skill Development?To gauge attitudes towards VR for soft-skill development, one can utilize the "Rated" event, which provides feedback on the VR experience. Metrics like "Duration," "EyeContactTime," and "Participation" can be indicative of users' engagement and their positive inclination towards VR training. Integrating post-training surveys under the "postsurvey" context can shed light on user recommendations, understanding, and engagement. Moreover, conducting qualitative interviews with users who have completed multiple modules can offer deeper insights into their attitudes and experiences with the VR platform.2.5 ReflectionThe data underscores the nuanced capabilities of the VR platform, emphasizing personalization, user experience adaptation based on device usage, and in-depth analysis of user interactions. VR's strength is evident in its immersive nature, transforming traditional learning scenarios like public speaking into rich, experiential sessions. It becomes apparent that VR holds particular promise for young professionals and those seeking leadership growth. The platform's design also suggests a keen interest in understanding user attitudes towards VR for soft-skill development. The integration of feedback mechanisms and in-depth interviews displays a commitment to refining the platform based on user experiences and perceptions.Improving customer discovery with metrics3.1 Useful metrics to captureSpeech Confidence Metrics:The platform measures an individual's command over language, nervousness, clarity, and emotional state. Metrics include speech rate, the frequency and duration of pauses, changes in speech pace, and speech clarity. Vocal fillers, variability in speech rate, and fluency are also tracked. The complexity of vocabulary and structure, the tone of speech, and syntactic complexity further shed light on an individual's speech confidence.Nonverbal Behavior Metrics:Nonverbal behaviors are essential indicators of a speaker‚Äôs influence, rapport with the audience, and overall comfort. Metrics include eye contact duration, the frequency of gestures, posture changes, and smiling frequency. The platform also tracks fidgeting, head nodding, gaze direction, arm movement range, pacing, and whether the speaker exhibits open or closed body language.Voice Analysis Metrics:The platform delves into the intricacies of a speaker's voice, assessing pitch variability, volume changes, and spectral energy distribution. The rate of speech variability, intensity, and signs of vocal fatigue are also measured. Moreover, breathing patterns, voice tremors, speaking rate consistency, and vocal resonance quality are analyzed to provide comprehensive feedback.Emotional Expression Metrics:Emotions play a pivotal role in communication. The platform captures changes in facial expressions, the intensity of emotions, and the accuracy of emotion recognition. Shifts between positive and negative emotions, the speed of emotion transitions, congruence with speech, and patterns of emotional valence and arousal are assessed. Time intervals between different emotional expressions and synchronization with audience reactions provide a holistic view of emotional expression.Audience Engagement Metrics:Audience engagement metrics provide insights into the speaker's effectiveness in maintaining attention. The platform measures interaction rates, audience response rates, and the time taken to respond to questions. Attention spans, feedback sentiments, engagement patterns, interaction consistency, and audience reaction times are also tracked. Additional metrics evaluate the quality of audience questions and the rate of interruptions.Speech Content Metrics:The content of a speech is fundamental. Metrics include clarity scores, transcript coherence, use of technical jargon, and the organization of the speech. Effectiveness of visual aids, verbal repetition, alignment with a prepared script, word choice, and engagement phrases are also assessed. The platform also evaluates the appropriateness of speech length relative to content.Face Tracking:Quest Pro's Face Tracking API detects facial movements, converting them into expressions like jaw dropping or nose wrinkling.This data offers insights into Engagement and Emotional Responses. Metrics derived include Facial Expression changes, Emotion congruence with speech, and Emotion duration & intensity. Facial expressions indicate user engagement, crucial for business retention and understanding user behaviors. They also serve as primary indicators for emotions, facilitating deeper user behavior interpretation.Overall Performance Metrics:The platform provides an overarching evaluation of a speaker's performance. Metrics include alignment of speech duration with the agenda, audience retention, and content recall. The effectiveness of using the virtual environment, adaptability to unexpected scenarios, use of audience feedback, and smooth transitions are also evaluated. An overall engagement score, the speaker's self-assessment, and post-speech evaluations from the virtual audience provide comprehensive feedback on performance.3.2 Creating insightful indicesThe following indices, derived from detailed metrics, can be instrumental in capturing user behaviors, measuring their progress, and ensuring the platform's alignment with real-world needs. This alignment not only enhances the user experience but also boosts the business's value proposition, making it more competitive and relevant in the educational and corporate sectors.Communication Index (CI):Definition: CI quantifies verbal and non-verbal confidence scores, providing continuous feedback for user improvement.Composition: Verbal confidence correlates with speech speed, spacing, articulation, clarity, tone, and intonation, and is inversely affected by filler words and word complexity. Non-verbal confidence is gauged through metrics like eye contact, hand gestures, body posture, head movement, and facial expressions.Business Value: CI offers insights into overall communication efficiency, emotional intelligence, and clarity of thought, essential for personal and professional growth.Skill Improvement Index (SII):Definition: SII contrasts pre and post-VR assessments on soft skills.Strategies: Design a module-specific skill improvement index, like a Product Management Skill Index combining Leadership, Critical Thinking, and Requirement Understanding.Benchmark against industry standards and potentially introduce accreditations similar to PMP for Project Management, setting a new industry standard.Business Value: Aligning with industry standards and educational guidelines legitimizes the VR program, ensuring its relevance and effectiveness in real-world scenarios.Engagement Index (EI):Definition: EI measures the ratio of active learners to the total number of learners within a week.Research Direction: Gather feedback from administrators or teachers about desired learning outcomes. Collect feedback from learners on their VR experiences. Combine qualitative insights with objective metrics for enhanced metadata analysis.Metrics for Active Interaction:Users actively engaging for at least 10 minutes.70% gaze tracking towards the subject.Achieving an assessment score of 75% or more.User feedback, progress tracking, and ensuring 80% of user response lengths meet the average.Ensuring users exhibit body and facial reactions 50% of the time during training.Business Value: High engagement indicates users' commitment to the program, translating to a higher ROI. It's also pivotal for effective soft skills learning.3.3 ReflectionThe platform harnesses a multitude of metrics to comprehensively assess an individual's speech and communication abilities. From verbal fluency and non-verbal cues to intricate voice analysis and emotional expressions, every aspect of communication is meticulously tracked. Quest Pro's Face Tracking API elevates this by capturing subtle facial expressions, crucial for understanding engagement and emotions. These metrics, when synthesized, form indices like the Communication Index (CI), Skill Improvement Index (SII), and Engagement Index (EI), each offering unique insights. These indices not only enhance user experience but also fortify the platform's position in both educational and corporate landscapes, underscoring its real-world relevance and value.Further Research Objectives4.1 Future GoalsTo further enhance the understanding and application of the VR training platform, several research objectives are proposed:Long-Term Effectiveness: One of the primary concerns is the sustainability of VR training. The objective is to assess if individuals who score high in VR modules consistently exhibit these soft skills in real-world scenarios over extended periods.Personalized Training: With a plethora of user behavior metrics at hand, the potential for creating a tailor-made training experience is vast. Research will focus on leveraging this data to adapt training modules to suit individual user needs and learning patterns.Inclusivity in Training: Ensuring that VR training is accessible to all, including those with disabilities, is crucial. Investigations will delve into the platform's adaptability and effectiveness for differently-abled individuals.Financial Viability: The cost implications of VR setups can be significant. Research will aim to determine if the purported benefits of VR training, in terms of skill enhancement and retention, offer a tangible return on investment.Device Efficacy Comparison: As the system is compatible with various devices, it's imperative to assess if the training experience and outcomes are consistent across platforms. This research will pinpoint if certain devices, especially immersive VR setups, provide a definitive edge in training efficacy.Emotional Intelligence Enhancement: Emotional intelligence is a cornerstone of effective communication and interpersonal interactions. Given VR's capability to mimic complex social situations, the objective is to ascertain if VR methods outperform traditional training means in enhancing emotional intelligence.4.2 Proposed Research DesignMethodology: A quasi-experimental design will be employed.Target Group: The primary focus group will comprise business students on the cusp of entering the workforce. Their fresh perspective and imminent transition to a professional setting make them ideal candidates.Data Collection Tools: A combination of self-assessments, surveys, and quantitative meta-data collection will be utilized. The Skill Improvement Index (SII) will be a pivotal tool, employing pre and post-intervention scoring to evaluate modular learning efficacy.Literature Review: A comprehensive literature review will be undertaken, employing numerical scoring methods. Both quantitative (random-effects and fixed-effect models) and qualitative approaches (thematic and narrative analysis) will be used to draw insights from existing research and literature.This research roadmap aims to address pivotal questions, ensuring the VR training platform's effectiveness, inclusivity, and relevance in today's fast-evolving professional landscape.4.3 ReflectionThe proposed research on the VR training platform is comprehensive, focusing on its long-term efficacy, customization, inclusivity, cost-effectiveness, and technological compatibility. By targeting soon-to-be professionals and utilizing a mix of data collection tools, the study aims to validate VR's potential in modern skill training. This approach promises to solidify VR's role in the contemporary professional development domain, ensuring its relevance and effectiveness.
Escaping your mind, into unimaginable futures,Understanding basics of futurecasting and learning from examples of people who did it wrong.,Futurecasting, - or the art of envisioning the unforeseen, is a crucial skill in our dynamic world where the tide of events can shift unpredictably. Our perception plays a pivotal role in determining how we steer our ship into the future.Mental Maps and PerceptionsAs individuals, we need to acknowledge that our understanding of the future is inevitably tinted by personal perceptions and biases, not objective truths. Each person's mental map, the internal framework we use to navigate decisions, varies, and these maps significantly influence the decisions made regarding the future.The Risk of SuccessIronically, success might be one of our biggest stumbling blocks. Past triumphs can make us overly reliant on the mental maps that brought us to our current position. A strategy that yielded success in the past isn‚Äôt guaranteed to work indefinitely, and overconfidence in these tried-and-tested mental maps might blind us to other possibilities and opportunities.The Pitfall of Incorrect MapsAs the legend goes, the first set of Spanish voyagers actually thought that there was a water body connecting south-west and north-west coast of USA. And they made a wrong map, with full confidence, not knowing their mistake. Fun fact, until 16 years, voyagers used the wrong map and after multiple complaints, they were finally able to break the mental model of the government to actually update the map. And that process took 160 years total. The story of the erroneous map of the USA, used for 160 years due to stubborn belief in its accuracy, serves as a telling example. An incorrect understanding leads to misguided actions, and once a particular map (or belief) is accepted, altering it becomes a formidable challenge.Asking Uncomfortable QuestionsFuturecasting is not about comfort; it's about preparedness and openness to disquieting truths. It requires us to challenge our convictions and ask questions that make us squirm. This process involves thinking divergently about what lies ahead, questioning the mental maps we hold dear, and being willing to redraw them as needed.Navigating with AwarenessMuch like skilled sailors navigating the waters with an acute awareness of tides and winds, individuals and organizations need to craft strategies with a deep understanding of global currents and shifts. Where you eventually land is a result of both your goals and the global factors that either facilitate or hinder your journey.Case in Point: The Oil BusinessA look at the oil industry, where success is traditionally measured by rig count, reveals a critical mistake. Businesses often take one scenario, tweak it slightly, and convince themselves that they are working with different models. However, slight variations of a single view do not capture the diversity and unpredictability of the real world.Learning from IBMA picture from forecasting by an IBM leader, saying that PCs weren't a good enough marketIBM‚Äôs monumental miscalculation of the PC market potential‚Äîestimated at a few hundred thousand dollars, while the real value was a staggering $30 million‚Äîcost them dearly. The mental maps of their strategists, albeit successful in the past, failed to envision a scenario where PCs would be ubiquitous, missing out on a colossal opportunity and leaving an estimated $200 billion on the table - $100 billion for the software OS (Windows, then called MS-DOS) and $100 billion for the chip making market (that Intel captured)Scenario Thinking: An Essential ToolTo prevent these decision traps, scenario thinking is invaluable. It helps to mitigate overconfidence, encourages outside-in thinking, enables seeing the full picture, corrects problem framing, and promotes asking the right questions. The goal is not to predict the future accurately but to ask better, more insightful questions, to prepare for a variety of possible futures.ConclusionFuturecasting, therefore, is a dance between confidence and humility, knowledge and curiosity, planning and flexibility. It's about continually asking "what if" and being prepared to redraw your mental maps to navigate the thrilling, unpredictable, and often unthinkable tides of the future. Let's escape the confines of our minds and be open to the vast, exciting possibilities the future holds.
Impact of Technology on Employability and Wealth,Understanding what the future of work and employment could be like, given advancements in tech,Checkout this comprehensive analysis of various stakeholders, factors and strategic outcomes - linkIf the very essence of employability is threatened then isn‚Äôt it important to talk about it? And this time it is different. We talk about how technology shifts have been there, but this time its different. Today there are 78 million jobs that are vulnerable to be automated (IMF Report, 2021). And not enough non-routine, cognitive jobs being created (62mill) to fill this unemployment. gap(IMF Report, 2021). And lets take today‚Äôs discussion to talk about how this unfair wealth battlefield can be given its justice. It is currently important to study how this divide is ‚Äúcreated strongly due to technology‚Äôs impact on employability‚Äù,at this point time - and prepare ourselves for an ambiguous future1. Job Displacement due to AutomationAutomation has pierced through not just manufacturing sectors, but a vast array of industries, radically transforming the employment landscape. Far from being limited to routine and repetitive tasks, modern technological tools are expanding into spaces we once believed were uniquely human: content generation, graphic design, music composition, and even video editing.Data Snapshot: Forecasts warn that a staggering 800 million workers worldwide could find their roles obsolete by 2030 due to automation. Furthermore, an alarming statistic states that 60% of all occupations have at least 30% of tasks that can be automated, signifying the breadth of the potential impact.2. The Daunting Skills GapThe velocity at which technology is advancing presents a conundrum. On one end, there's a surge in tech-centric roles, while on the other, there's a glaring skills deficit. This gap is even more pronounced among certain demographics: the elderly, individuals in rural settings, those encumbered by responsibilities, people with disabilities, immigrants, refugees, and professionals in non-tech domains.Data Snapshot: The World Bank's 2020 report on the digital economy illuminated the magnitude of this disparity, noting that a mere 1% of Africa's workforce possesses ICT skills. Moreover, Deloitte's findings underline this crisis, with 64% of employees believing their organizations are unprepared to bridge this skills chasm. Adding fuel to the fire, only 17% of institutions are confident about having a future-ready workforce.3. Escalating Income InequalityThe technological boom has inadvertently created a rift between high-skilled tech aficionados and low-skilled workers. As the former group reaps the benefits of a tech-driven economy, the latter, especially those susceptible to automation, grapples with stagnating or diminishing wages.Data Snapshot: As per the U.S. Bureau of Labor Statistics, the wage gap between those armed with a college degree and those with just a high school education has been expanding. The United Nations' ITU highlights another facet, pointing out that a lack of access to digital tools and the internet exacerbates income inequality.4. Concentration of Wealth: The Tech ElitesIt's undeniable that the tech era has given birth to a new class of ultra-wealthy individuals. This isn't just about riches but also the power and influence that come with such immense wealth, from influencing political decisions to shaping economic policies.Data Snapshot: 2021 data reveals that the collective wealth of global billionaires skyrocketed to over $10 trillion. During the initial phase of the COVID-19 pandemic, the ten wealthiest individuals added a colossal $450 billion to their coffers. To put this disparity into perspective, the top 1% of the global populace controls more than 44% of the world's wealth.5. Gig Economy: Freedom or Feudalism?The gig economy, bolstered by technology, promises flexibility and autonomy. Yet, it's essential to ponder whether it's a genuine avenue for empowerment or merely a modern form of feudalism, where workers, despite being 'connected', remain isolated and vulnerable.Data Snapshot: The gig ecosystem has witnessed explosive growth, with 36% of the U.S. workforce engaged in gig roles, cumulatively contributing to a market valued at $204 billion.In synthesizing these insights, the digital age emerges as a paradox. While it offers unparalleled opportunities for growth and innovation, it simultaneously casts shadows of inequality and uncertainty. As we navigate this era, it's paramount that stakeholders ‚Äì from governments to corporations to educators ‚Äì work in tandem to ensure that the digital future is inclusive, equitable, and just.
Switching streams midway,,I did my undergraduate in Civil Engineering and pursued my career in software. This blog would be the story and the specifics of what a lot of people have asked me over the years, about how I made the switch and some things that you can use too, if you're interested in the sameI'll keep a major part of this blog very generic to the topic "switching streams" in general and then I'll add a section, at the end, about things I did.Continuous learning is the name of the gameToday, learning anything is completely possible, at any given point, if you're willing to put in the work.A lot of times we're under the notion that only a few people can learn a skillset based on the course they're "officially" enrolled in or some background experience they've had. I don't believe so. I think, that in this all-knowing YouTube and ever-improving-chatGPT era - anyone can learn anything from the Internet - for free, at their pace and convenience and from tonnes of world leading tutors. Here are some pointers that might help you:There is no magical playbook - to learn anything, it takes a lot of time and persistent effort of finding the right material for yourself and creating a roadmap for improving in the skill. So it's fine if you're not having a clear path to start with. There are a couple of channels that can give a clear path, but I think, that just starting from a Google or YouTube search about the topic name is a good place to start.Taking notes really helps - Even if it's rough screenshots, scribbling random thoughts while learning or just copy-paste's of youtube videos - it works. I'm currently exploring YouTube video summarizer plugin in ChatGPT for this purpose. What's even better is revising and refining notes - for smarter reference later. It's like creating a power pellet from your learnings that you can consume at any point in the future to catch up on the skill super-fast. I used Notion for organizing whatever I've learnt into easily reference-able notes.Motivation is 2/3rds of the game - Dedicate a fixed time slot for learning and keep struggling to make the most of it. Have very little ego. Initially you might suck - that's cool. But after breaking your head for about 1-2months, you'll not even realize but you'll naturally start seeing patterns and connections. That's the next level.You shouldn't be afraid to suck at something new and give up if you struggle across even when understanding a paragraph of a new topic. Hell, when I was learning programming, it took me about 12hours to fully grasp and implement a 5 min youtube video about animation. But then, relentlessly pursuing on and on and on, I was able to condense 12 hr worth learnings into 5mins of revision through my notes. It's going to be hard, but if you just stick with it, it's going to be worth it.The world is project-drivenWe live in a fast paced world where most teams just look for how much value an individual can add to their team. If you've independently taught yourself and built impactful projects, no one really cares about your background coming from.This can be harvested well to your benefit if you want to even your odds with those, who are already in the stream you like. Just build projects that rival with them. And these days YouTube has walkthroughs of any project across any stream for free - it's just so good.Here are some key pointers to help you in this leg:Projects are strong applications of learning - Project can mean anything - a tiny robot, a sketch, a basic app, a research paper commentary, a spreadsheet, ¬†a 5sec musical composition, a youtube video - anything. Point is - you're supposed ¬†to have employed your learnings and you're supposed to have worked super hard. It will help grasp the reason of each thing you thought was dumb but you read anyways and help translate it into moving parts of something very impactfulStart by copying other people's work - A good place to start is searching for existing projects on the internet and interpreting and recreating their work. Painters, musicians, writers, chefs - do it all the time while practicing - just patiently learning from the experience of others. This would help avoid pitfalls and learn best practices from your teachers. Again - don't just blind copy as they do. Refer - understand - try to recreate - fail - go back to refer- then understand - then try to recreate and so on. Add your own flavor to each project - Employ a personalized touch to your project so that its unique. In the early days, I liked reading so when I came to programming projects - I would just add quotes of my favourite personalities in them as my touch. Now for you, it can be anything you like, but once you do this, you just differentiate extremely well from every other person who's just blind-copying and give a polished overall outcomeTie the project to your ego - I get why this might be a double-edged sword but hear me out. Once you set a benchmark for the finished expectations from a project - tie it to the fact that it's something the world will see as a finished project from "you" and hence it can't fall below a certain quality threshold. For me, it used to motivate me to always push the project to completion and have it satisfy some baseline criteria.Learn and grow with stakeholders - Some projects, you might not do alone and hence the coolest thing would be to learn from your collaborators and ensure they have a good experience working with you. Believe it or not, people and team skills are still one of the most important virtues for any team. Have fixed meetings, track your progress, hit your metrics and note down and share - what each of you learnt from the project. Projects offer collaborative opportunities and hence you can learn much beyond the books and the internet - you learn from people and in a lot of ways - what you learn will surprise you.The more you passionately employ your learnings into real-time applications the more strongly your education will stick to you. ¬†How to sell yourselfThe next step after building projects is to share your work with the world and capitalize on it.Few pointers on this are:Have an online gallery - Every time you learn something, it would really help if you could maintain a online browsable gallery of your work. Be it your GitHub page or Dribbble gallery or LinkedIn profile or blog page. Just everytime you wrap up learning something, create an online snapshot for the world to view your thinking, approach, knowledge depth etc.Look for interesting people and DM them: I used LinkedIn. Each time I found a person with interesting work - I would DM (Direct Message) them on LinkedIn and if possible, even get on a call to understand how they thought and learn from them. Seriously, it helps so much to get other people's feedback on your approach. Beyond this, there are obviously interesting conferences that you can just boldly sign up for, attend and be the newbie who talks and learns from everyone.Confidence is free:  In the early days in this new stream, you'll often feel imposter syndrome and underconfident - so start pretending otherwise. With time, it will grow into something more solid and with the knowledge and experience piling on, it would just compound. Confidence also helps express passion and clarity for the subject matter so the more you are confident about your projects and subject matter, the more people would value your workI guess that about sums it up - in a nutshell - randomly search for ways to learn, iteratively keep sucking till you get better, build toy projects that make you happy and confidently talk about them on platforms. Rest, I'm 100% sure, the journey you take will help you figure out.My storyAlright, now I'll be more specific about my story and things I did. I grew up in a middle-class family in India, thinking that a good college, a good job would be the only salvation. So naturally, once I got into the 4 year Civil Engineering program at IIT Madras, I thought that this would limit me from pursuing anything lateral. But in this internet-learning driven age, that's hardly the case.To find what I liked - for a year, I dated a lot of career options - photo and videography, standup comedy, automotive engineering, consulting, designing, architecture and many countless others - mainly through hanging with different clubs of students in my college. ¬†Once I stumbled upon "software startups" I've just felt an irresistable pull to it, that stands to this day. Obviously there are a lot of days where I've doubted it too, but if it's your calling, some part of you always knows and gets attracted to it.Now the specifics okay. I'll make it pointwise and list specific instructors and courses so that you can maybe check them out:Disclaimer: You don't have to do it like me. Just read my opinion and do it the way most apt to you :)I learnt the basics of programming in C Language from the hard copy book - How to C, by Prashant Kanetkar. It took me 4 months with my other college work helps understand the foundation of how a piece of code is designed, written, interpreted and executed.Next 4 months, I learnt web development (HTML, CSS, JS) from DevEd, WebDevSimplified and Brad Traversy. I also use Laith Harb for time to time learning of new skills. Also, I used to build a lot of toy projects that these youtubers would post walkthrough videos of - this udemy course of 20 projects with Javascript is pretty sweet - link. Also learnt Git during this timeI used to also pursue first 2 hrs of any random long 7-8hr videos from FreeCodeCamp's youtube channel - anything from Data Structures and Algorithms to CyberSecurity to even color designing. I even did a bit of digital illustrations from from Gary Simon.After this, I was consuming content and courses very fast. In the next 2months I learnt NodeJS, ¬†React and TypeScript, GraphQL, Docker and other misc topics. These are the most common industry technologies in use these days.I even learnt basics of Data Structures and Algorithms and some Systems Designing from Algoexpert. A good place to start learning the basics of literally anything programming or computer related is Fireship - who does amazing 100second video intros of a tech.From time to time, I've flirted with Machine Learning and AI courses from DeepLearning, ZeroToMastery and random blogs and projects and stuff. But these 2 sources are very strong for itNext up, I was having trouble deploying apps and understanding large scale software so I studied systems designing from Gaurav Sen since it helps fall in love with the strength of computing. I also completed certifications from AWS, Google and Microsoft with the help of Stephane Mareek's amazing contentIn fact, I feel that Stephane Mareek's course on Cloud Computing is a must-do for any software engineer to appreciate the beauty of internet and all cloud services that become the strong backbone of it. Just do this course, trust me.Yeah and after that I got into generalist mindset and I was brushing up on everything from Figma for designing to Flutter for app development and was tinkering with over 10 frameworks and DevOps products at any given point for building great software. To start getting more professional experience, I used to always offer myself up as a cheap developer to startups. I remember cold reaching out to 200+ engineers and founders (not HR since they have less of a say in hiring than actual engineers who lead teams) on linkedin over 4 months before I landed my 1st internship.In startups, ¬†I could learn a lot everything about businesses - in the high-growth-vacuum environment that startups offer. Plus I made some money, re-invested that back in random tech-courses and business-books that I used to pick like a kid at a candy store and kept building and building a 2nd brain of mine in Notion.I kept up my spirit and did create projects entirely from scratch on my own right after the 1st 6months even though they felt shitty and didn't work as well. Some, I couldn't even afford to keep running cause I would have to enter credit card details and I didn't have one back then. So I created YouTube videos or blog posts and shared the code as I sunsetted each project deployment. You can find a log here - link. Point is just keep trying honestly.2nd brain in NotionYeah and after that I tried to run my own software startup, made some money and I failed to run it long term (here's the story of it - link). I balanced all this work with college. Fun thing is, once you do projects, everyone wants to give you a shot - be it professors looking for programmers or startups looking for part-time developers.A lot of times, people ask me how I managed my time or remembered so much. I do not. I just used "Google Calendar" and practiced using a lot of tools to keep reminding me to meet my goals and do my things. Every night before I go to sleep, I plan my next day's calendar for 10mins to be a good student, programmer, student-club head, brother, son, friend or any role that I feel is important dedicating my time to. Recently I even started to take Mondays to plan 3 to 4 most important things I wanted to achieve in each sphere of life in that week - like learning to cook, or reading a book, finishing an online course, deploying a project, teaching my mom about tech or even learning to play squash or salsa dancing with my friend - I tracked it all so I knew if I was missing out on time for any of my commitments. My academics did take a hit no doubt - but I knew that I wasn't giving enough time to it and why I wasn't so it was cool. Recently, I ended up applying to Carnegie Mellon University.It took me a lot of research and advice - and I made this document to put down my thoughts, goals and opportunities - link. This was the single most important thing I did before choosing what I wanted to do next. I showed this doc to my seniors, people I knew and respected, family and friends and finally collated their advice into a decision. For applying, here's all I did:I gave the GRE test (325/340 with 4.5 in essay writing) and TOEFL test (111/120) though I didn't think GRE would have much weightage now. People say you gotta take paid courses and prep for 5 months and I don't believe it. Just mark your calendar. Go through this free material I aggregated in 2022, for GRE and TOEFL and you'll be able to get it done sooner.Next, I talked a lot about my projects in my SoP. I wrote about majorly, creating impact through startups and software, you can give it a read - link.My CGPA in academia was 8.01 out of 10 in my undergraduate college. I willingly didn't study as much in class or in pen-paper exams. But I pursued projects passionately and that's where I caught up on.For Letters of Recommendation - 1, I got from the CEO/Founder 1st startup I worked with for 9months. Next, a letter from a Computer Science professor in my undergraduate whom I helped for free as a DevOps engineer on his logistics project, back in my 3rd year. I got my 3rd reco letter from my favourite Computer Science professor who had taught me 1 course (and that too I didn't score that well XD), but he just looked at my profile and work and decided to give me a shot. And yeah, that's where sometimes just asking gets you to. I just wrote him (and other profs too) an email and he called me for a 20min meeting and decided to help me outI guess with all of that, I'm now headed for a Masters in Software Management to Carnegie Mellon Silicon Valley. I've messed up a lot. Have done a tonne of random things and kept exploring. Some of it worked. Most didn't. In the end, I just have 1 advice - read a lot of opinions to expand your world view but at the end, internalize all that info with with what feels right in your heart, and trust me, just fuck it and do it.
Technology and India - 1970 vs 2023,A very small mental snapshot of what my undegrad college (IIT Madras) felt like when I graduated,Here's just a basic email thread exchange I had with 1975 passed-out alumni of Indian Institute of Technology (Madras) - exchanging talks about the way things are at IITM right now in 2023 vs in 1970. What ensued were views about IIT Madras and India in the 1970s. This is a fun anecdotal blog that tells 2 stories from different ages and how, from studying what happens in India's colleges, we can study the broader outlook of technological progress in the nation1970sFollowing is the message from an alum - Sudhir Krishna who passed out in the 1970sB.Tech was a 5-year program. ¬† Half of the first year was spent toiling in the workshops wearing khaki uniform. ¬† The curriculum was course-heavy: ¬†Usually had 7 courses per semester, and reached 8 courses in the 7th semester (4th year). ¬† Continuous contact hours from 8 AM to 4:15 PM (with lunch from noon to 1:15 PM). ¬† ¬†Not much free time for creative projects except in the final (10th) semester. ¬† ¬†The German collaboration was still evident and some German faculty were still around. ¬† The campus population (students, faculty, administrators, staff) was much smaller than it is now. ¬† ¬†We all had single rooms in hostels. ¬† The annual cultural festival Mardi Gras was started and has now become Saarang (right?). ¬† ¬†Students were multi-talented -- academics, music, chess, bridge, quiz, drama/ acting, mimicry, sports, literary activities, etc. ¬† I myself was captain of the institute chess team, member of the institute tennis team for a couple of years, won the institute debate contest one year, represented IIT-M at the cultural festivals of other IITs, and became Under Officer in NCC Air Wing. Also, back in the 1970s, India was still a poor country with a socialist economy. ¬† The country's global image was associated with poverty, backwardness. ¬† Only some American universities and companies were aware of the IIT brand, most notably Harvard Business School (where I got my MBA) and management consulting firms like McKinsey. ¬† Even back then, HBS admissions officers said they viewed IIT B.Techs the same way they did MIT and Caltech undergrads. ¬† But they made fun of Indian typewriters that were used for completing the application formsI received an invitation from McKinsey for an interview that began with "Because of your Indian Institute of Technology background,......"2023I recently graduated from IIT Madras. The hostel sector is quite lush with trees starting to form canopies above. The monkeys are not only more in number now, but more in daring as well. They're smart, bold and know how to work the system to get what they want. The academics are strong and flexible with students having the option to pursue whatever they want, irrespective of their BTech department. A lot of application and project based courses have sprung up where we do assignments and mini-projects as opposed to just written exams. Students are a lot in number now and it's fun to see everyone put honest hard work in their passions - right from technology and sciences to art, music, comedy and gaming. You find people of all interests and talents. The labs and workshop are still hands-on, though only about 40% of what it used to be in the 1970s. The departments have also developing into independent schools of specialization on their own with tonnes of activities, projects, lectures, events, yearly festival and more. In fact, the new department called "Engineering Design" - (near Velachery Gate) offers a lot of product designing and entrepreneurship encouragement in both- ¬†hardware and software. There's the department for management studies, humanities and flexibility of leveraging online platforms like NPTEL (https://nptel.ac.in) apart from attending in-person classes.Here's my favourite part - the student-run project lab - Center for Innovation at IIT Madras - has over 20 teams and a 2 storey cool building, each one tinkering in topics all the way from AI, robotics, app development and racecar engg to energy efficiency, rocketry, product designing and rural tech (check this for more - https://blog.techsoc.iitm.ac.in/tech-iitm-in-5-mins/). So I'd say recovering from the pandemic, projects are at a powerful highHostel culture also seems to be re-igniting, guess a lot of it got hit during the pandemic years but teams like Schroeter, LitSoc and TechSoc are putting the fire back in. Athletics is strong and we won 1st in Men's championship and 4th overall in Inter IIT Sports Meet. We bagged 6 medals in Inter IIT Tech Meet (Golds in AI and entrepreneurship) and came 3rd in Inter IIT Cultural Meet.In a nutshell, IITM has become the perfect home for pursuing anything passionatelySpeaking of animals - monkeys are ruling the hostels so much so that we get a complaint email about monkey assault every week or 2. They're bold and smart and compete for food. ¬†The deer and blackbuck population is also fair and we do hangout with them in lawns sometimes. Dogs were actually moved out of the campus in 2019 since they used to hunt deer. There's a species of red ants we see all the time and snakes are barely spotted once or twice a year during rains, if anytime. Funnily, scorpions are seen near admin buildings and hostels sometimesWorkshops are lighter and we have much more time for creative activities. If you can think of it, there's a club for it working passionately.On nearby places - Right now, Taramani is still go to for tea, snacks and cigarettes and food delivery orders. There are 2 cycle repair shops in insti. We get great breeze, most likely attributed to the greenery since the construction has blocked out the sea breeze. Student's these days hang out mostly around a huge mall outside Velachery, the food shops along the 100 ft. Road outside Taramani, Elliot's and Marina beach, Chetpet Eco-park this place called "Hidden Lake", Tada waterfalls in Andhra, and Westin bar (if you have money :)). But insti itself is I think the most happening place for so many technical, sport and cultural things that folks barely step out.On innovation- Self-innovation and Entrepreneurship is being spearheaded by the new Director - Professor Kamakoti. In collaboration with Indian government, Professor Prabhu Rajagopal and his team - is doing several things to help students build products and eventually, companies. In fact, a couple of companies like Ather, HyperVerge, Solinas Integrity - born right inside IIT Madras - are making waves. Most recently Clueso got funded by YCombinator in 2023 and it furthered more students to do this. On inside insti ¬†- The Dhaba outside Saras is now a full-fledged restaurant with all kinds of Indian food, juices and snacks. It's currently managed by the caterer - Zaitoon - but the tender is open for bids every 3 years. We had to recently monkey proof it due to the stealing incidents XD. The first floor of this is converted into a colorful place with a large television screen for student clubs to sit and ideate. Mummy Daddy Andhra mess outside Taramani is our go-to place for Biryani. Our tuition fees, without scholarship, are now INR 1,00,000 and hostel fees is INR 26,000 for each semester. The hostel bonding, as Srikanth said, is more shifted to clubs and messes are detached from hostels. This has helped in knowing a lot more breadth-wise bonding.A lot of construction is going on now. There are 2 new Academic complexes. And a new sports complex is being made near the tennis courtsIIT Madras is a lot more creative, entrepreneurial. ¬† Its population is much larger. ¬† The post-graduate programs have grown a lot. ¬† Admissions quotas based on caste and gender have made the student population more diverse. ¬†(We had very few women during our time.) ¬† The IIT brand name is now world renowned, but still based largely on its undergrad B.Tech program and not yet for post-graduate and research programs. ¬† IIT alumni have distinguished themselves in America and reached the highest levels in industry and academia. Also, India as a country has changed. ¬† It is capitalistic, entrepreneurial and technologically advanced. ¬† The country's nominal GDP is world #5 and projected to become world #3 by 2028. ¬† PM Modi, in my opinion, is a superstar on the global stage and gets royal treatment wherever he goes. ¬† Geopolitically, India is viewed as a counterweight to China. ¬† This is shaping up to be a China-India century and will be very evident by 2050. ¬† As for IITs, there are 23 of them now. ¬† While the 5 original IITs still rank at the top in India, some of the newer IITs are establishing niches of excellence for themselves. ¬†IIT Madras is ranked at the top.Attaching a couple videos and pictures of campus here for your referenceCheckout these videos:Aerial view IITM - https://www.youtube.com/watch?v=0m3IlAmSiFsCampus tour - https://www.youtube.com/watch?v=fUfsG5xQnuUFruit juices are still cheap. Canteens still open till 3am. Sunlight still glitters through trees. Students still mess around but have that drive to innovate and lastly - friendships are still stupid and meaningful.
Ole ole Alakananda,Going from failed standup comedy and jugaading expenses to growing up and finding passion,I recently graduated from Indian Institute of Technology, Madras. Here's 8 lessons I learnt from my 8 semesters and stories of how I learnt them so this is not some lame vanity post.July 2019, entering collegeSemester 1: It's a big world and trying everything, even lying outside your comfort zone is free. Be open to talk to everyone and make tonnes of friends. Semester 8 will teach you why its important. I picked up this lesson while borrowing 300 from a hostel guard to pay for pizza at 3am, attempting failed jokes at open mics and also while countlessly sitting in club orientations. It's always a good time to suck at new things :)Semester 2: It's college, you can sign up for tonnes of things and asking anyone for help is free - people genuinely want to help. I learnt this while being disciplined and trained by the legends of the Raftar Formula Racing team as we championed 3 national trophies against all odds. Discipline and accountability take passion and convert it into wonders.Semester 3: Just build projects that teach you skills you find cool. Persistence and hard work will help you trump all odds to be good at what you love. Maintaining motivation is 2/3rds of the game. Sitting in a COVID ridden world, using my neighbour (Jay Bafna's) WiFi to learn software development and making dumb projects like todo apps taught me that just inching a little closer to ambition. Preferably start an online presence of you that tracks your work well - write about your work and shareSemester 4: At this point, I'd say start taking bets on yourself and bold risks is the best - you're young - what's the worst that could happen? I learnt this core lesson when I decided to leave home and work at Farmako (YC S20) with a different set of legends. Having very little resources and pure hunger to survive and succeed over 9 months taught me a lot about the brilliant impact a small determined set of people can have on the world. Be stupid, take more bets. You're young and smart - you'll figure it outSemester 5: You'll see a world of people who seem to have figured it out and gotten it right. But most of us are just winging it. I'd say at this point, just being real with a couple friends and still believing and taking bets is the hard path. I personally learnt it through shit tonne of mistakes, self-doubt and pain and tears that 2 things in life mattered - I was working on something I cared about, and I was around people I loved. I follow this principle to date. Not caring about a lot of external stimuli is a superpower you can acquire by just self-assessing.Semester 6: You're growing now. Tension is building up and you're probably expected to have figured out life. You don't. In fact it's a good thing if you've failed over 3-4 times by now - it means you're trying and getting back-up enough. Having a vague idea of how you want to spend time - that's something I'd recommend having. A good idea here is take up something, and no matter how much it pains, see it through completion. Working with Logistics Lab, IIT Madras and Coursepanel taught me that enduring and reaching the finish line is way more important than procastinating for perfection or giving up midway. That's a major differentiating factor too.Semester 7: You're probably multi-tasking at this point. Become boring, routine-driven - use a calendar and documentation tools to string multiple tracks together. Write a lot - it helps unclutter the mind and think clearly. I know it's boring but having control over how your time, energy and money flows by writing stuff down goes a long way. It helps aim long for a year and piece it down to what you and your team need to do today. I learnt this while helping Institute Technical Society (TechSoc), IIT Madras frame its vision from to 1700+ participants, 14 events and countless projects. I learnt that small tasks with directed effort moves mountains.Semester 8: It's your turn to give back now. Now your old friendships will blossom and it's time to be ready for new ones. Be there as a mentor your junior self would have appreciated. Preferably take up a position of responsibility that's undervalued and difficult and lead a team through and through, no matter what it costs. I think the Inter IIT Technical Meet at Indian Institute of Technology, Kanpur where we went from stranded on a railway station to nailing medals in Kanpur (2000 miles away) in under 40 hours.Most lessons are not my own and have been taught to me by amazing amazing mentors, friends, family members and I'm undeniably grateful to.This blog won't solve your problems - hell, mine also took a fuckload of meanderings before I got to understand them. Think of it as more like - not repeating my fuckups. College is a mix of confusion and opportunities, wins and losses - in a short time frame so screw everything and feel free to ping me anytime if you're going through something and need a ear. Listen to all this - but the best part is - don't do it like me, do it like you. üòâOle ole Alakananda !Featured videos for funzzForced guitar jamming on grad niteOle ole alakananda - is your hostel this cool?3 idiots, 1 car and some sunshine - roadtrip
The One-size-fits-all ML-project template?,A problem, framework, budget, library, language agnostic approach to ML,Machine Learning projects may seem complicated at first glance, but in reality, they follow a systematic flow that, once understood, makes the whole process quite straightforward. In this blog, we will provide a comprehensive guideline to help you navigate your machine learning projects smoothly.Let's first sing a song to kickoff this blogIn a nutshell, what are we gonna talk aboutIt's 6 basic steps - no matter what ML project or framework you're working with - it's usually always 6 steps. In this section, I'll be covering them in short but moving ahead, I'll share resources for templates and tricks for each step. Let's hear em out!Steps of ML0. Become One with the DataThe initial step to every machine learning project is understanding your data. Make sure you visualise your data in as many ways as possible to gain insights. This could include checking the distribution of your data, identifying potential outliers, and understanding the relationship between different variables.1. Preprocessing Your DataRaw data is rarely ready for a machine learning model. You must preprocess your data to optimize it for learning. This may include tasks such as cleaning, normalization, and standardization of values.When dealing with large datasets, it's common to divide them into smaller subsets known as "batches". A batch is a subset of the dataset that the model looks at while training. This is done because processing large datasets might not fit into the memory of your processor, and trying to learn patterns in the entire dataset in one go could result in the model not learning very well.2. Creating a ModelStarting with a baseline model is an excellent approach for any machine learning project. A "baseline" is a relatively simple, existing model that you set up when you start your machine learning experiment. As you progress, you try to beat the baseline by experimenting with more complex or suitable models. Here you can explore the State of the Art algorithms to identify suitable models for your project.3. Compiling and Fitting the ModelOnce you have decided on your model, the next step is to compile it. This step involves defining the loss function, optimizer, and the evaluation metrics. After compiling, fit your model to the training data, allowing it to learn the underlying patterns.4. Evaluating the ModelAfter your model has been fitted, it's time to evaluate its performance. Use your test data to evaluate how well the model generalizes to unseen data. This will give you an idea about how your model might perform in real-world scenarios.5. Hyperparameter TuningIn order to improve the model's performance, you will need to tune its hyperparameters. This process involves experimenting with various values for different hyperparameters to find the combination that provides the best performance.Remember, overfitting and underfitting are two crucial aspects to consider during hyperparameter tuning. If your model is overfitting, consider using techniques such as data augmentation, regularization layers, or increasing your data. These techniques can help your model generalize better.6. Iterate Until SatisfiedThe above steps are not a one-time process. You might need to iterate over them several times before you achieve a satisfactory model performance. Keep experimenting until you have optimized your model.8. Explore Advanced Techniques for ImprovementOnce you have beaten your baseline, you can start exploring advanced techniques to further improve the performance of your model. This could involve adding more layers to your model, training for a longer period, finding an ideal learning rate, using more data, or using transfer learning.Deep Diving into templates and tricksProcessing the DataThe first step in any machine learning project is to preprocess your data. Data augmentation and data shuffling are critical aspects of this stage.Data augmentation is a technique used to increase the diversity of your data by making slight modifications to it, thereby enabling your model to learn more generalizable patterns. This technique is especially effective in image data, where actions such as rotating, flipping, and cropping an image can help diversify your dataset.Shuffling data is essential to disrupt any inherent order that may exist in your dataset, such as all examples of a particular class clustered together. Shuffling ensures your model is exposed to various classes during training and avoids overfitting to a specific class. If your model displays erratic accuracy while training, consider shuffling your data.When processing your data, you may consider various scaling techniques to standardize your dataset. For example:StandardScaler can give each feature zero-mean and unit standard deviation.MinMaxScaler provides a non-distorting, light-touch transformation.RobustScaler is useful for reducing the influence of outliers.Normalizer provides row-based normalization, using l1 or l2 normalization.Refer to this cheat sheet for more information on preprocessing techniques.Start with a BaselineStarting with a simple baseline model allows us to have a reference point for comparing the improvements gained from enhancing our model. Fitting a baseline model typically involves three steps:Creating a model: Define the input, output, and hidden layers of a deep learning model.Compiling a model: Define the loss function, optimizer, and evaluation metrics for the model.Fitting a model: Train the model to find patterns between features and labels.Baseline models can be created using various libraries such as PyTorch and TensorFlow, based on your project's requirements.PyTorch
torch.manual_seed(42)

# Set the number of epochs (how many times the model will pass over the training data)
epochs = 100

# Create empty loss lists to track values
train_loss_values = []
test_loss_values = []
epoch_count = []

# Put data on the available device
# Without this, error will happen (not all model/data on device)
X_train = X_train.to(device)
X_test = X_test.to(device)
y_train = y_train.to(device)
y_test = y_test.to(device)

for epoch in range(epochs):
    ### Training

    # Put model in training mode (this is the default state of a model)
    model_0.train()

    # 1. Forward pass on train data using the forward() method inside 
    y_pred = model_0(X_train)
    # print(y_pred)

    # 2. Calculate the loss (how different are our models predictions to the ground truth)
    loss = loss_fn(y_pred, y_train)

    # 3. Zero grad of the optimizer
    optimizer.zero_grad()

    # 4. Loss backwards
    loss.backward()

    # 5. Progress the optimizer
    optimizer.step()

    ### Testing

    # Put the model in evaluation mode
    model_0.eval()

    with torch.inference_mode():
      # 1. Forward pass on test data
      test_pred = model_0(X_test)

      # 2. Caculate loss on test data
      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type

      # Print out what's happening
      if epoch % 10 == 0:
            epoch_count.append(epoch)
            train_loss_values.append(loss.detach().numpy())
            test_loss_values.append(test_loss.detach().numpy())
            print(f"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} ")

Tensorflow
# Now our data is normalized, let's build a model to find pattern

# Set a random seed
tf.random.set_seed(42)

# 1. Create a model
model_2 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(4, activation="relu"),
    tf.keras.layers.Dense(4, activation="relu"),
    tf.keras.layers.Dense(10, activation="softmax"),
])

# 2. Compile the model
model_2.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"]
)

# 3. Train the model
norm_history = model_2.fit(
    train_data_norm,
    train_labels,
    epochs = 10,
    validation_data = (
        test_data_norm,
        test_labels
    )
)

Plotting training and testing loss
## Plotting training and test loss

# Plot the loss curves
plt.plot(epoch_count, train_loss_values, label="Train loss")
plt.plot(epoch_count, test_loss_values, label="Test loss")
plt.title("Training and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();

Evaluating Our ModelVisualizing our model's predictions and its performance metrics is an integral part of model evaluation. Plots comparing ground truth labels and model predictions can provide insights into how well our model is performing. For regression problems, evaluation metrics like Mean Average Error (MAE) and Mean Squared Error (MSE) can be used to quantify the average errors of our model's predictions.Tracking ExperimentsVisualizing our model's predictionsApart from that, a good way to proceed is visualize, visualize, visualize!It's a good idea to visualizeThe data - What data are we working with? What does the data look like?The model itself - what does our model look likeThe training of a model - How does a model perform as it is learning?The predictions of the model -How do the predictions match up against the ground truth (the original labels)To visualize our model's predictions, it's a good idea to plot them against ground truth valuesdef plot_predictions(train_data=X_train, 
                     train_labels=y_train, 
                     test_data=X_test, 
                     test_labels=y_test, 
                     predictions=None):
  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize=(10, 7))

  # Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")
  
  # Plot test data in green
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data")

  if predictions is not None:
    # Plot the predictions in red (predictions were made on the test data)
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")

  # Show the legend
  plt.legend(prop={"size": 14});

To efficiently compare the performance of different models and configurations, it is essential to track the results of your experiments. Tools like TensorBoard and Weights & Biases can be useful in managing and visualizing your experiments' results.Improving Our ModelBuild the model -> fit it -> evaluate it -> tweak it -> fit it -> evaluate it -> tweak it -> ....Improving model accuracy is an iterative process that involves revisiting the model creation, compilation, and training stages.During the model creation stage, you might add more layers, increase the number of hidden units in each layer, or change the activation function. In the compilation stage, changing the optimization function or adjusting the learning rate could improve model performance. The training stage could involve training the model for more epochs or using a larger dataset.This process of modifying various parameters to enhance the model's performance is known as "hyperparameter tuning".Approaches to improve accuracyGet more data -> Get a larger dataset that we can train our model onHyperparameter tuning -> Use more hidden layers or increase number of neurons in each hidden layerTrain for longer -> Give your model more of a chance to find patterns in the datasetSave a ModelSaving a model is essential for using it outside the training environment, like in a web or mobile application. Models can be stored in the SavedModel Format or the HDF5 Format in TensorFlow, depending on whether you plan to tweak and train the model further in TensorFlow or outside TensorFlow. In PyTorch, the recommended method is saving and loading the model's state_dict().PyTorch code
from pathlib import Path

# 1. Create models directory 
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

# 2. Create model save path 
MODEL_NAME = "01_pytorch_workflow_model_0.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 3. Save the model state dict 
print(f"Saving model to: {MODEL_SAVE_PATH}")
torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters
           f=MODEL_SAVE_PATH)

Run on GPURunning your model on a GPU can significantly speed up training. You can check if your model is set to run on a GPU using the model.parameters().device function and set it to GPU using the model.to(device) function, where device is set to "cuda" for GPU or "cpu" for the Central Processing Unit.ConclusionIn conclusion, understanding these steps will assist you in structuring your ML projects effectively. Remember, machine learning is an iterative process, and it's okay to go back and forth between steps to improve your model's performance.
Getting started with Serverless, AWS Lambda and SAM on MacOS,Discover how to leverage the AWS Serverless Framework and AWS SAM for efficient, cost-effective cloud computing. This guide walks through the setup on MacOS, from installing necessary tools to deploying and managing serverless applications.,IntroductionAs we progress in the age of digital transformation, organizations are migrating more and more of their operations to the cloud. The Serverless Framework, an open-source project that provides a convenient way to build and deploy applications on AWS Lambda, is a perfect tool for this migration. It eliminates the need for server management, which helps to reduce costs significantly. But what is AWS Serverless, and how does it function? In this blog, we'll go over the essentials of the Serverless framework and how to set it up on a MacOS machine. We'll also introduce the AWS SAM and its uses.What is AWS Serverless Framework and How is it Helpful?The Serverless framework is a cloud-computing model in which the cloud provider automatically manages the provisioning and allocation of servers. This model allows developers to focus on their core product instead of managing and operating servers or runtimes, either in the cloud or on-premises. With AWS Lambda, you only pay for the compute time that you consume. This eliminates the need to provision and manage servers, making serverless a cost-effective solution for many applications.Installing Node.js and Python on Your MacOS MachineBefore we start with the Serverless framework, we need to have Node.js and Python installed on our machine.To install Node.js, we use the package manager npm. Open Terminal and type the following command:brew install nodeFor Python, MacOS already comes with a pre-installed version. However, it's a good idea to install a separate instance using Homebrew:brew install pythonTo verify successful installation, type node -v for Node.js and python --version for Python. You should see the respective version numbers displayed.Setting Up AWS and Serverless CLI on MacOS MachineAfter installing Node.js and Python, it's time to install the Serverless CLI. You can install it globally with npm:npm i -g serverlessUpon installation, verify the version using serverless -v command.To deploy your functions on AWS, you need to set up AWS credentials. Run the following command, replacing KEY and SECRET with your actual AWS credentials:serverless config credentials --provider aws --key KEY --secret SECRET --profile serverless-adminHow to Invoke, Deploy, and See Logs of Serverless Lambda FunctionsTo create a new service or function, use the sls create command along with the appropriate template and path. You can get a list of available templates using sls create --help command. For example, to create a Python service:sls create --template aws-python --path myServiceTo deploy this function to AWS, use sls deploy. If you want to deploy a specific function only, use sls deploy function -f functionName.To test a function on AWS, use the sls invoke -f functionName command. Adding the --log flag shows the function logs directly in the console. You can also get function logs separately using sls logs -f functionName.What is AWS SAM and How to Set it Up on MacOSAWS SAM (Serverless Application Model) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings.To install AWS CLI and SAM on MacOS, first, install the AWS CLI:brew install awscliNext, install AWS SAM:brew tap aws/tap brew install aws-sam-cliInitialize a new SAM application using sam init, choose your desired runtime (e.g., Python), and template. To build your application, navigate to the project directory and use sam build.You can invoke your function locally using sam local invoke. For an API, you can start a local API Gateway with sam local start-api.To deploy your application, use sam deploy --guided. You can then invoke it using sam local invoke "FunctionName".If you need to delete your stack for any reason, you can do so with the aws cloudformation delete-stack --stack-name SAM_APP_NAME --region us-east-1 command.You can also use SAM with the VSCode AWS Toolkit. Press Cmd+Shift+P in VSCode, click on "Add Debug Configuration" above a handler function and run Docker to start debugging.For Python packages management, you can install the serverless-python-requirements plugin by running sls plugin install -n serverless-python-requirements. This will automatically bundle required packages.Conclusion and Why Serverless is Useful for Cost-SavingAs you can see, the Serverless Framework and AWS SAM greatly simplify the process of creating, deploying, and managing serverless applications. Moreover, by leveraging these tools, we can focus more on the application's business logic rather than infrastructure management.Serverless architecture is not only efficient but also cost-effective. As there are no servers to manage, it eliminates the costs related to idle time. You pay only for the execution time of your functions, which leads to significant cost savings, especially for applications with variable workloads.We hope this guide has provided you with a clear understanding of how to get started with the Serverless Framework and AWS SAM on a MacOS machine.
A year of driving tech @ IITM,A journey of impacting 2000+ students in 365 days, 40+ projects, a 20 member insane team and 1 fricking vision,TechSoc IIT Madras was one of the most stressful and fulfilling decisions I made in my last year in institute. We upscaled the tech culture at the institute through organizing 20+ competitions, 15+ guest lectures - resulting in 22products, events impacting 2000+ students. I‚Äôd personally recommend this to any person who shares these 2 traits -passion of driving a solid tech culture at our universitywillingness to test their limits to stand for something bigger than themselvesI‚Äôll talk mainly about 6 things about my tenure as TechSoc Head from 2022 to 2023, where we captured the engagement of over 2000+ students, having conducted 15+ guest lectures and 20+ technical events.The InitiativesThe ExperienceThe GrowthThe De-meritsA crazy storyCool stuff I got to doFYI - Although this blog is all motivational stuff, we failed a tonne of times, but we did end up making people smile and make amazing things - and that's what it's all about.The initiativesMy approach was and always has been - invest in the team, then invest in the customer and then invest in the future. The fun part about this 3 step process is the more you invest in the earlier phases, you're actually also investing well enough in the latter parts.Investing in the teamThe most important part was always that each member on the team felt proud and challenged and happy about the work we did. They had to grow, they had to be happy and challenge their own problem solving skills - else it wasn't worth it.So here are a couple things I tried and liked:not the full team, some suckers missed picture dayA training periodSpent atleast a month for new joinees to get used to "entrepreneurial activities" - like studying companies, making presentations and pitching, drafting detailed reports, running collaborative projects, cold emailing, corporate relations. This helped the team learn a lot in a short period of time and gain confidence. Here's a syllabus if it helps - linkBut yeah, if you want top-quality, hard-working, smart people actually working with you on your vision - gotta train your army to fight. You can start by defining the vision of your team, and what culture you want to carry into this.Meeting structuringCollege is a complex time with lotta priorities, commitments and spontaneous plans. It is important to have a definite meeting structure so that you stay consistent and add a punctuality aspect to respect everyone's time. If anyone came late, they bought ice-cream to everyone who came before. If the meeting didn't end on time, then I would buy ice-cream for everyone in the meetingEvery meeting had a document and actionables that people could take back so we always met with a direction, discussed agenda as per time and walked away with usable data and direction. This structure is something I learned from Sharavanakumar at Khatabook and to this day, I'm grateful for it.OKRsOKRs - helping break down the overall vision into just 3 things each of the 18 of us had to get right each week - while working with each other. We were 18 disruptors - 54 things per week, pointing in 1 direction - helped us grow like crazy.Team Happiness and CultureAt TechSoc, you always get a birthday cake. If you're in person - team folks even show up for your celebration at 12AM. We ensured that in our weekly retros, we had shoutouts for everyone who did great work in that week. Every event we saved on, the surplus would go back to team treats and gifting goodies. We were transparent about our money and trusted each other with it. Apart from that, we'd enjoy late night sessions on the lawns playing games (Mafia, Truth and Dare, "Albatross") or singing or sharing stories about ourselves. Investing in the experiencegive them pizza, they will build amazing thingsEvery event at TechSoc was a collaboration with another technical team on campus. And they're usually busy with projects but do want the publicity on campus. Every meeting we had with them, went as follows:Why are we doing this event? What do we want to takeback from itWhat does the event do? What's the best date and why?How will the overall flow of conducting look like?How do we market it?What do we need (human resources and props) to run it successfully?What are some loopholes we should consider when organizing this? Metrics tracking and SWOT analysisAll this information would then be captured in a document - sample. This document would contain all the plan, the necessary links and people collaborating on it.On ground, here are some values we ensured each time we conducted the eventParticipant is god. If it means you're cycling with one hand and doing repeated samosa deliveries, sticking posters everywhere or ensuring that they have water - you gotta do it.TechSoc never gives up. No matter how much things screw-up. And they did, like, all the time. Once there was an internet issue in a seminar we were streaming while a billion dollar CEO waited on the other end. But we recouped and did the damn thingAfter-event work happens super fast. Everything from prizes, to score updation, to social media, website updating and everything. Investing in the big pictureMandakini hostel team, who made a car that did obstacle detection realtime"Always build on top of your previous work". A lot of what we did at TechSoc compounded in on itself over time. We started with arguing and justfying INR 7000 rupee budgets for paper plane events and in 1 year got to INR 1.5 lakh AI hackathons. The way to do this was:Update the fricking website - ensure that the overall presence of TechSoc was always up to date with everything that we had achieved so other people could review. With time, as we did more things, we became more awesome and it kept compounding with time. This helped get external attention and investment as well.Build extensive documentation - Every document, going in, with time, created such a strong clarity and database - everything from important contacts to event documents and plans and costs and the whole thing - just mind-blowing. It helped a lot of people collaborate and introspect and evolve work with time. Align everyone. Repeatedly - Every meeting, or party, or failure point - it was super important to just tell everyone what we stand for.The ExperienceTechnical Society is a relatively growing student body and so the job is obviously hard. The place my team had to start from was how to even get 5 people to show up for an offline event when we were reeling back from the pandemic. Key points of my experience are:the aero contingent team from our university (L to R - Akshat, Ayush, Pranit, Reddy Anna, my Alak boi, Sukriti)It‚Äôs a fricking rollercoaster - There are events that go super well and then there are others where 0 people show up. My highs were really high and lows were equally painful. I‚Äôve wanted to quit and dump so many times but then there were equally balanced nights of euphoria and adrenalineIt‚Äôs about the team- Investing in the team‚Äôs growth during the summer following my recruitment was one of the best decisions I made. Because this strong loyal team stood while we encountered multiple hits over failed early events in the first semester. And now I see the same shy-to-even-turn-on-their-video-camera students stay up 14 straight hours, jugaading food and leading the conduction of tech events on campus.Diplomacy is everything - I‚Äôve had to be nice in multiple f-you conversations and taken a lot of sit from multiple participants and stakeholders at a lot of points. Being rational more than emotional, knowing the right words to pitch to the right decision maker for an approval, collaborating with other student bodies for mutual vision alignment - that kind of thingThe tech community has heart - Tech-community is introverted, reserved and fit the ‚Äúnerd‚Äù description (barring a few outliers). But these are the same people who will pour their heart out for finessing a project - right from scribbling on whiteboards to presenting products, stand by and help each other for free, bond over late night tea conversations and pizzas, fight when they‚Äôre sleep deprived and drive real impact to push the society forward. This inspirational set of people is worth fighting forThe GrowthGigantic network - This is the most golden essence of being in an technology college. I reached out far and wide:interacted and connected with the most passionate tech enthusiasts across all domains at my universityconnected to nationwide college's top talent through a joint Tech boardbecame part of a top class university entrepreneur communitymultiple companies (GitHub, Unacademy) that sponsored our eventsBeing a better planner - Every TechSoc event had to be granularly figured out - right from the why, to the domain, to the logistics (approvals, finances, food), marketing plans, grading patterns, feedback loop and process documentation. And prior to that - figuring out which initiatives to run for which target audience and how that tied into the TechSoc vision to provide something to students that they were missing out on.TechSoc taught me about this more than anything. You have to be a better planner than you ever were when you‚Äôre handling over 5 Lakhs of budget and getting around 30-40 things right in stressful weeks.Becoming a stronger person - A leader is looked up to be a responsible and strong person. But no one is born that way. You learn by getting hit with a brick in the face. I learnt to be resilient, have more patience, lose the fear of crowd handling, stand my ground in hard negotations, take in a lot of negative feedback and deal with some failures. There were a lot of times when I felt like running away from the problem but the beauty of this PoR is that it won‚Äôt let you. Even when you‚Äôre tired, or tied by administration or resource constrained and super close to giving up. It has definitely increased my self-confidence to stand up to any problem in the world with a - ‚ÄúWe‚Äôre TechSoc, we‚Äôll figure it out‚Äù attitude.Being part of something bigger - I had a chance to stand for a strong tech culture at my college - even though it sometimes meant being unhealthily altruistic. Looking back, I‚Äôll have not regretted that I spent my last year complaining about not having a strong tech scene at univer. I‚Äôll carry with me the legacy of having led highly passionate believers who wanted to push the limits of what technology can be.The De-meritspassing out after 40hrs on non stop competition work and logisticsConstantly stressed - In the early days, especially with the offline shift and even during the competitions, I felt responsible for too many things. It‚Äôs an unprecedented level of stress - that no one should rationally take. Passion might be good reason to do it, but not otherwise. You‚Äôre worried for an event, a team member‚Äôs growth, someone resigning, some plan getting delayed and simultaneously 0 participation in another event on the next day. Since the team is small, unlike other bodies, you‚Äôre involved personally in most things and while the growth is good - the cost it comes at, is also highPersonal sacrifices - Barely any time on most weekends. I remember just bouncing across meetings post 8pm on a lot of weekdays and then standing by the team. I‚Äôve missed a lot of social life, sometimes even personal goals (heck, I didn‚Äôt work on my startup during national competitions) and relaxing, as I struggled to move the needles of 3 different verticals whilst the cores were still warming up. The role does take a lot out of you and it‚Äôs not a good thing always.A lot of ground-work - My leadership style has always been more hands-on. Might be a good or bad thing, I don‚Äôt know the balance. There are days when people won‚Äôt work upto your standards and you‚Äôll have to step in and help even though you really are just mad and might want to panic and yell at them to get it done. Compassion taught me to share the load - right from being a TShirt distributor to a sweeper to poster designer to food delivery agent.A crazy storyI'll tell a story of how I learnt persistence. In 2023, I was leading a 90 member contingent from our university to participate in Inter IIT Tech Meet (a national level competition for solving technical challenges). I was managing 15 projects over 2 months and was also in charge of the logistics, travel and accommodation in Kanpur. I booked the train tickets for my contingent through our institute and they assured us of tickets to Kanpur. However, in on the day of travel, amidst 100+ things I juggled all the way upto the train station, I realized that our tickets from Chennai to Kanpur were waitlisted. The train was full and keeping in mind the safety of my team, I told them to get off and said we'd find another way. And so, this story begins with a huge group ¬†of people, staring down at me at 12am on a railway station, with hopes that I'd get them to our competition - 2000 miles away - in the next 36 hours.I arranged for a bus back to the campus and stayed up till 3, trying to figure out how I'd get my team to Kanpur safely, with the limited budget I had. For context, our institute had a no-flight refund policy for competitions. We brainstormed multiple routes via permutations of bus and train but we'd reach late and fatigued. At 3:15am, Shreepoorna found a reasonable flight ticket deal from Chennai to Delhi. At 3:30am, I poured in all my personal savings of about INR 85000 (with a INR 10000 loan) and booked the tickets to Kanpur. Here's the celebration video of that too. the moment the tickets for kanpur were booked at 3:30am (gut feeling- all these people are going to make great founders 1 day)Next, I spent 2 hours breaking it to the 2 people, whose tickets couldn't be booked due to limited funds. At 4am, I was standing at Cauvery Hostel, getting rattled by a junior on how I'd failed my entire purpose as a leader. Returning to my room in a mess, I wrote a long email to the Dean, explaining the situtation before I passed out.After a 1 hr nap, I woke up in cold sweats, informing our Dean with a long email - the entire situation. 9am, I showed up to their office, shaking. But the admin rallied with us and with their support, got the tickets for the remaining team as well. After 2 hours we flew to Delhi. In-flight, I worked with the people solving the "entrepreneurship challenge" and got their submission right. Abhishek Gupta (on campus) was making the pitch deck all the way to 12 when we submitted it, before we finally got kicked out of the cafe we made our makeshift office, with just a single coffee order. Next, at Delhi, we haggled and arranged bus transport to Kanpur for all with a 40% discount. Turns out, back then, online bookings weren't considered valid and you had to physically haggle and get people in the bus. Fun fact, we got screwed over by the bus guys who dropped us about 25km away and once again, stranded on a highway at 7am, our team is bargaining with auto drivers to get us there.Finally, after the entire long journey, we reached Kanpur campus 10 hours before time. 24 hours after that, with a parched throat, I pitched and won the silver medal for my college at the entrepreneurship challenge. Oh and the savings I put in? I got them reimbursed a month later by tweaking around my cloud provider SaaS costs and showing AWS credits as expenditure (technically, I did lose the money). This is my favourite story that I like to share as I can neither put it in my resume nor will it be a conventional interview question - but it is so fundamental to my way of working - Persistence. I'd rather sweat in a garage than recline in a cabin. Cause where other's see unconvention, I see genius. "Ones who are crazy enough to think they can change the world, are the ones who really do"Here's another story blog I wrote about an initiative TechSoc created and ran - to motivate students to believe and build projects - linkCoolest things I got to doOrganized the 1st paper plane flying contest at my university, with having only semi-permission for the badminton courtsRevived a culture of building finished products in 10 weeks through BuildSchoolEarnt a chance to lead the Tech Contingent and shout a war cry on the stage (Actual onstage video - link)Institute Tech Team picture day, "Veer Madrasi - Aadi Kole", (April 28, 2023)Grow TechSoc from a ‚Äúwill students show up‚Äù body to a ‚Äúoh cool, they wanna sponsor our event‚Äù warrior teamConnect with all the top Center for Innovation (CFI) talent at universityInterview and interact with cool founders (Ather, Zepto) during BuildSchool talksBuy over 200 pizzas during my tenure (40 of them, during a train journey from Kanpur to Chennai)Part of Institute Innovation Council and Inter IIT Tech Board - decision making that can actually drive some changeConnected with the top startups of insti and alums - Desklamp (now Clueso), Ather, Hyperverge, CorevoiceMet my co-conspirators to build a software venture with - Prabhat, Jayanth K, Subham - made some money to.Free tshirts and appreciation goodies for the work doneSorted access to a cool DoSt util, CRC, CLTBuilt a completely automated no-code websiteRevive what I felt tech should feel like, working alongside a team I have grown to lovepaper plane flying
RAG-ging like a boss with AI,How we indexed all of AWS, GCP and Azure blogs to build a powerful AI Copilot for cloud engineers,Okay so this post is not actually about college ragging but majorly about Retrieval Generated Augmentation (RAG), a cool machine learning and indexing application I came across while building CloudPilot - "the copilot for cloud engineers"What is CloudPilot? ‚úàÔ∏è

CloudPilot was a project we built with the vision of going from idea to cloud within 10 minutes.

The goal of the project was to enable any thinker to start with a business problem, quickly to research and come up with a wholesome engineering document that had the solution approach, its architecture and performance analysis done well in under 10 minutes.
This product was built at a hackathon in under 24 hours. Let me know if there are any improvements you want me to look at.
And with that, let's dive in, into the product flow


What does a cloud engineer do?
First, let's understand how cloud architects function and come up with infrastructure solutions for a given business problem. See, as a typical cloud architect - the flow is as follows

Step 0: - It starts with a problem statement such as a architecture for payment gateway or a data migration pipeline or a high availability server fleet setup.
Step 1: Ask the right questions - Several times, we go back and forth with the problem statement to understand what are the exact business requirements and what are important aspects that we need to optimize for.
Step 2: Research - With the given requirements, the standard approach is to go online, read up some blogs by cloud providers, read case studies and open source articles to get context and understand our problem statement. Here, typically a lot of time is just spent in finding the right article. Most software problems have precedents that have solved the requirement wholly or in parts. Hence, finding the right documentation for similar requirements is of paramount importance.
Step 3: Technical Requirements - Once we have asked the right questions, we prepare a brief technical document, highlighting the most important performance expectations from the software and then start researching what kind of cloud offerings do we require. We also quantify that to what extent do we want to look at satisfying a particular goal. eg: 500 TB of storage, 80ms latency in communication, 7 0s of availability, 8GB GPUs for ML training.
Step 4: List the building blocks - Here, we simply map each techinal requirement to an existing cloud service we can use and do a comparative analysis. If it's ML, it's gotta be Google Cloud, for availability and cost, it's gotta be AWS - that kinda thing. So we start listing cloud offerings such as EC2 (AWS), Maps API (Google Cloud) or Azure OpenAI (Microsoft Azure). We think of using these offerings as builing blocks for the entire architecture and justify why the offering is important
Step 5: Join the blocks - Here, we join together the building blocks as services that communicate with 1 another and draw a systems diagram that helps to understand how the overall system works. Which part does what function and how it's role fits into the overall requirement.
Step 6: Iterate - Once an end-to-end flow is ready, we do overall Product, Engineering and Cost Analysis and then we iterate upon the design to optimize for certain aspects - make it faster, make it cheaper, increase the storage, increase the redundancy - that kind of thing. And based on that, we improve the choice of building blocks and integrating them together in 1 system
Step 7: Templatize - Once a cloud architecture is ready, we start looking at using Cloud templates such as CloudFormation by AWS, Terraform by Hashicorp to easily just deploy the entire architecture and have a skeleton of it functional to understand performance on a practical level.

Thus, this is the overall flow of the entirety of coming up with a cloud architecture. Beautiful and methodical.
So if we were to copilotize this process we would need an AI agent who could read through all cloud architecture related blogs, docs, case-studies like an engineer and intuitively go through steps 1 to 8 and help arrive upon a solution.
And that's why we used something called Retrieval Augmented Generation (RAG) - a technique where we retrieved the right list of documents from the internet and asked our Large Language Model to generate an answer from text that it studied from these docs.
Before we segway into what we did, let's study in short - what is this "Retrieval Augmented Generation" anyways?
Retrieval Augmented Generation (RAG)

Imagine you're a chef and you're trying to create a new dish. First, you go to your pantry (which has a vast array of ingredients). This is akin to the retrieval part. You take a look at different recipes you have used in the past or a recipe book for ideas. You fetch or 'retrieve' ingredients and inspiration based on these recipes.Next, you start to combine these ingredients together, perhaps in a way you've never done before. You might modify some of the recipes to match your taste or to make the dish unique. This is similar to the 'generation' part of the process.Now, let's say you have some guests coming over and you know their preferences. To accommodate them, you decide to modify your new dish even further or add some garnish to make it look more appealing. This is comparable to the 'augmentation' part, where you make some tweaks to make the final result better suited for its purpose.Bringing it back to the context of AI and natural language processing, Retrieval-Generated Augmentation is a method where you:'Retrieve' relevant data or content from a large dataset (like a chef searching through the pantry or recipe books).'Generate' new data or content based on this retrieved data (like a chef creating a new dish using the fetched ingredients), and then'Augment' this generated content, usually by making some modifications to better suit the task at hand (like a chef adjusting the dish to suit the guests' taste).This method is very useful for tasks that involve generating text or other types of data, especially when you want to produce content that is tailored for a specific task, purpose, or audience.It's also a way to improve the performance of machine learning models, by providing them with a richer, more varied dataset to learn from. The richer and more varied the 'ingredients' (i.e., the data), the better the 'dish' (i.e., the model's output) can potentially be.
How we built the damn thing
Part 1: "Retrieve"
For this we would leverage the open-source documents, blogs, systems diagrams and articles by major Cloud Providers such as AWS, Google Cloud and Microsoft Azure.
We created a custom index of just these documents. Using Amazon Kendra, we indexed over 192000 such information webpages using its web-connector. Here, the major difficulty we faced was understanding which XML sitemaps did we have to index and how. Some documents were in Japanese and some were just plain irrelevant marketing content. So intelligent document structure filtering was required at this level to index the right documents for our search.
Thus, this helped us to get the best documents to pull in for a given query. For example the queries below.
Using Kendra for getting top documents for a given search queryPart 2: "Generate"
Alright so you've got the top 10 documents you're supposed to refer to. But how can we improve the experience of gleaning valuable insights for your given business problem? That's where we use the superpower of an LLM. It's like asking it to read all documents and answer over it
So, once we "retrieve" the documents from Amaz√∏n Kendra, we ask the LLM to take the top 5 most relevant documents, pass all the text in them as tokens to the LLM (number of tokens varies depending on the LLM, we used GPT-4 by OpenAI that takes in upto 8000 tokens). The LLM parses the text and via few-shot learning over the context of business problem and the documents from Kendra, "generates" a response for the end user.
This is the simple task. Get the relevant docs and then generate a textual response from the same. Let's improve the experience now.

Part 3: The end-to-end architecture
Stringing together this in an entire experience involved developing the following flow.

Get the input prompt for the business requirement from the user and generate a set of relevant questions about the business to ask the user. For this we used an LLM




We recorded the question answers and the business requirement, did some text summarization and trimming (using nltk to remove whitespaces, punctuations etc.) and then we pass it all as a cumulative context for searching best documents in Amazon Kendra


Next, we get the best docuements from Amazon Kendra as text, combine that with business prompt and QnA and pass it to our LLM for giving a textual response


Once we get the response, we return that, along with hyperlinks to the documents that Kendra returned so the user can get our summarized,structured answer as well as refer the source documents for more information at their own leisure


To help the cloud engineer with more specific information, we added a feature to generate the following from the textual answer
6. Systems diagram for the architecture proposed to understand how to link together the building blocks
7. IaaC code template - Basically a CloudFormation code template to quickly just spin up the architecture and study its performance and behavior in cloud
8. Cost estimation - based on information that the LLM has been trained on, using AWS pricing docs as data source


Lastly, we provided a chat interface for the user to keep passing more requirements (such as "make it cheaper?", "make the system faster?") to the service. Now, we pass a combination of these 4 things to the LLM to get an improved answer = original business prompt + QnA + generated answer + additional user request



What can we improve upon?
Here's the current complete system that was built:

Okay so this project was awesomely exciting but the main complaint I have from it is that it is too slow. It still takes 2 minutes to generate answers and diagrams. A way to improve that would be some compute on frontend or some level of caching
Also, the answers are still generic and unless the prompt is well-engineered, we wouldn't discover novel architectures and solutions. So, that's something we need to fine-tune for. Maybe some sort of a framework for the user to engineer their prompt even better. Asking better technical requirement questions for a given business prompt could be a good starting point.
Lastly, it's expensive. Kendra is expensive and so is using OpenAI. So the approach for solving this is - open-source it! A search index with Elasticsearch combined with a custom LLM such as the Llama, Lima and Flan-T5 models can help cut down on costs
Oooh, a cool feature could have also using voice to speech and language translation (with using localized cloud services docs) to provide multilingual support to our beloved copilot
In closing....
We're having a copilot for pretty much everything these days ever since the Generative AI hype cycle started so I just thought of creating something that could help engineers to better adopt the Cloud Services by any cloud and help leverage them in the best possible way
Here are some useful reference links:
Building a Retrieval Augmented System using an LLM
Indexing web documents using Amazon Kendra
Let's see how fast you get from 0 to cloud.
Coursepanel: My 1st startup,What I innovated and struggled as I tried to build my 1st software business,üí°I'd rather sweat in a garage then recline in a cabinThe story for this starts with wanting to figure out how a student can get from point A in skillset to point B in skillset (their desired ambition) - given an educational ecosystem that provides certain opportunities. And we did realize it, atleast for 3000+ students in my undergraduate college.I ran Coursepanel for a year and we built several cool things in this journey. I'll keep this blog fairly technical but also throw in certain snippets about entrepreneurship and lessons learnt along the way. Most of this blog is going to be about the technical implementation of this project. I have mentioned my learnings as an entrepreneur in the end.To read the story of the early days and launch, checkout - hereTechnical Deep-dive
Understanding Natural Language Processing and Vector EmbeddingsNatural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics. It involves programming computers to effectively process large amounts of natural language data. As part of NLP, Large Language Models (LLMs) like GPT-3, developed by OpenAI, are designed to generate human-like text based on the inputs they receive. They are trained on a diverse range of internet text and can generate contextually relevant text.However, they do not know specific documents or sources they were trained on and don't have the capability to access or retrieve specific documents or databases during the generation process. Their strength lies in understanding and generating text that closely resembles human-like responses, thus forming a key part of systems like CourseGPT.Vector embeddings are another crucial component of such systems. In the realm of machine learning, embeddings are a way to convert categorical variables into continuous vectors so as to feed them into a model. They can be used to capture the semantics of words in a language, relationships between items, and even user behavior in an application.Basic demo of how words are converted into vector embeddings (source)These embeddings, when stored in a vector database, form the basis of effective search and recommendation systems.Building a vector database using a knowledge graphFor the purpose of this project, based on the previous training data, we turned our knowledge graph (aka ‚Äúskillmap‚Äù) into a vector database for storing entries and answering queries related to academic data. This was done in 4 steps:Creating a Knowledge Graph in Neo4j: The first step involved modeling our data as a graph and storing it in Neo4j. Our nodes represent entities such as NPTEL courses, books, departments, etc., and relationships represented the connections between these entitiesGenerating Vector Embeddings: The next step involved generating vector embeddings from our graph. These embeddings are a form of representation that captures the semantic meaning and relationships of your data in the form of vectors in a high-dimensional space. There are several techniques to generate graph embeddings, like Node2Vec, GraphSAGE, etc. This project used the Node2Vec library in Python to achieve the same. We ran Node2Vec algorithms on our Neo4j graph to generate node and relationship embeddings.Store Vector Embeddings: The generated embeddings then needed to be stored for future use. Typically, we‚Äôd want a database that is optimized for storing and querying high-dimensional vectors. You could use a vector database like Pinecone, Faiss, or even Elasticsearch with a vector plugin for this purpose.Querying the Vector Database: The final part involved using these vector embeddings to make our application smarter. For example, we could now perform operations like semantic search, recommendations, or similarity checks by comparing vectors. This involved querying our vector database for the nearest vectors to a given input vector, which gave us the most semantically similar entities to our query.Scoping NPTEL data into the Vector Database and building a chatbotTo create a system like CourseGPT, we first need to load the relevant data into the vector database. Let's assume we have course data from NPTEL in a CSV format. This data can be processed and converted into vector embeddings using various techniques like Word2Vec, GloVe, or even using transformers-based models. These vector representations can then be loaded into the vector database, which allows us to perform efficient similarity search operations.The vector database enables us to compare the query vector (which can be a representation of a user's query or a specific course interest) with all vectors in our database, and retrieves the most similar entries. These entries represent courses that are most relevant to the user's query.Once the entries are returned, we need to translate these course vectors back into a human-readable form. This is where LLMs like GPT-3 come into play. These models can generate contextually relevant, human-like text based on the returned entries. The generated text can be as simple as a course name and description, or as complex as a detailed career path recommendation.In this way, the synergy of NLP, LLMs, and vector databases leads to the development of an effective system like CourseGPT. Such a system can revolutionize the way we approach professional upskilling and corporate training by providing personalized, contextual, and interactive learning experiences.The journey in a nutshell
The development and evolution of CourseGPT, spread over a 12-month period, could've been a significant step forward in the application of advanced technologies such as AI, Natural Language Processing (NLP), and Knowledge Graphs to the critical area of professional upskilling and corporate training.The journey began with understanding the problem domain, setting the objectives and scope, and constructing the methodology flowchart. The decision to pick a test sandbox environment of similar nature proved instrumental in aligning the development of CourseGPT closely with the real-world challenges encountered in professional learning and development.Deep dives into models of corporate training, specifically the Kirkpatrick Model, provided insights into the metrics for successful training outcomes. Skill mapping using a knowledge graph emerged as a novel and efficient approach to resolving course recommendations and facilitating an effective learning journey for the students.Working with the NPTEL team and building a skill map for IIT Madras underscored the potential of this tool in academic environments as well. Implementing Natural Entity Recognition and NLP allowed us to extract valuable course tags and derive meaningful insights from the data.Moreover, the research conducted at HyperVerge Academy (HVA) enabled a deeper understanding of the professional upskilling space. Facing and overcoming various challenges along the way, we succeeded in developing a competency engine and data model that significantly enriched the capabilities of CourseGPT.Finally, CourseGPT emerged as an intelligent product, leveraging the power of Knowledge Graphs to recommend courses based on career goals, plan academic journeys, solve student doubts, and inclusively cater to diverse learners. The journey, although filled with complexities and challenges, offered valuable insights into the future of AI-enabled learning and set a foundation for ongoing research and development.Here's the various launch videos of products that I built, just as a nostalgic flash from the past. Why I failed to run it as a company.With Coursepanel, we made around 35000 INR in revenue with 3000+ free users and then got stuck in a loop of developing without continuous customer interaction. I shut down Coursepanel in May 2023. I was embarassed, mentally disturbed and self-doubting when I did it. It was hard and it was like I had given up on a dream ¬†and failed myself. But picking up and moving on is the way forward. Here are a few things I learnt from this experience - these aren't novel learnings - these are the same fuckups most startup folks advise against but I did it anyway and learnt it again - at a heavy price:Don't do it alone. It's not that I'm doubting individual capability - it's just that context switching has a very difficult toll when you ¬†want strong velocity in both business and innovaiton. Also there's the emotional burden of doing it alone. Worst case, atleast have a friend to talk to and help sort out a lot of operations and communicationsIt's best to build a shitty product with continuous customer interaction but no point of siloed building, no matter what you feel about current systems. Even asking and then building then again asking then building - i.e. an intermittent month-to-month setup might not help. Atleast talk to users each week.Make money from the start. It's the only way to stay afloat. Don't get to emotionally attached to your innovation or amazed by exciting new things you can do. Focus on the minimum that gets you paid, get it right and then expand with bolder experiments.It's boring and requires discipline and routine to continuously outperform other teams. You have to consistently just keep pushing. Motivation is 2/3rds of this game.So yeah, I guess that's about it. I was very passionate, worked very hard and was heartbroken - but I guess I'm carrying forward a lot - something that classroom would have never taught me. Let's see, I'm about to do something again in the Generative AI space. This time - business first though - get the "what are people gonna pay for" right and then expand from that base thought into technical innovation and exquisite design.
Problem to Product,A summary of my learnings from "The Lean Product Playbook" - of moving from problem to product,Problems define marketsHaving a more accurate understanding of the market in which your product is really competingA market is not tied to any specific solutions that meet those needs. That is why you see ‚Äúmarket disruptions‚Äù: when a new type of product (solution space) better meets the market needs (problem space)The ‚Äúwhat‚Äù describes the benefits that the product should give the customer‚Äîwhat the product will accomplish for the user or allow the user to accomplish.The ‚Äúhow‚Äù is the way in which the product delivers the ‚Äúwhat‚Äù to the customer. The ‚Äúhow‚Äù is the design of the product and the specific technology used to implement the product. ‚ÄúWhat‚Äù is problem space and ‚Äúhow‚Äù is solution space.üí°What = problem spaceHow = solution spaceLean product teams articulate the hypotheses they have made and solicit customer feedback on early design ideas to test those hypotheses.Should you listen to customers?It‚Äôs true that customers aren‚Äôt going to lead you to the Promised Land of a break-through innovative product, but customer feedback is like a flashlight in the night: it keeps you from falling off a cliff as you try to find your way there.Using the solution space to discover the problem spaceHard for customers to talk about abstract benefits and the relative importance of each‚Äîand when they do, it‚Äôs often fraught with inaccuraciesThis can be solved by techniques like "contextual inquiry" or "customer discovery"The reality is that customers are much better at giving you feedback in the solution space. If you show them a new product or design, they can tell you what they like and don‚Äôt like. They can compare it to other solutions and identify pros and cons.The best problem space learning often comes from feedback you receive from customers on the solution space artifacts you have createdDivergent and convergent thinking
 You should be practicing divergent thinking, which means trying to generate as many ideas as possible without any judgment or evalua-tion. There will be plenty of time later for convergent thinking, where you evaluate the ideas and decide which ones you think are the most promising.you want to capture all the ideas that your team generated, then organize them by the benefit that they deliver.Then, for each benefit, you want to review and prioritize the list of feature ideas.You can score each idea on expected customer value to determine a first-pass priority.The goal is to identify the top three to five features for each benefit.There is not much value in looking beyond those top features right now because things will change‚Äîa lot‚Äîafter you show your prototype to customers.A user story is a brief description of the benefit that the particular functionality should provide, including whom the benefit is for (the target customer), and why the customer wants the benefit.üí°As a [type of user], I want to [do something], so that I can [desired benefit]. ¬†eg : As a professional photographer,I want to easily upload pictures from my camera to my website,so that I can quickly show my clients their pictures. ¬†üí°INVEST frameworkI - independent of other storiesN - not explicit and must be flexible, open for discussionV - valuable to customerE - reasonlaby estimate scopeS - smallT - testable ¬†Small tickets and smaller ticket batch sizes are better
Break tickets into atomic chunks. Trim extra stuffThe batch size is the number of products worked on together at the same time in parallel. Faster velocity of small batches -> faster feedback -> reduce risk and wasteThe longer you work on a product with-out getting customer feedback, the more you risk a major disconnect that subsequently requires significant rework.Scoping with story points
Story points - type of currency for estimating relative ticket sizeHave a max threshold for story pointsGood operating principle ¬†- break down stories with high points into smaller storiesReturn on investment to prioritize
When you are building a product or feature, the investment is usually the time that your development resources spend working on it, which you generally measure in units such as developer-weeks (one developer working for one week).It‚Äôs true that you could probably calculate an equivalent dollar amount, but people use units like developer-weeks because they are simpler and clearerVisualizing ROI
Good product teams strive to come up with ideas like idea G in Figure 6.1‚Äîthe ones that create high customer value for low effort.Great product teams are able to take ideas like that, break them down into chunks, trim off less valuable pieces, and identify creative ways to deliver the customer value with less effort than initially scoped‚Äîindicated in the figure by moving idea G to the left.The accuracy of the estimates should be proportional to the fidelity of the product definitionThe main point of these calculations is less about figuring out actual ROI values and more about how they compare to each other ¬† Approximating ROI and Deciding on MVP candidate
Just comparing feature importance wrt one another. Once that‚Äôs done, arrange all features in this format as per benefit categoriesIntentionally give them generic names so that you can more easily envision replacing them with what would be relevant for your product. ‚ÄúM1A‚Äù means feature chunk A for must-have 1. ‚ÄúP2B‚Äù means feature chunk B for perfor-mance benefit 2, and ‚ÄúD2C‚Äù means feature chunk C for delighter benefit 21.Decide on the MINIMUM SET OF FUNCTIONALITY that will resonate with TG usersLook down the leftmost column of feature chunks and determine which ones you think need to be in your MVP candidate. While doing so, you should refer to your product value propositionAfter this, focus on the main performance benefit you‚Äôre planning to use to beat the competitionDelighters are part of your differentiation, too. You should include your top delighter in your MVP candidate.So if you‚Äôve made tentative plans beyond your MVP, you must be prepared to throw them out the window and come up with new plans based on what you learn from customers.Moving on to MVP and early Prototyping
Use the broad term MVP ‚Äúprototype‚Äù to capture the wide range of items you can test with cus-tomers to gain learning. While the first ‚Äúprototype‚Äù you test could be your live MVP, you can gain faster learning with fewer resources by testing your hypotheses before you build your MVP.What is/isn't an MVP?There has been spirited debate over what qualifies as an MVP. Some people argue vehemently that a landing page is a valid MVP. Others say it isn‚Äôt, insisting that an MVP must be a real, working product or at least an interactive prototype.The way I resolve this dichotomy is to realize that these are all methods to test the hypotheses behind your MVP. By using the term ‚ÄúMVP tests‚Äù instead of MVP, the debate goes awayProduct vs Marketing MVP TestsMarketing test - You‚Äôre simply describing the functionality to prospective customers to see how compelling they find your descriptionMVP Tests - validate your product will involve showing prospective customers functionality to solicit their feedback on it. You may be showing them a live beta product or just low fidelity wireframes to assess product-market fitMarketing tests can provide valuable learning, but they‚Äôre not an actual product that creates customer value. At some point, you need to test a prototype of your MVP candidateQuantitative vs Qualitative MVP TestsQualitative - you are talking with customers directly, usually in small numbers that don‚Äôt yield statistical significance eg: If, for example, you conducted one-on-one feedback sessions with 12 prospective customers to solicit their feedback on a mockup of your landing page, then that would be qualitative researchQuantitative - conducting the test at scale with alarge number of customers. ¬†You don‚Äôt care as much about any indi-vidual result and are instead interested in the aggregate results.eg: If you launched two versions of your landing page and directed thousands of customers to each one to see which one had the higher conversion rate, then that would be a quantitative test.Quant = what, how; Qual = whyQuantitative tests are good for learning ‚Äúwhat‚Äù and ‚Äúhow many‚Äù: what actions customers took and how many customers took an action (e.g., clicked on the ‚Äúsign up‚Äù button). But quantitative tests will not tell you why they chose to do so or why the other customers chose not to do so. In contrast, qualitative tests are good for learning ‚Äúwhy‚Äù: the reasons behind different customers‚Äô decisions to take an action or not.Matrix of MVP Tests
Hello OKRs,,üí°If you don't know where you're going, you might not get thereObjective Key-Result (OKR) is a framework of approaching a goal/project that ensures that you and your team make 1000s of decisions and steps, while being aligned on a common vision.This blog will largely explore the key learnings I've gathered from this awesome book on the topic called "Measure What Matters" by John Doerr. If you're curious, this person learnt the OKR framework from top management at Intel and replicated successfully, the same principles - at Google and grew the young startup at breakneck speed with people and products - whilst retaining the core vision of making information easily accessible to all.The very 1st slide John showed to team Google said OKRS: "A management methodology that helps to ensure that the company focuses efforts on the same important issues throughout the organization"I'll litter this blog with excerpts and stories that the book explored and I found most helpful.OKR short intro
Objective is the WHAT is to bo achieved. These are significant, concrete, action oriented and ideally inspirational.
Key Results benchmark and monitor how we get to the objective. Effective KRs are specific and time-bound, aggressive yet reliastic. Most of all they're measurable and reliable.
Where an objective can be long-lived, rolled over for a year or longer, key results evolve as the work progresses. Once the KRs are completed -> the objective is achieved
Thus, OKRs channel efforts and coordination. They link diverse ops, lending purpose and unity to the entire org. This is supremely important at startups or young companies where people absolutely need to be pulling in the same direction
The maker of OKR
Andy Grove, an Intel exec is credited for having created the management system of OKRs when Intel had taken up the responsibility of Operation Crush - a race to win in the 16-bit microprocessor market with their 8086 product in a time where cheaper and less performant products were eating market share.
It was the glorious story of how Intel dumped the business of memory-chip and went all-in on microprocessors and completely "crushed" every other contemporary compute company.
Here, are the 1st tenets of this system, by Dr. Grove himself:

Less is more
Set goals from the bottom up
No dictating
Stay flexible
Dare to fail

Here's a free OKR template for your use - link
Learning from Unusual Billionaires,Learning from 7 unconventional stories - how a lasting, profitable business is to be built,üí°Great companies take the surplus cash flows created by their sustainable competitive advantages and then either return those to shareholders or reinvest those in their core franchise or - and this is the litmus test of greatness - reinvest those successfully in new activities or markets"Unusual Billionaires", by Saurabh Mukherjea, is an unusually riveting & practical thesis. It is a collection of stories of brilliant companies that made their dent in the world by continuously innovating, providing great service, and building powerful teams & stable assets. It teaches us the unconventional yet core fundamental process of sustainably building great companies amidst this era of venture-backed rocketship dreams.In this blog I'll be listing snippets from the book, that teach us all, the values of how great companies our built and share my thoughts on the same. ¬†In this market of the recession of fly-high crashes of crypto exchanges, the gloom of overpowering AI agents & plummeting stock prices, I think we can all take a page out of these learnings & realize how lasting services are created.Asian paintsGreatness is not a function of circumstance. Greatness, it turns out, is largely a matter of conscious choice. ~ Jim Collins (Good to great)An empire that cannot sustain a blow and remain standing is not really an empire ~ Yuval Nova HarariA great company attracts the best talent, commands respect in the business community and, more often than not, trades at a premium in the stock marketGreatness is meant to be an enduring quality. Stock prices are an effect and not a cause of a company‚Äôs greatnessMetrics in a great companyRevenue growth of 10% and ROCE of 15% every year for non-financial services companiesLoan book growth of 15% and ROE of 15% every year for financial services companies‚ÄúWe have consciously stuck to a sector which is growing and offers decent potential and have always strived for excellence across all functional areas. But it is not because somebody is observing us and is going to applaud us. We have just stuck to our knitting sensibly as the sector has evolved over decades - Jalaj Dani (promoter)By obsessively using tech and talent to improve on internal processes and efficiencies, these companies put considerable distance between themselves and the competitions. In particular, this focus on efficiency and cost reduction helps the champion companies sustain high levels of profitability in a way that most of their rivals simply can‚Äôt matchCreate and nurture an environment in which a multitude of talented minds work in harmony so that mutual competence is reinforcing rather than debilitatingIBAS framework - Innovation, Brands & rep, architecture, strategic assetsInnovation - shopkeeper would get 3.5% extra discount, if, without fail, he made payments on time throughout the year. adoption of computers & GPSBrands & rep - Asian Paints changed their messaging to maintain an emotional connection with their target audience. Thus came the brand ambassador of a mischievous kid with a can of paint (name - ‚ÄúGattu‚Äù)Architecture - Creating a truly independent board of directors to help shape evolution of the firm. Rapid career progression for those who perform with job rotation at reg. intervals.Empowerment allows creativity. It‚Äôs about the feeling of being a family - Jalaj DaniStrategic assets - sources of competitive advantages - intellectual property, licences, regulatory permissions, access to natural resources and natural monopoliesOn the trust Asian paints built - ‚Äú50% of dealers won‚Äôt check their accounts, despite having access to a web portal which gives them real time updates on their accounts‚ÄùBergerThe most difficult thing to change at a company is to change its peopleDon‚Äôt recruit someone who is not able to do a job, just to fill a vacancy. Secondly, even if we don‚Äôt have a job, but you come across the right person, you must recruit himIn FY13, talent and skill are increasingly becoming scarce and it requires considerable effort to identify, engage and retain such talents. The report also added that salary alone is not the criteria for satisfaction of deserving employees and that the firm needs to offer a participative work environment and an open cultureMaricoI strongly believe that culture can be a source of competitive advantage in an org and it is impossible to copy. The org‚Äôs culture is a major driving force in the execution of strategy. Correct culture helps in proper execution of strategy by helping everyone align on the same page (Mariwala)A promoter supporting and listening to the board, although being a 60 percent shareholder, makes indep directors feel proud because we think that we are able to contribute to Marico. These things don‚Äôt reflect directly on the balance sheet, P&L or share price, but these are inputs which go into creation of long-term shareholder wealthPageMany companies justify risking balance sheet & profits for fueling growth. However, Page has maintained steadfast financial discipline whilst expanding its operations and at the same time, its share prices have also kept up an upward trend‚ÄòOur managers understand that it‚Äôs not about having power but about how you empower your subordinates & bring about the best results from your team members. Hence each member behaves like a leader and that gives us a winning culture - Sunder Genomal‚ÄòFrom day one, Mr Genomal has pushed us into adopting a process-oriented approach. Almost everything we do at Page is part of an Excel sheet.‚ÄòWe want to keep our distributors‚Äô balance sheets healthy because our business is dependent on their finances. If they have the wrong inventory stuck with them, it neither helps us nor themEven the core projects will not be taken up unless they promise an ROCE of atleast 20%Axis bankHemant Kaul on Supriya Gupta - My offer letter from Axis Bank had no details about compensation. But the temptation to start a new bank with Gupta overshadowed all my inhibitions - On joining Axis‚ÄúWhen you‚Äôre in a commodity business, the only way to thrive is to be a low-cost producer. And when you‚Äôre selling money, you‚Äôre in a commodity business - Duff McDonaldShikha Sharma - I advise other companies‚Äô CEOs not to fall into the trap where you go - Where‚Äôs the growth? Where‚Äôs the growth? Where‚Äôs the growth - They feel a tremendous pressure to grow. Well, sometimes you can‚Äôt grow. Sometimes you don‚Äôt want to grow. In certain businesses, growth means you either take on bad clients, excess risk or too much leverage - Jamie Dimon (CEO of JPMC)The shift from people to processes was critical as the bank was moving away from reliance on corporate banking to retail banking (which needs very high use of data & analytics based decision making to grow the business whilst maintaining commercial viabilityThis operating structure makes the CEO of a pvt bank both less powerful than a promoter of a non-bank company but more influential than the CEO of a non-bank company (pg 209)Employee policies are very consistent at Axis Bank and there is little ambiguity and subjectivity in the pay hikes and bonuses. All supervisors have to clearly explain the ratings they assign to their subordinates - something that decides pay rise, promotion & bonus. You won‚Äôt find this in other pvt sector banks, where a lot of randomness prevails in salary hikes, bonuses & promotions where lower rated employees have got higher bonusesHDFC bankBuilding a visionary company requires 1% vision & 99% alignment - Jim CollinsAn org‚Äôs ability to learn & translate that learning into action rapidly is the ultimate competitive advantage - Jack WelchHDFC Bank‚Äôs strength rests not so much on the uniqueness of tech it uses but in the way that tech is lined up in a clever process flow i.e. not envisaged by other banks.Strong focus on systems & procedures meant that the people who succeeded them did not find it difficult to take over responsibilities quicklyAstralWe came to a point where our day to day business was to just survive. Sandeep EngineerWe had to build scale. We made the market for CPVC in India and if we didn‚Äôt increase capacities aggressively, someone else would have borne the fruit of our hard work. Sandeep EngineerInstead of copying foreign technologies, Engineer forged technical partnerships with global majors to launch innovative products in IndiaHis biggest strength is simplicity and humility. There has been no change in his personality even after the manifold growth of Astral. He still drinks tea from a roadside stall along with his dealersEngineer would tell plumbers not to see themselves as plumbers, but as doctors who treat the big problem of leaking pipes in homesGreatness is not Everyone‚Äôs Cup of TeaStrategy is about making smart choices and execution is about relentless implementation ~ Phil RosenzweigThe business of business is a lot of little decisions every day mixed up with a few big decisions ~ Tom MurphyThose who achieve long-lasting success in India are often those who are unflashy , introverted, determined, and intelligently tenaciousRahul Dravid‚Äôs of the business world - rare, determined and constantly seeking to improve the edge or the advantage they enjoy vis-a-vis their competitorsWhat makes a great company:Theme A - Focus on the long term (more than ten years) without being distracted by short-term gamblesTheme B - Constantly deepen the moat around the core franchise using the IBAS frameworkTheme C - Sensibly allocate capital whilst studiously avoiding betting the balance sheet on expensive and unrelated foraysAbility to steadfastly focus on doing the same thing for decades on end whilst gradually strengthening their franchise, generating surplus cash from that endeavour and then giving much of that back to shareholders whilst reinvesting the rest judiciouslyRam Charan - Leaders who say ‚ÄúI have got ten priorities‚Äù don‚Äôt know what they‚Äôre talking aboutJim Collins(Great by Choice, 2011) - 20 Mile March requires hitting specified performance markers with great consistency over a long period of time. It requires 2 distinct types of discomfort, delivering high performance in difficult times and holding back in good timesGood management teams work on proving a concept before investing a lot of capital. They are not likely to put a lot of money in all at once, hoping for a big payoff - Michael ShearnFirstly, diversifications if any, are consciously restrained, instead of breaking or stretching the balance sheet. Secondly, surplus cash, when available, is returned to shareholders when in can‚Äôt be effectively deployed without dragging the ROCE down sharply. Whilst all of this might sound straightforward, these axioms are rarely implemented by Indian promoters who all too often treat their listed companies as their personal fiefdoms and short-change the minority shareholdersA company‚Äôs profitability increases as it moves up the value addition chainCompetitive advantage - if value added is the difference between the cost of a company‚Äôs inputs and that of its output, then competitive advantage allows a company to add more value than its rivals. Furthermore, a sustainable competitive advantage enables a company to continue sustaining this extra value addition over long periods of timeUnless innovation becomes a process which a firm lives day in and day out, the firm will appear impressive initially, and then ultimately disappoint. A product can be quickly outdated, but a successful brand is timeless
Twitter 2.0 at the Back of an Envelope,Let's understand how we can do back of the envelope calculations, and view the Twitter's engineering challenges from the eyes of its decision makers.,What is Back of the envelope?A Back of the envelope calculation is a simple arithmetic operation frequently carried out on a little piece of paper, like an envelope. Back of the envelope analysis quickly generates a ballpark figure using approximated or rounded figures. An estimate is more accurate than a wild guess but less exact than a formal calculation using precise numbers in a spreadsheet or other program. These computations are typical for folks who have a sudden inspiration for an idea or wish to estimate a quantity quickly.In honor of physicist Enrico Fermi, back of the envelope computations are often referred to as Fermi issues. Fermi was renowned for his ability to approximate issues accurately even with very little data.In this article, we will talk about this methodology's history, applications, and best practices. Furthermore, I will provide a storage cost analysis of Twitter 2.0 and a cheat sheet at the end so that you can conduct your estimations. Just a disclaimer - the calculations that are about to be added, are made with heavy assumptions - looking at Twitter 2.0 as a new service that we‚Äôll be building & basic components required for the same. The calculations don‚Äôt involve considerations of economies of scale, redundancy, replication and most estimations that involve scaling. We are just understanding a basic implementation of storage for the app, for the scale it is itTwitter 2.0‚Äôs System Design diagram shared by Elon Musk in November 2022 - SourceWhy are back of the envelope calculations needed?The calculation generally informs us if the design can satisfy the functional requirements, such as the number of supported users, response latency, and resource needs. For instance, when deciding between different architectural designs, the simplest question to ask is whether one server is enough. To get a ballpark figure, we can quickly calculate how many servers we would require, if not one.Notable Industry ApplicationsSome of the use cases where back of the envelope calculation has been used:An important Internet protocol, the Border Gateway Protocol, was sketcBeforein 1989 by engineers on the back of "three ketchup-stained napkins", and is still known as the three-napkin protocol.UTF-8, the dominant character encoding for the World Wide Web was designed by Ken Thompson and Rob Pike on a placemat.The Bailey bridge is a type of portable, pre-fabricated, truss bridge and was extensively used by British, Canadian and US military engineering units. Donald Bailey drew the original design for the bridge on the back of an envelope.The Laffer Curve, which claims to show the relationship between tax cuts and government income, was drawn by Arthur Laffer in 1974 on a bar napkin to show an aide to President Gerald R. Ford why the federal government should cut taxes.We've looked at some of the important historical applications of the back of the envelope, now we'll examine the arithmetic process to see how we can utilise it to solve our own issues. This will help you retain this knowledge as well as expand your understanding of how these calculations work.Getting Started with Back of the EnvelopeWhenever we try to learn something new, we have to be well-versed with the most basic & frequently used tools. Similarly, when beginning system design estimations, we have to know certain common numbers that help us consolidate our understanding. A subset of these numbers, used for estimating the speed of a system, are known as ‚ÄúLatency Numbers.‚Äù Latency, as a term, helps measure the time it will take for data to pass from one point in a network to another. It is measured in milliseconds for most systems. For example, the latency of the best video chat applications, such as Google Meet is 100 milliseconds or less. This means it takes 100 milliseconds or less for data to be exchanged between the video chat participants‚Äô computers.How can we implement this?Before approximating the real utilization, we must acknowledge that our system has limited resources. For instance, we want to know if we can handle all user requests on a single server despite our servers only being equipped with 2GHz CPUs.How can we guess the real usage?The utilization must be divided into its component parts before being roughly estimated (and, if necessary, further divided) and combined.For instance, we might anticipate 1K active users, each of whom submits 15 requests daily.15K requests per day, or 15K/86400 requests per second, are involved.An effective method for assembling the components is to round tightly. Nobody likes to multiply by 86400. To give 0.2 seconds to serve a single request, let's round to 20K/100K. We need to set up at least 4 computers if we know that it takes 0.7 seconds to serve one request. Of course, you don't want to live on the edge, so let's increase the number of machines by a buffer and make it ten.How to calculate quicklyIf someone asked you to calculate: 2456000 x 4598; that would slow things down, wouldn‚Äôt it?The simpler way to calculate quickly would be instead of spelling down a whole number, use small numbers paired with an abbreviation for magnitude (or, if necessary, exponents). 1K as opposed to 1000.When given a huge, excessively exact number, such as 7432, immediately translate it to 7K. Anyhow, you are estimating.This format for numbers makes multiplication and division quick.KK is M. [K is Kilo; M is Mega]G/M is K. [G is Giga]4K7M = 28G.Round both of them towards a small multiple of a power of 2 or 10 to work with larger numbers.2714 ~= 3010 = 3000.6500/250 ~= 6400/256 ~= 100 * 2^6 / 2^8 ~= 100 / 2^2 = 25.Where to start Back of the envelope?List the common use-cases of the system and decide which critical resources they require. You'll need a lot of storage for a document repository. Estimating document counts and sizes is a good place to start, but further usage will determine specifics. How frequently do new papers get added? The papers - can you search them? Are any indices required? Is the app more read-heavy or write-heavy?Varied use cases will probably require significantly different resource shapes. For instance, serving documents might require a lot of RAM but not CPU, whereas preprocessing new documents would require CPU but not RAM. Since you need to purchase machines that are maxed out on both dimensions, hosting all those services on homogeneous machines would waste CPU and RAM.Such differences indicate those features should be split to different services, hosted on independent sets of machines.Real-world case study - Storage Costs borne by TwitterSourceProblem: Estimate Twitter QPS and storage requirementsPlease note the following numbers are for this exercise only as they are not real numbers from Twitter.Assumptions:300 million monthly active users.50% of users use Twitter daily.Users post 2 tweets per day on average.10% of tweets contain media.Data is stored for 5 years.Estimations:Query per second (QPS) estimate:Daily active users (DAU) = 300 million * 50% = 150 millionTweets QPS = 150 million * 2 tweets / 24 hour / 3600 seconds = ~3500Peek QPS = 2 * QPS = ~7000Let‚Äôs start solvingThere are 2 important queries that need to be addressed for any system:‚ÄúRequests per second‚Äù at the server level‚ÄúQueries per second‚Äù at the database level.Let‚Äôs start with ‚Äúrequests per second‚Äù & what inputs are required to estimate its numberUsers: Twitter has 300 million active MAUs. Not every user on Twitter is an active post-maker. 50% use daily - 150 million DAUUsage per DAU: 10-25% make a tweet. Again, this doesn‚Äôt have to be exact; getting within an order of magnitude should be fine. Say each DAU, on average, makes 2 tweets: so 0.25%*2 = 0.5 tweets per DAU.Scaling factor: Usage rate for a service has peaks & valleys throughout the day. Eg: The usage rate for your favorite e-commerce store or food delivery app will heavily increase in the case of discounts or sales in a particular festive season. We need to estimate how high the traffic would peak compared to the average. This would reflect the request per second peak where the design could potentially break. Most people tweet in the morning, so the tweet creation traffic is twice the average when US east coast wakes up.Thus, total tweets in a day = 150 M DAU * (0.5 tweets/DAU) * (2x scaling factor in the morning)Thus, requests per second = n/(number of seconds in a day) = n/86400 ~ 1500 tweets per second.Let‚Äôs now look at how much storage is required for storing multimedia files for tweets.150 million daily tweets. Say 10% of these tweets contain pictures of average size 300kB each & 1% of tweets contain videos of about 50MB each.Let‚Äôs assume that the files are replicated with 3 copies each & atleast stored for 5 years.Storage required for pictures = (150 M ¬†tweets per day)* (0.1 of tweets have images)* 300kB (image size) * 3copies = 13.5TB.Storage required for videos per day = (150 M ¬†tweets per day)* (0.01 of tweets have videos) * (50MB video size) * 3copies ~ 225TB.Total storage required for multimedia files per day = 238.5TBJust the text for a tweet can be stored within 160B (i.e. 1280 bits ~ 160 characters per tweet on average) Storage required for tweets per day = 150 M * (160B tweet size) * 3copies = 72GBLet‚Äôs calculate overall storage cost for TwitterThus, we learnt that, for running Twitter, in the most basic sense, the following requirements must be met:Assuming no data compression & our simple 3x replication, in the most basic form - 238.5TB blob storage required for storing all the multimedia of tweets that arrive in a day. 72 GB storage in database is required just for storing the basic tweet content.Twitter was founded in 2006. However, it launched image sharing in 2011 & video sharing in 2013.Thus, we can assume, that average storage required per day for videos uploaded by a growing user base can be our current estimate multiplied by a normalization factor of 0.6 (considering 15% of user growth on average, every year, since year of founding).Thus, we have about (225TB video storage per day) * (365 days in a year) * (9 years since video sharing was launched) * 0.6 = 443.5PB videos stored.Similarly for image storage we have (13.5TB photo storage per day) * (365 days in a year) * (11 years since video sharing was launched) * 0.6 = 32.52PB images storedTo store total multimedia for 11 years worth tweets, in the most fault tolerant & redundant manner, we need - 443.5+32.52 ~ 476.02PB (this figure is close to the actual ballpark around 500PB that Twitter has officially shared here)Twitter uses AWS for storage of tweets & accompanying multimedia files. Say we use intelligent tiering by S3 to pay for data, that is most recent & popularly accessed. Other data, we can archive, & fetch on demand & pay for.Estimating the media storage cost476.02PB can be broken down into, say - 300 PB till 2019 (considering that video & images got introduced late) and 276PB after 2017.Assuming the worst case pricing of S3 (the standard pricing) - All the multimedia of the last 3 years‚Äô tweets can be stored in S3 regular access & the other, can be stored in S3 archives (so we save cost) Thus, as per S3 pricing values, we end up paying: For 300PB in archive: 308,073.20 USD monthly For 276PB in regular tier: 5,935,645.70 USD monthlyEvery year, our estimate of Twitter, pays 6,243,718.90 USD monthly, to store our images & videos.Estimating the database cost for our tweetsLet‚Äôs use AWS relational database DynamoDB to store tweet data in a structured form - such as user info, tweet text, number of likes & retweets.Twitter needs to store 72GB of tweets in a regular relational database, each day, to keep the lights on.Again, estimating our normalization factor of 0.6 to aggregate tweet data, across 16 years since Twitter supporting text sharing right from the start, we get - (72 GB tweet data per day) *(365 days)* (16 years since tweeting was launched) * 0.6 = 252.28TBThus, using a standard on-demand DynamoDB (unoptimized for cost, but great for scale) - the storage cost comes to 603,355.92 USD monthly, to store our data.For an actual analysis of all the databases Twitter uses, checkout this reference linkHere‚Äôs the actual AWS calculator cost estimation performed for this purpose.Thus, this most basic, unoptimized design of that we made, pays 82,164,897.81 USD = 82 million USD every year to AWS, to keep providing its storage services only.We have not even begun to discuss the computing costs, the data science costs in the cloud. Also, to keep this article a light read, we haven‚Äôt taken into consideration scalability solutions such as caching, multi-AZ databases and several aspects of storage itself.Thus, you might now understand why Elon Musk is bent on optimizing this system a lot & even implementing the ‚ÄúDeep Cuts‚Äù plan to reduce infrastructure costs by a billion. But trying to fire 50% of the workforce and have the rest leave because the ship is sinking still doesn't make sense. To top it off, if your major problem is infrastructure cost you don't fire engineers, you task engineers to reduce infrastructure costs. Now, we may not agree or disagree with this logic, but atleast we‚Äôve learnt how to ballpark the financial element of the same. Cheat Sheet for Back of the EnvelopeWe studied a way of estimating the storage cost borne by Twitter. But what about getting the speed of a video-game streaming platform? Or the requests per second of a payment gateway?Fear not, here are a few important numbers that we can use to estimate useful metrics of any software system & what corresponding technical requirements it must be able to satisfy.For estimating storageByte Number SizesImage SourceDataThe numbers vary depending on the language and implementation.char: 1B (8 bits)char (Unicode): 2B (16 bits)Short: 2B (16 bits)Int: 4B (32 bits)Long: 8B (64 bits)UUID/GUID: 16BObjectsFile: 100 KBWeb Page: 100 KB (not including images)Picture: 200 KBShort Posted Video: 2MBSteaming Video: 50MB per minuteLong/Lat: 8BFor estimating load in a given time durationThe following numbers are heavily rounded and help determine how often something needs to happen over a period of time. For example, if a server has a million requests per day, it will need to handle 12 requests per second.Use the table below to convert requests handled by server across different durations of timeImage SourceHeavily rounded per time period numbers.For estimating average database limitationsSQL DatabasesStorage: 60TBConnections: 30KRequests: 25K per secondCache: using Redis (key-value database)Storage benchmarks available in docs. To get client connections capacity- refer linkStorage: 300 GBConnections: 10kRequests: 100k per secondFor estimating average microservice limitationsWeb Servers: Requests: 5‚Äì10k requests per secondQueues/Streams - Requests: 1000‚Äì3000 requests/s. Throughput: 1MB-50MB/s (Write) / 2MB-100MB/s (Read)Example queue service limits - Pub/Sub ‚Äî limits, Kinesis ‚Äî limits, SQS ‚Äî limitsScrapers - Requests: 1000 per second. Example scraper service - Colly ‚Äî go scraperFor estimating speed/latency of the systemThese 14 latency numbers, revealed by Dr. Dean (Google), form the foundation of systems designing & performance optimization. Using these numbers we can predict how long it would take for a system to perform its computations. Refer to this website and the image below as a simple lookup for these numbersImage SourceWhat do we understand from thisMemory is fast, but the disk is slow.Avoid disk seeks if possible.Simple compression algorithms are fast.Compress data before sending it over the internet if possible.Data centers are usually in different regions, and it takes time to send data between them.In a nutshellA lot of back of the envelope calculations are done with just coefficients and exponents, e.g., c‚àó10e. Your goal is to get within an order of magnitude right that‚Äôs just e, c matters a lot less.What did we learnBack of the envelope calculations allow you to take a look at different variations.When designing your system, these are the kind of calculations you should do over and over in your head.Know the back of the envelope numbers for the building blocks of your system. It's not good enough to just know the generic performance numbers, you have to know how your subsystems perform. You can't make decent back of the envelope calculations if you don't know what's happening.Monitor and measure every part of your system so you can make these sorts of projections from real data.I hope you understood how systems are operated in terms of estimating engineering infrastructure requirements, provisions & costing. What‚Äôs the one system that you want me to target next for back of the envelope analysis?
What's a tensor & why is it in ML?,A beginners introduction to tensors & every stupid jargon people throw around them,üí°Tensor is a number/collection of numbers that maintains its meaning under transformationsüí°More complex, but correct definition - An nth-rank tensor in m-dimensional space is a mathematical object that has n indices and m^n components and obeys certain transformation rules.Each index of a tensor ranges over the number of dimensions of space. However, the dimension of the space is largely irrelevant in most tensor equations (with the notable exception of the contracted Kronecker delta).Tensors are generalizations of scalars (that have no indices), vectors (that have exactly one index), and matrices (that have exactly two indices) to an arbitrary number of indices.Tensors provide a natural and concise mathematical framework for formulating and solving problems in areas of physics such as elasticity, fluid mechanics, and general relativity.Beginner-friendlyClearing the meaning, usage and confusions about tensors in ML and mathTensors, a crucial concept in the field of machine learning, deep learning, and data science, often become a stumbling block for many learners. To fully grasp the idea, we'll break it down, explore examples, and demystify various related aspects such as rank, dimension, components, indices, size, and shape.What are Tensors?In simple terms, tensors are a generalization of scalars, vectors, and matrices. They can be seen as multidimensional arrays of numbers, with each dimension known as a rank.A scalar is a single number, a tensor of rank 0.A vector is an array of numbers, a tensor of rank 1.A matrix is a 2D array of numbers, a tensor of rank 2.A 3D array, or a "cube" of numbers, is a tensor of rank 3.The ranks continue to increase to define higher dimensional spaces.Breaking Down the Definition of a TensorThe formal definition of a tensor, "An nth-rank tensor in m-dimensional space is a mathematical object that has n indices and m^n components and obeys certain transformation rules," often overwhelms beginners. So, let's simplify it:"nth-rank tensor": Rank refers to the number of dimensions or indices the tensor has. For example, a rank 1 tensor is a vector (which has one dimension), a rank 2 tensor is a matrix (which has two dimensions), and so on."m-dimensional space": This refers to the size of each dimension. For a matrix (rank 2 tensor), this would be the number of rows and columns."m^n components": Components are the individual elements or data points that make up the tensor. For example, in a matrix, each number is a component.Simplifying with ExamplesConsider a 3x3 image with three color channels (Red, Green, Blue). It is represented as a 3D array or a rank 3 tensor, with dimensions of 3 (height) x 3 (width) x 3 (color channels). It would have 3^3 or 27 components (individual color values for pixels).Delving Deeper: Case of a 4x4 ImageLet's take a more specific example: a 4x4 pixel image with three color channels. Here, the tensor would have a rank of 3 (height, width, and color channels). The dimensions would be 4 (height) x 4 (width) x 3 (color channels), and there would be 4^3 or 64 components.The indices here refer to the position of each component within the tensor. For example, you might reference the red color value of the pixel in the second row and third column as [2,3,1], where '2' refers to the row, '3' refers to the column, and '1' refers to the red channel.The size of this tensor would be calculated by multiplying the sizes of all its dimensions together, resulting in 4 (height) x 4 (width) x 3 (color channels) = 48. The shape of this tensor would be (4, 4, 3), describing the number of elements along each dimension.Difference Between Rank and DimensionWhile both terms describe the structure of a tensor, they represent different aspects. The rank is the number of dimensions, while the dimensions refer to the size (or length) of each of those dimensions. In the case of our 4x4 pixel image tensor, the rank is 3, and the dimensions are 4, 4, and 3.Sample Log of a 4x4x3 TensorA 4x4 pixel image with three color channels can be represented as a 3D array (or rank 3 tensor). Each pixel's color is represented by a 3-element vector (for the three color channels), and the image consists of a 4x4 grid of such pixels, giving us a shape of (4, 4, 3).A 4x4 pixel image with 3 color channels could be represented as a 4x4x3 tensor, like this:

tensor = [
    [  # First row of pixels
        [230, 10, 10],    # Red-ish
        [230, 230, 10],  # Yellow-ish
        [10, 230, 10],   # Green-ish
        [10, 230, 230]   # Cyan-ish
    ],
    [  # Second row of pixels
        [10, 10, 230],   # Blue-ish
        [230, 10, 230],  # Magenta-ish
        [220, 220, 220], # Light gray
        [120, 120, 120]  # Gray
    ],
    [  # Third row of pixels
        [30, 30, 30],    # Dark gray
        [230, 50, 10],   # Different shade of red
        [230, 230, 60],  # Different shade of yellow
        [10, 60, 230]    # Different shade of blue
    ],
    [  # Fourth row of pixels
        [40, 230, 40],   # Different shade of green
        [50, 10, 230],   # Another shade of blue
        [230, 40, 230],  # Another shade of magenta
        [10, 40, 10]     # Dark green
    ]
]




Clearing the Confusion Around "Dimension"The term "dimension" can lead to some confusion. In the context of tensors, "dimension" often refers to the depth of nested lists, not the geometric or mathematical space that a vector might represent. So, a vector like [3, 4] is a rank-1 tensor (or one-dimensional array) because it involves no nesting of lists. Its shape is considered to be (2,) because it contains two elements.In summary, tensors, while initially appearing complex, can be demystified and understood through the step-by-step breakdown of their components and properties. As multidimensional arrays, they provide the foundation for manipulating data in machine learning and deep learning, making them a fundamental concept for anyone delving into these fields.AdvancedExploring more properties of tensorsUnderstanding Covariance & ContravarianceA vector can be expressed in terms of its contravariant or covariant componentsCASE 1: Expressing in terms of contravariant components ‚Üí vector described in terms of components with basis vectorsüí°We need 1 index for each of the components as there is only 1 directional indicator per component. 1 index, 1 basis vector, per componentIf we decrease the length of the ‚Äúbasic vectors‚Äù (in a vector space), then the number of components required to make up a vector increases. Because the index & the number of components change contrary to each other, they are known as contravariant components of a vector Describing a vector in terms of these contravariant components (eg : 2 i + 3 j + 4 k) is how we usually describe a vectorsCASE 2: Expressing in terms of covariant components ‚Üí vector described in terms of dot-product with basis vectorsIf we decrease the length of the basis vectors, then the dot-product decreases & vice versa. Since these properties are varying in the same way, we say that these are covariant components of a vector.Specifying the notation for covariant & contravariant componentsNow, say we take 2 vectors V & Püí°Index (for this rank 2 tensor, i.e. a vector) = unique combination of basis vectorsCASE 1: We multiply the contravariant values of V & P. On considering all possible ways, we get a matrix:[¬†V1P1V1P2¬†V1P3V2P1V2P2¬†V2P3V3P1V3P2¬†V3P3¬†¬†] \begin{bmatrix} ¬† V^{1} P^{1} & V^{1} P^{2} & ¬† V^{1} P^{3} \\ V^{2} P^{1} & V^{2} P^{2} & ¬† V^{2} P^{3} \\ V^{3} P^{1} & V^{3} P^{2} & ¬† V^{3} P^{3} ¬† ¬†\end{bmatrix} ‚é£‚é°‚Äã¬†V1P1V2P1V3P1‚ÄãV1P2V2P2V3P2‚Äã¬†V1P3¬†V2P3¬†V3P3¬†¬†‚Äã‚é¶‚é§‚ÄãThis will give us a tensor of rank 2 with 2 contra-variant index values. Here, each T value is called an element of this tensor. And since for each element, 2 indices (2 directional indicators are required) eg: Understanding by example, say we have a combination of 3 area vectors & 3 force vectors. To get the combination of all the forces on all the area vectors, for each such combination, we require 2 indices (i.e. 2 directional indicators ‚Üí 1 for the force & 1 for the area). We get 9 components, each with 2 indices (referring to 2 basis vectors)[¬†T11T12¬†T13T21T22¬†T23T31T32¬†T33] \begin{bmatrix} ¬† T^{11} & T^{12} & ¬† T^{13}\\ T^{21} & T^{22} & ¬† T^{23}\\ T^{31} & T^{32} & ¬† T^{33}\end{bmatrix} ‚é£‚é°‚Äã¬†T11T21T31‚ÄãT12T22T32‚Äã¬†T13¬†T23¬†T33‚Äã‚é¶‚é§‚ÄãCASE 2: We multiply the covariant components of P with contravariant values of V . On considering all possible ways, we get a matrix:[¬†V1P1V1P2¬†V1P3V2P1V2P2¬†V2P3V3P1V3P2¬†V3P3¬†¬†] \begin{bmatrix} ¬† V_{1} P^{1} & V_{1} P^{2} & ¬† V_{1} P^{3} \\ V_{2} P^{1} & V_{2} P^{2} & ¬† V_{2} P^{3} \\ V_{3} P^{1} & V_{3} P^{2} & ¬† V_{3} P^{3} ¬† ¬†\end{bmatrix} ‚é£‚é°‚Äã¬†V1‚ÄãP1V2‚ÄãP1V3‚ÄãP1‚ÄãV1‚ÄãP2V2‚ÄãP2V3‚ÄãP2‚Äã¬†V1‚ÄãP3¬†V2‚ÄãP3¬†V3‚ÄãP3¬†¬†‚Äã‚é¶‚é§‚ÄãThis will give us a tensor of rank 1 co-variant index value & 1 contra-variant index value[¬†T11T12¬†T13T21T22¬†T23T31T32¬†T33] \begin{bmatrix} ¬† T_{1}^{1} & T_{1}^{2} & ¬† T_{1}^{3}\\ T_{2}^{1} & T_{2}^{2} & ¬† T_{2}^{3}\\ T_{3}^{1} & T_{3}^{2} & ¬† T_{3}^{3}\end{bmatrix} ‚é£‚é°‚Äã¬†T11‚ÄãT21‚ÄãT31‚Äã‚ÄãT12‚ÄãT22‚ÄãT32‚Äã‚Äã¬†T13‚Äã¬†T23‚Äã¬†T33‚Äã‚Äã‚é¶‚é§‚ÄãCASE 3: We multiply the covariant values of V & P. On considering all possible ways, we get a matrix:[¬†V1P1V1P2¬†V1P3V2P1V2P2¬†V2P3V3P1V3P2¬†V3P3¬†¬†] \begin{bmatrix} ¬† V_{1} P_{1} & V_{1} P_{2} & ¬† V_{1} P_{3} \\ V_{2} P_{1} & V_{2} P_{2} & ¬† V_{2} P_{3} \\ V_{3} P_{1} & V_{3} P_{2} & ¬† V_{3} P_{3} ¬† ¬†\end{bmatrix} ‚é£‚é°‚Äã¬†V1‚ÄãP1‚ÄãV2‚ÄãP1‚ÄãV3‚ÄãP1‚Äã‚ÄãV1‚ÄãP2‚ÄãV2‚ÄãP2‚ÄãV3‚ÄãP2‚Äã‚Äã¬†V1‚ÄãP3‚Äã¬†V2‚ÄãP3‚Äã¬†V3‚ÄãP3‚Äã¬†¬†‚Äã‚é¶‚é§‚ÄãThis will give us a tensor of rank 2 with 2 contra-variant index values[¬†T11T12¬†T13T21T22¬†T23T31T32¬†T33] \begin{bmatrix} ¬† T_{11} & T_{12} & ¬† T_{13}\\ T_{21} & T_{22} & ¬† T_{23}\\ T_{31} & T_{32} & ¬† T_{33}\end{bmatrix} ‚é£‚é°‚Äã¬†T11‚ÄãT21‚ÄãT31‚Äã‚ÄãT12‚ÄãT22‚ÄãT32‚Äã‚Äã¬†T13‚Äã¬†T23‚Äã¬†T33‚Äã‚Äã‚é¶‚é§‚ÄãWhat makes a tensor a tensor, is that, when the basis vectors change, the components of the tensor would change in the same manner as they would in one of these objects ‚Üí thus keeping the overall combination the sameWhat is it about the combination of components & basis vectors that makes tensors so powerful ‚Üí all observers in all reference frames agree, not on the components or the basis vectors ‚Üí but on their combinations, because they are the sameIndex & Rank of tensorsüí°Rank = the total number of indices = total number of coordinate systems (we need to have knowledge of to understand the location of the tensor) = total number of unique basis vector combinations = total number of unique linearly independent combinations = The total number of contravariant and covariant indices of a tensor. The rank R of a tensor is independent of the number of dimensions N of the underlying space.A tensor does not necessarily have to be created from vector components as is shown in these examplesA tensor of rank 1 is a ‚Äúvector‚Äù and has a number associated with each of the basis vectors. Only 1 index i.e. 1 combo of basis vectors is required to know the locationHere, \($V^1 = 5, V^2 = 3, V^3 = 2$\)A tensor of rank 2 - we associate a number with every possible combination of 2 basis vectorsIn a tensor of rank 3 - we associate a number with every possible combination of 3 basis vectors - composed of the components of 3 basis vectors. We can create different descriptions of this tensor by using different contravariant & covariant components of the basis vectorsMore mathematical factsWhile the distinction between covariant and contravariant indices must be made for general tensors, the two are equivalent for tensors in three-dimensional Euclidean space (Euclidean space is the fundamental space of geometry, intended to represent physical space) , and such tensors are known as Cartesian tensors.Objects that transform like zeroth-rank tensors are called scalars, those that transform like first-rank tensors are called vectors, and those that transform like second-rank tensors are called matrices. In tensor notation, a vector v would be written $v_i$, where i=1, ..., m, and matrix is a tensor of type (1,1), which would be written $a_i^j$ in tensor notation.Tensors may be operated on by other tensors (such as metric tensors, the permutation tensor, or the Kronecker delta) or by tensor operators (such as the covariant derivative). The manipulation of tensor indices to produce identities or to simplify expressions is known as index gymnastics, which includes index lowering and index raising as special cases. These can be achieved through multiplication by a so-called metric tensor $g_{ij}, g^{ij}, g_i^j$, etc., e.g.,Tensor notation can provide a very concise way of writing vector and more general identities. For example, in tensor notation, the dot product u¬∑v is simply written u¬∑v = $u_iv^i$ If two tensors A and B have the same rank and the same covariant and contravariant indices, then they can be added in the obvious way.The generalization of the dot product applied to tensors is called tensor contraction, and consists of setting two unlike indices equal to each other and then summing using the Einstein summation convention. Various types of derivatives can be taken of tensors, the most common being the comma derivative and covariant derivative.If the components of any tensor of any tensor rank vanish in one particular coordinate system, they vanish in all coordinate systems. A transformation of the variables of a tensor changes the tensor into another whose components are linear homogeneous functions of the components of the original tensor.Tensors in the context of machine learningThe property of tensors to maintains its meaning under transformations is used also by machine learning, where real world data is transformed into corresponding tensors & then heavy calculations are done on these tensors to get valuable insights.Reference videosWhat is a tensor? - LinkCovariant & Contravariant Matrices - Link
The Next Decade of Indian Higher Education,A look into Indian Higher Education system and what its future looks like,Indian higher education will undergo rapid changes as the nation shifts from an industrial factory system to a developed laboratory of inventions, knowledge & holistic understanding. This opens up avenues for several startups in the education space to help shape the very direction of the next decade of Indian Higher EducationThis article is a combination of multiple Government articles, policies & news that helps the reader understand the initiatives, trends & estimations about various aspects of this arena.This blog comprises of 2 parts :Education - it's importance, its future & government policiesHigher Education - current problems, reforms & mindsetEducationThe Indian education system, like the Indian bureaucratic system, is Victorian and still in the 19th century. Our schools are still designed to produce clerks for an empire that does not exist anymore.Why do we need a new direction for Indian education?
Good education -> more efficient society -> more progressive nationIndia will have the highest population of young people in the world over the next decade, and our ability to provide high-quality educational opportunities to them will determine the future of our country.Many unskilled jobs worldwide may be taken over by machines, while the need for a skilled workforce, particularly involving mathematics, computer science, and data science, in conjunction with multidisciplinary abilities across the sciences, social sciences, and humanities, will be increasingly in greater demandThere will be a growing demand for humanities and art, as India moves towards becoming a developed country as well as among the three largest economies in the world.It will be increasingly critical that children not only learn, but more importantly "learn how to learn"The teacher must be at the centre of the fundamental reforms in the education system.Moving forward, what is a "good educational institute"?A good education institution is one in which every student feels welcomed and cared for, where a safe and stimulating learning environment exists, where a wide range of learning experiences are offered, and where good physical infrastructure and appropriate resources conducive to learning are available to all studentAttaining these qualities must be the goal of every educational institution.However, at the same time, there must also be seamless integration and coordination across institutions and across all stages of education.The principles of National Education Policy of India, 2020 (in simple words)Harness the best of each student - holistic developmentFoundational literacy & numeracy by all students of grade 3Flexibility in trajectory and programsNo hierarchies between curricular & extra-curricular learningLess rote learning - more conceptual understanding. creativity & critical thinkingLife skills - communication, cooperation, resilienceExtensive use of techDiversity - respect local contextSynergy in overall curriculum across all levels of educationTeacher/faculty is the heart of the learning processA ‚Äòlight but tight‚Äô regulatory framework to ensure integrity, transparency, and resource efficiencyContinuous review of progress in curriculum, researchAccess to quality education must be considered a basic right of every child;Higher EducationThe highest education is that which does not merely give us information but makes our life in harmony with all existence~ Rabindranath TagoreQuality universities and collegesAs India moves towards becoming a knowledge economy and society, more and more young Indians are likely to aspire for higher educationIt must enable an individual to study one or more specialized areas of interest at a deep level, and also develop character, ethical and Constitutional values, intellectual curiosity, scientific temper, creativity, the spirit of service, and 21st-century capabilities across a range of disciplines including sciences, social sciences, arts, humanities, languages, as well as professional, technical, and vocational subjects.üí°HEI = Higher Education InstituteODL = Open and Distance Learning (ODL) system is a system wherein teachers and learners need not necessarily be present either at same place or same time and is flexible in regard to modalities and timing of teaching and learning as also the admission criteria without compromising necessary quality considerationsCurrent Problems of Indian Higher Education ecosystemFragmented higher educational ecosystemLess emphasis on cognitive skills & learning outcomesA rigid separation of disciplines, with early specialization and streaming of students into narrow areas of study;Limited access in socio-economically disadvantaged areasLimited teacher and institutional autonomyInadequate mechanisms for merit-based career management and progression of faculty and institutional leaders;Less emphasis on research - lack of competitive peer-reviewed research funding across disciplinesIneffective regulatory systemLarge affiliating universities resulting in low standards of undergraduate educationInstitutional Restructuring and ConsolidationHEIs must become "large multidisciplinary universtities" - just like Takshashila, Nalanda, Vallabhi, Vikramshila (in our past)UNIVERSITY - A university will mean a multidisciplinary institution of higher learning that offers undergraduate and graduate programmes, with high quality teaching, research, and community engagementAutonomous degree-granting College (AC) - Multidisciplinary institution of higher learning that grants undergraduate degrees and is primarily focused on undergraduate teaching though it would not be restricted to that and it need not be restricted to that and it would generally be smaller than a typical university.Open system of accreditation - Colleges will be encouraged, mentored, supported, and incentivized to gradually attain the minimum benchmarks required for each level of ¬†accreditationIndian Government's 20 year plan -By 2040, all higher education institutions (HEIs) shall aim to become multidisciplinary institutions and shall aim to have larger student enrolments preferably in the thousands, for optimal use of infrastructure and resources, and for the creation of vibrant multidisciplinary communities. Since this process will take time, all HEIs will firstly plan to become multidisciplinary by 2030, and then gradually increase student strength to the desired levels.There shall, by 2030, be at least one large multidisciplinary HEI in or near every district. Steps shall be taken towards developing high-quality higher education institutions both public and private that have medium of instruction in local/Indian languages or bilinguallyGER should be from 26.3% (2018) to 50% in 2035 - this means that the load that systems managing enrollment data, will be taking, will doubleNOTE : Gross Enrolment Ratio (GER) : Total enrolment in primary education, regardless of age, expressed as a percentage of. the eligible official primary school-age population. in a given school-year.More growth in the public HEI sector - Fair & transparent system for determining increase levels for support for public HEIsquality online courses will be suitably integrated into curricula of HEIs, and blended mode will be preferred.Single-stream HEIs will be phased out over time, and all will move towards becoming vibrant multidisciplinary institutions or parts of vibrant multidisciplinary HEI clusters, in order to enable and encourage high-quality multidisciplinary and cross-disciplinary teaching and research across fields.Through the attainment of suitable accreditations, all HEIs will gradually move towards full autonomy - academic and administrative - in order to enable this vibrant culture.PORTING OUT THE OLD SYSTEM - "Challenge mode" between colleges - What happens to current system of "affiliated colleges" - will be phased out - they shall retain the required benchmarks over time to secure the prescribed accreditation benchmarks & eventually become ACs (autonomous degree granting colleges)NOMENCLATURE CHANGE - The present complex nomenclature of HEIs in the country such as ‚Äòdeemed to be university‚Äô, ‚Äòaffiliating university‚Äô, ‚Äòaffiliating technical university', ‚Äòunitary university‚Äô shall be replaced simply by 'university' on fulfilling the criteria as per norms.More holistic & multidisciplinary approach
This notion of a ‚Äòknowledge of many arts‚Äô or what in modern times is often called the ‚Äòliberal arts‚Äô (i.e., a liberal notion of the arts) must be brought back to Indian educationRESEARCH ON MULTIDISCIPLINARY EDUCATION - ¬†Assessments of educational approaches in undergraduate education that integrate the humanities and arts with Science, Technology, Engineering and Mathematics (STEM) have consistently showed positive learning outcomes, including increased creativity and innovation, critical thinking and higher-order thinking capacities, problem-solving abilities, teamwork, communication skills, more in-depth learning and mastery of curricula across fields, increases in social and moral awareness, etc., besides general engagement and enjoyment of learningA holistic and multidisciplinary education, as described so beautifully in India ‚Äôs past, is indeed what is needed for the education of India to lead the country into the 21st century and the fourth industrial revolution. Even engineering institutions, such as IITs, will move towards more holistic and multidisciplinary education with more arts and humanitiesImaginative and flexible curricular structures will enable creative combinations of disciplines for study, and would offer multiple entry and exit points, thus, removing currently prevalent rigid boundaries and creating new possibilities for life-long learningFlexibility in curriculum and novel and engaging course options will be on offer to students, in addition to rigorous specialization in a subject or subjects. This will be encouraged by increased faculty and institutional autonomy in setting curricula.Increase industry exposure - students at all HEIs will be provided with opportunities for internships with local industry, businesses, artists, crafts persons, etc., as well as research internships with faculty and researchers at their own or other HEIs/research institutions, so that students may actively engage with the practical side of their learning and, as a by-product, further improve their employability.The undergraduate degree will be of either 3 or 4-year duration, with multiple exit options within this period, with appropriate certifications, e.g., a certificate after completing 1 year in a discipline or field including vocational and professional areas, or a diploma after 2 years of study, or a Bachelor ‚Äôs degree after a 3-year programmeAn Academic Bank of Credit (ABC) shall be established which would digitally store the academic credits earned from various recognized HEIs so that the degrees from an HEI can be awarded taking into account credits earned.HEIs will have the flexibility to offer different designs of Master‚Äôs programs: (a) there may be a 2-year program with the second year devoted entirely to research for those who have completed the 3-year Bachelor‚Äôs program; (b) for students completing a 4-year Bachelor‚Äôs program with Research, there could be a 1-year Master‚Äôs program; and (c) there may be an integrated 5-year Bachelor‚Äôs/Master‚Äôs programMERUs (Multidisciplinary Education and Research Universities) will be set up and will aim to attain the highest global standards in quality education. They will also help set the highest standards for multidisciplinary education across India.More focus of HEIs towards research. The NRF will function to help enable and support such a vibrant research and innovation culture across HEIs, research labs, and other research organizations.üí°NOTE - National Research Foundation (NRF) in India, as a body to catalyse, facilitate, coordinate, seed, grow, and mentor research in institutions around the country.Optimal Learning Environment & Support for studentsThe curriculum must be exciting and relevant and updated regularly to align with the latest knowledge requirements and to meet specified learning outcomesThus, curriculum, pedagogy, continuous assessment, and student support are the cornerstones of quality learning.First, in order to promote creativity, institutions and faculty will have the autonomy to innovate on matters of curriculum, pedagogy, and assessment within a broad framework of higher education qualifications that ensures consistency across institutions and programs and across the ODL, online, and traditional ‚Äòin-class‚Äô modesThe Choice Based Credit System (CBCS) will be revised for instilling innovation and flexibility.Incorporate extra curricular positions of responsibilities, into the credit-based framework of the academic curriculum Students from disadvantaged backgrounds require more counseling, fundsA better framework backing ODL quality - Norms, standards, and guidelines for systemic development, regulation, and accreditation of ODL will be prepared, and a framework for the quality of ODL that will be recommendatory for all HEIs will be developed.Internationalization
Transferring credits -provide greater mobility to students in India who may wish to visit, study at, transfer credits to, or carry out research at institutions abroad, and vice versa.Credits acquired in foreign universities will be permitted, where appropriate as per the requirements of each HEI, to be counted for the award of a degree.Becoming "Vishwa Guru" and global presence - Setup satellite campuses abroad & welcome campuses from abroad universities hereResearch collaboration between campusesIn Closing
Education has an important role to play in making India self-reliant. The research here hints at an optimistic leap for Indian education to evolve and eventually help the nation spearhead technical breakthroughs & deeper thinking on the most pressing matters of the world.
And that's a future I look forward to growing up into.
The History of Cloud Computing,Building a fast, inexpensive & secure internet through the power of abstraction,Checkout the article here - Link
Migrating from MongoDB to DynamoDB using AWS DMS,Using AWS DMS to migrate from MongoDB to DynamoDB,For the deployment of v4 of a project's backend setup - A Nest JS-Prisma-MongoDB Stack, I used AWS Data Migration Service that helped me create the object mappings between the 2 databases & easily transfer all the data seamlesslyWhat would have been 6 hours of documentation & writing a migration script, actually became 12 hours of figuring out how the heck does AWS DMS even work & hence, this blog is a collection of all the 30+ blogs, GitHub issues & Stack Overflow answers that led to this successful migration on Sept 15th, 2022Here‚Äôs the UI of the AWS DMS Service as of Sept 15th 2022 and now I‚Äôm going to run through the basic steps followed to do this migrationPrepare the MongoDB cluster for migrationHere, I did nothing, except opening the DB to be network-accessed from anywhere and created an admin user whose credentials I could give to AWS DMS for the migrationCreate the replication serverAWS DMS creates a replication instance in a virtual private cloud (VPC). Select a replication instance class that has sufficient storage and computing power to perform the migration task, as mentioned in the whitepaperAWS Database Migration Service Best Practices.Choose the Multi-AZ option for high availability and failover support using a Multi-AZ deployment, as shown in the following screenshot.You can specify whether a replication instance uses a public or private IP address to connect to the source and target databases.üí°A replication instance should have a public IP address if the source or target database is located in a network that isn‚Äôt connected to the replication instance‚Äôs VPC using a virtual private network (VPN), AWS Direct Connect or VPC PeeringYou can create one replication instance for migrating data from all shard source endpoints, or you can create one replication instance for each shard source endpoint. We recommend that you create one replication instance for each shard endpoint to achieve better performance when migrating large volumes of data.Create the source MongoDB endpointReference LinkThe following screenshot shows the creation of a source endpoint for the MongoDB database.Create one source endpoint for the primary replica of each shard. This step is required to migrate data from each shard individually.IMPORTANT - Ensure that you specify shard name as server_name while creating source endpoint. i.e. while mongodb actually tells you that the endpoint is CLUSTER_NAME.PREFIX.mongodb.net, the endpoints are actually of the form CLUSTER_NAME_SHARD_NAME_SHARD_NUM.mongodb.net. Use this python command to get the shard names & ports of each shard that you can use as an endpoint# / SRC -> <https://www.mongodb.com/developer/products/mongodb/srv-connection-strings/>
import srvlookup ¬†# pip install srvlookup
import sys

import dns.resolver ¬†# pip install dnspython

host = None

if len(sys.argv) > 1:

¬† ¬† host = sys.argv[1]

if host:

¬† ¬† services = srvlookup.lookup("mongodb", domain=host)

¬† ¬† for i in services:

¬† ¬† ¬† ¬† print("%s:%i" % (i.hostname, i.port))

¬† ¬† for txtrecord in dns.resolver.query(host, 'TXT'):

¬† ¬† ¬† ¬† print("%s: %s" % (host, txtrecord))

else:

¬† ¬† print("No host specified")

Ensure that you select SSL as require while making the source endpoint & authentication_mechanism is set to SCRAM_SHA_1Here's a JSON file for the source endpoint - mongodb+srv://coursemapp:RANDOM_PASSWORD@CLUSTER_NAME.2g6bq.mongodb.net/coursemap-db?retryWrites=true&w=majority :You can choose or choose not to mention db-name while setting up src endpoint. I tried both ways - both worked. In my case, the db-name, as evident from the mongo_connection_string above, was coursemap-db. Can even notice this in mongo db{
"Username": "coursemapp",
"ServerName": "CLUSTER_NAME-shard-00-00.2g6bq.mongodb.net",
"Port": 27017,
"AuthType": "password",
"AuthMechanism": "scram-sha-1",
"NestingLevel": "one",
"ExtractDocId": "false",
"DocsToInvestigate": "100",
"AuthSource": "admin"
}

The following example shows the source endpoint for three shards:Create the target DynamoDB endpointReference LinkThe following screenshot shows the creation of a target endpoint for Amazon DynamoDB:AWS DMS creates a table on a DynamoDB target endpoint during the migration task execution and sets several DynamoDB default parameter values.Additionally, you can pre-create the DynamoDB table with the desired capacity optimized for your migration tasks and with the required primary key.You need to create an IAM service role for AWS DMS to assume, and then grant access to the DynamoDB tables that are being migrated into.Create tasks with a table mapping ruleBoth MongoDB and Amazon DynamoDB allow you to store JSON data with a dynamic schema.DynamoDB requires a unique primary key‚Äîeither a partition key or a combination of a partition and a sort key. You need to restructure the fields to create the desired primary key structure in DynamoDB.The partition key should be decided based on data ingestion and access patterns.As a best practice, we recommend that you use high-cardinality attributes (attribute having high number of unique/distinct values) . For more information about how to choose the right DynamoDB partition key, refer to the blog post Choosing the Right DynamoDB Partition Key.Based on the example query patterns described in this post, we recommend that you use a composite primary key for the target DynamoDB table.The following screenshot shows the attribute mapping between MongoDB and DynamoDBFor a sharded collection, MongoDB distributes documents across shards using the shard key. To migrate a sharded collection to DynamoDB, you need to create one task for each MongoDB shard.Create an AWS DMS migration task by choosing the following options in the AWS DMS console for each shard endpoint:Specify the Replication instance.Specify the Source endpoint and Target endpoint.For Migration type, choose Migrate existing data and replicate ongoing changes to capture changes to the source MongoDB database that occur while the data is being migrated.Choose Start task on create to start the migration task immediately.For Target table preparation mode, choose Do nothing so that existing data and metadata of the target DynamoDB table are not affected.If the target DynamoDB table does not exist, the migration task creates a new table; otherwise, it appends data to an existing table.Choose Enable logging to track and debug the migration task.For Table mappings, choose Enable JSON editing for table mapping.The following screenshot shows these settings on the Create task page in the console:Used the object-mapping functionality in AWS DMS. 2 types of rules against wildcard matched entities can be specified hereselection rules -> Link - to just select which all tables you want from src dbtransformation rules -> LinkEach of these rules is applied on tables whose names you can enter manually, or pattern match them - Link - to select which tables you want from src db + transform (eg : rename, change data type, make composite attributes) certain attributes/columns while the migrations are happeningHere's the object-mapping rule set I used (to get all the mongo tables into ddb) without any transformation{
    "rules": [
        {
            "rule-type": "selection",
            "rule-id": "213057977",
            "rule-name": "213057977",
            "object-locator": {
                "schema-name": "%",
                "table-name": "%"
            },
            "rule-action": "include",
            "filters": []
        }
    ]
}
You can also choose to create something called a "pre-migration assessment report" in DMS, that just tests all the connections & other things, before the data migration starts. Is useful and this option appears when you're about to create a data migration taskTable mappings in Create taskAWS DMS uses table mapping rules to map data from a MongoDB source to Amazon DynamoDB. To map data to DynamoDB, you use a type of table mapping rule called object mapping.For DynamoDB, AWS DMS supports only map-record-to-record and map-record-to-document as two valid options for rule-action.For more information about object mapping, see Using an DynamoDB Database as a Target for AWS Database Migration Service.In our example, we set the object mapping rule action as **map-record-to-record** while creating the AWS DMS task. The map-record-to-record rule action creates an attribute in DynamoDB for each column in the source MongoDB.AWS DMS automatically creates the DynamoDB table (if it‚Äôs not created already), the partition key, and the sort key, and excludes any attributes based on the object mapping rule.The following table mapping JSON has two rules.The first rule has a rule type as selection, for selecting and identifying object locators in MongoDB.The second rule has a rule type as object mapping, which specifies the target table-name, definition, and mapping of the partition key and sort key.For more information about the object mapping for DynamoDB, see Using Object Mapping to Migrate Data to DynamoDB.The following image shows migration tasks for three shards (corresponding to three source endpoints that were created in the previous step) :Monitor the migration tasksThe AWS DMS task can be started immediately or manually depending on the task definition.The AWS DMS task creates the table in Amazon DynamoDB with the necessary metadata, if it doesn‚Äôt already exist. You can monitor the progress of the AWS DMS task using Amazon CloudWatch , as shown in the following screenshot. For more information, see Monitoring AWS Database Migration Service TasksYou can also monitor the AWS DMS task using control tables, which can provide useful statistics. You can use these statistics to plan and manage the current or future tasks. You can enable control table settings using the Advanced settings link on the Create task page. For more information, see Control Table Task SettingsThe following screenshot shows the log events and errors captured by CloudWatch:Problems facedDuring this migration, the list types from MongoDB got stored as plain strings in DynamoDB & this became a huge problem since the data was corrupt.For eg. {prerequisites: ["p1", "p2"] } in MongoDB was stored as { prerequisites: "["p1","p2"]"} in DynamoDB.This was solved by using a piece of code that migrates the data from 1 dynamodb table to another and modifies the data while in-transit// The approach for this process would be to create a temporary "Full DDB access" credential
// and use it to actually migrate the data across tables & delete the credentials after
// reason being : we can't trust the package with such an important IAM credential for our AWS account

var copy = require('copy-dynamodb-table').copy

var globalAWSConfig = { // AWS Configuration object <http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#constructor-property>
    accessKeyId: 'ACCESS_KEY_ID',
    secretAccessKey: 'ACCESS_KEY_SECRET',
    region: 'ap-south-1'
}
 

const convertStringToArray = (string) => {
  return string.replace(/['"\\[\\]]+/g,"").split(',').map((item) => item.trim()).filter(item => item.length>0)
}

// For courses
const courseDtoConverter = (item) => {
  const dtoItem = {
    id: item.oid__id,
    courseContent: convertStringToArray(item.array_courseContent),
    prerequisites: convertStringToArray(item.array_prerequisites),
    referenceBooks: convertStringToArray(item.array_referenceBooks),
    textBooks: convertStringToArray(item.array_textBooks),
    courseCode: item.courseCode,
    courseType: item.courseType ?? "Theory",
    credits: item.credits,
    deptCode: item.deptCode,
    description: item.description ?? "",
    name: item.name
  }
  console.log(dtoItem)
  return dtoItem;
}

const transferData = (srcTableName, destTableName, dtoConverter) => {
  copy({
    config: globalAWSConfig, // config for AWS
    source: {
      tableName: srcTableName, // required
    },
    destination: {
      tableName: destTableName, // required
    },
    log: true, // default false
    // create : true, // create destination table if not exist
    schemaOnly : false, // if true it will copy schema only -- optional
    continuousBackups: true, // if true will enable point in time backups
  transform: function (item, index) {
    return dtoConverter(item);
  } // function to transform data
  },
  function (err, result) {
    if (err) {
      console.log(err)
    }
    console.log(result)
  })
}

// For copying "courses" table
transferData("SRC_TABLE_IN_DYNAMODB",'DEST_TABLE_IN_DYNAMODB', courseDtoConverter)
Introduction to OSI Model of Networking,A explainer on the 7 foundational layers on which modern networking systems are built,The Open Systems Interconnection (OSI) model describes seven layers that computer systems use to communicate over a network. It was the first standard model for network communications, adopted by all major computer and telecommunication companies in the early 1980s.The stack contains 7 layers, namely - Application, Presentation, Session, Transport, Network, Data Link & Physical layer - & they are arranged from the highest level of abstraction i.e. the webpages & email protocol to the lowest level i.e. the cables actually relaying the internet.Here's a picture to help you understand this :Let's now look at the working logic & use of each of these layers, one by one : Application Layer (Data)The application layer provides an interface between end-users and software applications.It receives data from end-users, and displays received data for them.This layer does not contain the end-user applications; instead, it facilitates communications with the lower layers.Some protocols found within this layer include HTTP, HTTPS, FTP, TFTP, Telnet, SNMP, DNS, Rlogin, SMTP, POP3, IMAP, and LDAP.Presentation Layer (Data) - Translation, Compression, EncryptionThis layer facilitates the presentation of Data to the upper layer.Mainly, it provides the encoding scheme and encryption/decryption for secure transmission.For instance, it translates applications format to network format and vice-versa.Protocols of this layer: JPEG, BMP, GIF, TIF, PNG, MP3, MIDI, ASCII & ANSI, etc.Session Layer (Data) - Session management, AuthN, AuthZWhen two computing devices need to communicate, a session must be created, which happens at this layer.Some of this layer's functions are the establishment, management (coordination), and termination of sessions.Analogy : a telephone call where you first establish the connection, exchange a message, and finally terminate the session.Some of the protocols of this layer are SIP, NFS, SQL, ASP, and RDBMS.Transport Layer (Segment) - Segmentation, Data Flow, Error Control, Connection and connectionless transmissionThis layer, often considered the heart of the OSI model, is responsible for controlling data flow between two devices.For example, this layer determines the amount of data needed to send and the location where it should be sent.This layer is also responsible for data flow and error control.For instance, the flow control determines the optimal speed of sending data to avoid flooding the receiver with data if the connection speed is different between the two communicating parties.Simultaneously, error control ensures retransmitting the data again if some packets were lost on the receiver side.This layer's best-known example protocol is the TCP protocol, which resides as part of the TCP/IP protocol suite. Some other protocols on this layer are TCP, UDP, and SPX.Network Layer (Packet) - Packets from segments, IP addressThe network layer is responsible for data packet forwarding and routing data between routers.It facilitates data transfer between two devices residing in two different networks.For example, if you want to send a message from your computer in New York to a server in San Francisco, there are thousands of routers and ‚Äì maybe- millions of paths between these two points.However, the routers at this layer help you do this efficiently by automatically selecting the nearest way.The network layer is also responsible for translating the logical addresses into physical addresses and is responsible for data fragmentation.Hence, it breaks segments of data into smaller units called packets before sending them to other networks.Data Link Layer (Frame) - Frames from packets, MAC addressThis layer provides a connection between two devices residing on the same physical network, for example, between two devices in the same LAN.This layer receives packets from the network layer and breaks them into small units called frames.The data link layer also performs data flow and error control within intranets. It contains two other sub-layers: the Media Access Control (MAC) layer and the Logical Link Control (LLC) layer.Most ordinarily, networking switches operate at this layer. Some protocols within this layer are PPP, HDLC, ATM, Frame Relay, SLIP, and Ethernet.Physical Layer (Binary) - Physical stuff like cables, radiosThis layer exists at the bottom of the OSI layer.It represents the OSI model's physical component, including cable type, radio frequencies (when using a Wireless connection), the layout of pins, and voltages.This layer is responsible for delivering the raw data from the sending device's physical layer to the receiving device's physical layer.Popular devices found at this layer include network hubs, cabling, repeaters, and modems.SummaryAlthough created years ago, the OSI model is still the primary model used to represent network architecture.The OSI reference model is still the primary guide used by software developers and hardware vendors to create interoperable programs and devices that facilitate digital communications. A really cool acronym to remember the order in which these layers appear is :üí°"All People Seem To Need Data Processing"All - ApplicationPeople - PresentationSeem - SessionTo - TransportNeed - NetworkData - Data Link LayerProcessing - Physical Layer
Knowledge Graphs,An deep dive on the power of data intelligence from knowledge graphs,A knowledge graph is an interconnected dataset enriched with meaning so we can reason about the underlying data and use it confidently for complex decision-makingThe why?
Data is complex and of many typesData in orgs, is stored in silos that have relations with each other - which when harnessed, can provide very valuable informationSmall and wide data provides more context (which leads to less data hungry ML)Complex forms of data can be analyzed in graphsGraphs
Data & relations stored togetherFind relationships as quickly as dataRelationships provide 1st level of context to data aka ‚Äúdynamic context‚ÄùWhen you take data and throw it into a dynamic structure like a graph, you get a structure that is connected contextually to all of its neighbors and those to their neighbors and so on - graph grows richer with more data ‚Üí adding context dynamicallyOrganizing principle ‚Üí comes in many formsKnowledge Graphs (KG)
No 2 KGs are the sameEvery KG is the integration of data from multiple sources When data comes from multiple sources, you have to solve the problem of identity. You have to determine that this data coming from 1 data source is the same as another chunk of data coming from another data source. Might involve merging the KG into a single node or representing them in some form that they are equivalentOn this foundation, we can divide KGs into 3 categories -üí°üí°NOTE : These 3 categories might not represent the complete picture & aren‚Äôt mutually exclusive. Any complex real world KG will involve combining many of themCATEGORY 1 - Semantic Search - Starts from a ‚Äúcollection of things‚Äù (eg : product descs, documents). We want to enrich this representation of these things with features about them such asannotations about entities mentioned in themclassificationwhen we‚Äôve enriched this representation, we overlay the domain modelling - overlay an ontology or taxonomy - allowing multiple paths for exploration, navigating from data to metadata and back to answer interesting questionsCATEGORY 2 - Pattern matching - Based on ‚Äúshapes on your data‚Äù.Can be a star - find a highly connected node that is central to your network (maybe like a business infra that many things depend on) - a single point of failure.Can be a ring ¬†- indicating an unusual way in which data is flowing in your dataset that can help uncover fraudulent behaviorCan be a chain of events - uncover a particular customer behavior, that can help predict churnCan be a cluster - a set of related topics, highly connected customers etc.All these shapes are easy to identify using a graphCATEGORY 3 - Dependency modelling - Representing dependencies. Task & resource allocation. Interesting thing about modelling dependencies is that when we provide the building blocks, like, for eg - this task depends on another - the graph will build a complete picture By giving in ‚Äúdirect dependencies‚Äù to graph, it will help us uncover the ‚Äúindirect dependencies‚Äù (tricky ones) eg : telecoms network. When a router goes down, easy to say which incoming/outgoing connections will fail but difficult to predict higher dependencies such as what happens to the services, customers - so this is a hard thing to uncover. Dependency based KG help us uncover theseIn Closing
Knowledge graphs are a very promising way of organizing data, offering a great way to build intelligence on top of them. The initial setup might be complex but the rewards once the data pipeline receives more ingress, the rewards will be exponential.
When it comes to OALP, the age SQL tables is coming to a slow end. Graph technologies will spearhead most such processes by 2025.
If there's one thing I want you to take back after this blog, is, when it comes to data intelligence :
üí°Let's not join, let's connect :)
How to start coding,A guide on getting started with exploring software,It's all about creative ways to interact with the computerThat's what it is. Talking the computer to do something in a language it can understand. Steve Jobs' described it as "the bicycle for the mind".Why I'm advocating this is primarily because of the equalization of impact-creating opportunities that software empowers us with - right from the students in the dorm room to the juggernauts on whose efficacy our world depends.That's enough chatter. I'll talk about 4 amazing paths you can explore in this domain & will be sharing resources for the same.More than anything, remember to give any learning experience an honest try before figuring the scope it carries for you. Never outright reject anything as an uninformed choice.So here they are - Software Engineering, Software Development, Data Science, ¬†Newer technologiesSoftware EngineeringSolve complex problems using code & mathQ. What do they do on a daily basis?Algorithms, in the most broad sense. Efficiency, speed & optimization. Great examples of this would be route optimization for the Amazon delivery vans, speed of transaction in netbanking services, encoding & compressing large files when storing them or loading them - to save internet bandwidth. If you're curious about the inner workings here, read on.Software Engineering, is that career path, which will require the most solid foundations in Computer Science - courses such as Data Structures & Algorithms (DSA), Object Oriented Programming, Database Management Systems, Operating Systems, Networking, Security - are really important here; primarily DSA.DSA can be best compared to a building construction. Say you want to construct a large building. You get an "input" of cement, steel & other building materials. Now, you take all this "input data" i.e. building materials and arrange it into a tall "structure". This might take more time to build since workers have to build at a height but it'll occupy less floor space. Finally you define an "algorithm" aka a series of steps that will take this structure and interact with the data inside it to offer functionalities such as, in this case, shelter, sanitation etc.Alternatively, we could have still built a really large building, by building it as wide as possible. This would be faster as multiple parts of the building are built in parallel, but it takes up more space. The end "output" in both solutions is still a large building, it's just a different approach will have different tradeoffs.And that's what a software engineer does. They take any of the world's problems, map the data involved in this problem into an efficient structure & write algorithms to offer functionalities on top of these structures.Eg : Waiting for your number at a fast food restaurant? They've put all orders in the data structure - "queue" which offers the "first in - first out" property, by its nature. Struggling to see which course is a prerequisite for which ? Arranging the courses into a "graph" helps understand how they interact with each other.Thus, competitive programming - the process of competitively coming up with the best solutions to problems using DSA, is a great way to flex those thinking muscles. In terms of languages, you can choose to master any, although usually, a more low level language like Java or C++ is recommended to begin with, since they solidify your foundations really well on how each component works.To know more - checkout T5E, IITM's blog on the sameFree resources that will help you grasp these concepts well are :Data Structures & Algorithms #1 - What Are Data Structures?Data structures and algorithms tutorial #1 - let‚Äôs go!Check out Brilliant.org, a website for learning computer science concepts through solving problems: htt...YouTubeBig O Notation - Full CourseThis course will teach you how to understand and apply the concepts of Big O Notation to Software Engineering. Big-O notation is a way to describe how long a...YouTubePython Object Oriented Programming (OOP) - For BeginnersIn this beginner object oriented programming tutorial I will be covering everything you need to know about classes, objects and OOP in python. This tutorial ...YouTubeWhat is a Database?What is DBaaS? ‚Üí https://ibm.biz/what-is-dbaasLearn more about Db2 ‚Üí https://ibm.biz/learn-more-db2Learn about Db2 on Cloud ‚Üí https://ibm.biz/learn-db2-on-cl...YouTubeSoftware DevelopmentBuild apps with codeQ. What does a software developer typically do?Build apps that we know and love each single day. Right from the button on the screen to the 5TB data being stored.Software Development is the art of looking at a problem, breaking it down into solution features & coalescing them into a useful application that can be used by someone to do a job better. Eg : Can't order food at 3am? Swiggy it. Can't find a cab? Uber/Ola. Instant communication? WhatsApp.Building, improving & fixing errors in these apps, also called as SaaS (Software as a service) is the role of a software developer. The simplest piece of software has 2 ends to it - a frontend & a backend.A "frontend" developer, works on the user interface from the buttons you click or the inputs you fill to live messaging & video streaming - on a website, a mobile app or that tiny display of your smartwatch. This frontend interface - basically anything that a user can see or play with - is colloquially called "client".A "backend" developer, typically works on the business logic & the data being shown & exposed for interaction by the frontend. Eg : Calculating your bank account balance & breaking it down into spending categories such as food, TV, shopping etc. - details that are later shown on the "frontend". All the data for such logic is stored in a "database" (think of it as a collection of tables where each table is our row-column entry table). If I were ICICI Bank, I would have a "users" table, storing all information such as name, address, etc. of app users, a "transactions" table, storing information about transactions between accounts.We write code on a piece of machine that takes this data from "database", performs some logic, and "serves" it to the client, hence, it is called the "server".APIs - An important term here is API or Application Programming Interface, which is a common medium of communication between client & server. The best way to understand it is this - You are at a restaurant as a "client". You ask the waiter for something "spicy". The waiter understands what you said and relays this "request" to the chef. The chef searches the "vegetables" table for spicy options & fetches the stored "data" (ingredients) & performs some logic on them to cook up a delicious chilli soup. The chef acts as the "server" of this soup as a "response" to the "client's" "request", which then the waiter relays back to the client. If you caught on the analogy, the waiter is the API - an interpreter of messages offered by the server, that helps a client communicate with the server & get what they want.Cloud Computing - As you build bigger applications, you will realise the need for more compute power, better fault tolerance & faster experiences for your customers. This is when you move to cloud computing - running your logic on someone else's servers. Kind of like giving your chilli soup recipe to a chef in France so that you can even serve a French customer really fast & charge them for it. Checkout our blog on cloud computing for more details.Finally, since you'll hate repeating work - you will explore things such as DevOps - Developer Operations - to automate app rollouts, app testing, recovery from crashes & more.I personally feel that a great way to get started with software development is the by making websites with JavaScript or mobile apps with Flutter.Trust me, the more projects you build, the more fun it becomes. You have to be patient at the start, but this skill gives an all-round view of everything around building a software company from zero-to-one - a very powerful tool in your belt.Here are some great channels and resources to help you get started with your journey :Introduction to Web Development || Setup || Part 1In this video you will learn how to set up your computer for web development. We will be setting up your browser, text editor, and version control system.Dow...YouTubeFlutter Tutorial for Beginners #1 - Intro & SetupHey gang, in this Flutter tutorial for beginners, I‚Äôll give you a quick introduction to the course and Flutter itself, a preview of what we‚Äôll be building & ...YouTube5 HTML, CSS & JS Mini Projects - Scroll Animation, Rotating Navigation, Drag Events, etc5 projects from my 50 Projects In 50 Days course on Udemy. These projects are meant to sharpen your CSS skills and help you work with the DOM using JavaScrip...YouTubeData ScienceDerive valuable insights from data to make effective decisionsQ. What does a data scientist do ?Ideally, the job is to derive useful insights from really large amounts of data, that is important to make business decisions. It could also encompass automating this process by training a machine to analyze the data & suggest a step-wise method to increase chances of success. Want to unlock a phone ? How about scanning your face, mapping it to studied images of you to verify if its you & then unlock. Bored of browsing endlessly ? How about a recommendation algorithm to help you pick your tshirt to buy or movie to watch ? All this is possible through data science.As we saw the advent of data, particularly with the rise of internet over the past 2 decades, it became a really important source of truth about how customers were interacting with your SaaS and how you can use valuable information from this data to build better features to provide better experiences to your users.And as time went on, humans became lazier & so they "programmed machines to learn from this data" and suggest the next steps. That is what we call "Artificial Intelligence" today.This, field requires good amount of understanding in Matrices, Statistics & Linear Algebra, since the entire framework is dependent on these 3 topics.In the broad sense, what you usually do in software, involves "input" and your function that spits out an output. In machine learning, however, it involves having an "input" & "output" and the machine learning from the trend in multiple input-output pairs to spit out a function that reasonably maps an input to an output well.A while back, I'd written a comprehensive blog on Machine Learning to understand & compare various algorithms and their applications.There are some great industry-accepted frameworks such as TensorFlow (developed by Google), Keras, PyTorch - names you might have come across. These are nothing but ways to actually use best practices to prevent reinventing the wheel each time you want to solve an ML problemHere are some useful resources (you should definitely do the 1st one)Machine Learning#BreakIntoAI with Machine Learning Specialization. Master fundamental AI concepts and develop practical machine learning skills in the ... Enroll for free.CourseraPython TensorFlow for Machine Learning ‚Äì Neural Network Text Classification TutorialThis course will give you an introduction to machine learning concepts and neural network implementation using Python and TensorFlow. Kylie Ying explains bas...YouTubeMore specific fields...While the above 3 topics cover most of what is involved into coding, there are certain more specialized fields evolving, such as "blockchain", "ethical hacking" that have their roots in software engineering, but are starting to branch off as separate paths.Typically, as a blockchain developer, you'd be writing code called "smart contracts" which is nothing but contracts that have trust so inherent, that they will fail as soon as trust is breached.In a career as a cybersecurity engineer, you'll not only test & make your system fault tolerant, you'll also come up with methods to find possible vulnerabilities before they're rolled out.‚≠êNOTE : Fun fact - Netflix built something called the "Simian Army", a set of code servers that continuously battle against Netflix's system, to improve its performance.Here are some resources that can help you get started here :Learn Blockchain, Solidity, and Full Stack Web3 Development with JavaScript ‚Äì 32-Hour CourseThis course will give you a full introduction into all of the core concepts related to blockchain, smart contracts, Solidity, ERC20s, full-stack Web3 dapps, ...YouTubeFull Ethical Hacking Course - Network Penetration Testing for Beginners (2019)Learn network penetration testing / ethical hacking in this full tutorial course for beginners. This course teaches everything you need to know to get starte...YouTubeIn closing....This was the exhaustive set of resources through which anyone can get started with coding, irrespective of their background. It is difficult & frustrating to get used to 40 open Chrome tabs, a flood of feature requests & bug fixes, but the fulfillment majorly lies in having solved impactful problems.In fact, fun thing is, you won't be coding as much as you will be making detailed plans & diagrams for landing upon the most efficient solution. My colleague used to say "20% of time invested in planning, saves 80% of the time while coding"You gotta walk before you run. You'll search a lot, teach yourself a lot - now that's a superpower. You'll run the code, it won't work. You won't stop though. If anything, this art teaches you patience, that's for sure XDwhile(!(succeed = try()));
Until next time.....
The Everything Store : Review,Lessons drawn from the twisted tale of bookselling online retailer that became an internet juggernaut,üí°When you are eighty years old, and in a quiet moment of reflection narrating for only yourself, the most personal version of your life story, the telling that will be most compact & meaningful will be the series of choices you made. In the end, we are our choices~Jeff Bezos (Founder, Amazon)This book is a twisted tale of how an unkillable technology business was built, sustained & transformed into the "everything company" juggernaut it is today.It's been easy for me to disagree, from a personal belief standpoint, certain business decisions that Amazon took, over the years, to provide the best experience to customers - be it its philosophies of exhaustive work ethic, beating its competition and unfair stakeholder deals, I now marvel at the genius that had to make these tough calls to ensure that the company endured.If anything, I've learnt a couple of things about building a lasting company from the book, and here, I'm summarizing the best thoughts I came across.On building a company

Each meeting begins with everyone silently reading the document

This, I felt was a brilliant move to ensure all participants in a meeting had an in depth understanding of the discussion since documented points stick for longer & inspire much more intricate examinations & debates.

It's one thing to have a good idea but it's another to have confidence in a person to execute it.

'Accountability'. The one necessity we all want in our co-workers. Changes the way your work moves & how well you gel with each other on such a fundamental bond of trust

He embraces the truth. A lot of people talk about the truth, but they don't engage their decision making around the best truth at the time

Amazon CTO Verner Wogel shed light on why Jeff was point blank in his opinions. Also, he was brave enough to accept the truth when things were turbulent and keep the company afloat, by making hard decisions, be it cutting costs or killing highly-funded experiments
On thinking

'regret-minimization framework'

Bezos teaches a very balanced decision making framework, that's logical & emotional. Basically, make conscious moves today such that when you're reminiscing your choices around 40 years from now, you have as less regrets as possible.

One day, you'll understand that it's harder to be kind than clever

Bezos' grandfather adviced a teenage Jeff on this - to always be respectful and compassionate instead of being coldly smart and analytical. We even carry the impact of what we say, on others.

I have realized about myself that I'm very motivated by people counting on me. I like to be counted on

An good leader, I feel, likes the trait of being responsible and carries the load of several others' concerns.

Companies need to think not just what they can get for themselves from new technologies but how they can enable others
We tried to imagine a student in a dorm room who would have at his or her disposal the same infrastructure as the largest companies in the world. It would be a great playing-field leveler for startups & smaller companies to have the same cost structure as big companies

This was the stark vision for Amazon Web Services. The most lasting products are the ones that win, when the overall society wins.
On innovation

'Amazon isn't what's happending to the book business, the future is happening to the book business'

A good thought nugget and a alarming reminder suggesting the inevitable perishability of the current forms of most products that seem everlasting.

High margins justified rivals' investments in R&D and attracted more competition, while low margins attracted customers and were more defensible

Snippet on how Amazon created a highly defensible business by always keeping the lowest costs possible. This helped it overturn monopolies and undercut incumbents

I absolutely know it's very hard. We'll learn how to do it

The words with which, Jeff Bezos announced the Kindle project

It is not enough to be inventive - that pioneering spirit must also come across and be perceivable by the customer base.

An overarching impression of the company should not just be internally valued within the team, but also admired by each user of its services, since they too are valuable stakeholders

Defects that are invisible to the knowledgeable may be obvious to the newcomers. The simples solutions are the best.

Why the odds of an underdog are slightly strong in any given domain.
On business

Great companies fail, not because they want to avoid disruptive change but because they are reluctant to embrace promising new markets that might undermine their traditional businesses and that do not appear to satisfy their short term growth requirements.

Jeff on why after a point, companies that don't adapt, sink.

Profit margin is finite. Better financial terms with suppliers translate directly into a healthier bottom line - and create the foundation on which everyday low prices become possible


Acquire other firms only when they had fully mastered their virtuous circles and then 'as an accelerator of fly-wheel momentum, not a creator of it

Words on when & why exactly should you acquire another company

Dyson pulled its vacuums from Amazon in 2011, though some models are still sold on the Amazon Marketplace by approved third-party merchants
Sellers know they should not be taking the heroin, but they can't stop taking the heroin
Just some excerpts on the power of network effect that marketplace platforms weild. How they've hooked both sides of the network into an irresistible experience


They force you to look at the numbers & answer every single question about why specific things happened

Data driven approach

Frugality breeds resourcefulness, self-sufficiency & invention. There are no extra points for headcount, budget size or fixed expense
Values molded through 2 decades of surviving in the thin atmosphere of low profit margins & fierce skepticism from the outside world.

How the parsiminous, undying principles of a company that survived 2 market crashes were forged
"It's almost like he fired an arrow & then followed that arc" - ability to keep leaping gorward vs protecting existing groundüí°I see companies these days where thoughts of 'exits' are foremost in the mids of top management & board, and it is so clear that this value will infect the decision making down to the smallest choice by the most junior employee. Do we create something that is good, or just that seems good & might get us acquired or funded ?Amazon bet on the internet & built its legacy. Right from a ¬†'duct tape and WD40 engineered' website (thanks to early engineer - Shel Kaphan), to a behemoth masquerading as a missionary and a mercenary at the same time, that all traces back to the vision of building - "an everything store".
CBDC & Digital Rupee,What is the CBDC & why it is going to revolutionize wholesale & retail finance,üí° A more efficient and cheaper currency management system

What is a central bank digital currency (CBDC)?

üí° It is the same as a fiat currency and is exchangeable one-to-one with the fiat currency. Only its form is different


Central bank digital currencies (CBDCs) have recently emerged as a hot topic in the financial space. Banks, Institutions, and governments are performing research and analysis on the economic and technical feasibility of introducing a new form of digital money and its impact on monetary and fiscal policy.
A Bank of International Settlements¬†report¬†states that over 80% of central banks are already researching CBDC. It begs the question: why are these institutions preoccupied with CBDCs? In this explainer, we'll cover CBDCs, their importance in digital economies, countries exploring their use cases, and the road to mass adoption.
CBDCs explained
Cryptocurrencies, as we know them today, are extremely volatile and lack government backing ‚Äî CBDCs overcome these concerns while using the same underlying distributed ledger technology of cryptocurrencies. Governments recognize CBDCs as legal tender (coins or banknotes that must be accepted if offered in payment of a debt) in the issuing central bank's jurisdiction, meaning anyone can use them for payments and every merchant must accept them.
In simpler terms, CBDC is short for Central Bank Digital Currency, an electronic form of central bank money that citizens can use to make digital payments and store value. A CBDC offers three main elements:

A digital currency
Issued by the central bank
Universally accessible

But why should a government issue a CBDC when fiat currency (Fiat money is a type of currency that is not backed by any commodity such as gold or silver, and typically declared by a decree from the government to be legal tender. Throughout history, fiat money was sometimes issued by local banks and other institutions) exists?
A CBDC is thus the sovereign equivalent of existing cryptocurrencies or crypto assets like bitcoin and ethereum. However, unlike bitcoin and ethereum which are mined and generated accordingly and are fully decentralised, CBDCs will be issued and controlled by a country‚Äôs central bank.
Why issue a CBDC?

üí° People are looking at this new money ledger to get frictionless payments that are inexpensive and secure.

If a country issues a CBDC, its government will consider it to be legal tender, just like fiat currencies; both CBDC and physical cash would be legally acknowledged as a form of payment and act as a claim on the central bank or government.
A central bank digital currency increases the safety and efficiency of both wholesale and retail payment systems. On the wholesale side, a central bank digital currency facilitates quick settlement of retail payments. It could improve the efficiency of making payments at the point of sale or between two parties (p2p).
No physical coins or notes are available to individuals in a digital society, and all money is exchanged in a digital format. If a country intends to become a cashless society, a digital currency with government / central bank backing is a credible alternative.
The pressure for governments to adopt a CBDC is strong, as the market for private e-money is on the rise.
If it becomes mainstream, beneficiaries are at a disadvantage because e-money providers aim to maximize their profits instead of the general public's.
Issuing a CBDC would give governments an edge over the competition from private e-money.
In addition to domestic transactions, the current cross-jurisdiction payments' model depends heavily on central banks operating the¬†real-time gross settlement (RTGS)¬†infrastructure within which all local banks' obligations must settle. Since time lags exist in cross-border payments, participating parties are exposed to settlement and credit risk.
A CBDC is available around the clock, while privacy is taken into account to eliminate counterparty credit risk.
In all the existing formats of digital payments, banking solutions have been at the core. You need a bank account to transfer the money from one account to another. Even in the case of wallets, cards and UPI payments, an account is a must. The liability of these account transfers lies with the corresponding commercial banks.
Similarly, when banks pay each other, they pay in reserves from accounts with the RBI. However, consumers can‚Äôt make direct transactions where the liability lies with the Central Bank until and unless it‚Äôs physical cash. Now, this makes each person, their own bank.
In the case of CBDC, the liability will be that of the Central Banks (or RBI in India‚Äôs case). One may not need to have a bank account to make digital payments as they do in the case of existing digital transactions.
Different types of CBDCs
CBDCs are categorized into two different proposals based on the targeted users:
Retail Central Bank Digital Currency

Retail CBDC, based on distributed ledger technology, is traceable, anonymous, and available around the clock.
It offers possibilities for interest rate applications, as well.
Due to these advantages, a retail central bank digital currency focuses, in particular, on supporting the general public.
Additionally, it helps lower the cost of cash printing and promotes financial inclusion.

Wholesale Central Bank Digital Currency

Wholesale CBDC increases payments and security settlement efficiency while resolving liquidity and counterparty risk issues.
It‚Äôs a great fit for financial institutions which have reserves deposited in a central bank.
With their capability to improve wholesale financial systems' speed and security, even central banks consider wholesale central bank digital currency a favored alternative to existing systems today.

Countries experimenting with CBDCs
China: Digital Yuan
The People's Bank of China is one of the first central banks to develop a CBDC. They deployed a special task force in 2014 to research and implement a digital Yuan. It gained traction in 2020 when China announced the testing of a CBDC prototype. The first trial of digital Yuan was held in October 2020 in the Luohu district of Shenzhen. The second pilot program took place in Suzhou City at the beginning of 2021. As per reports, the Chinese digital Yuan will impact the $27 trillion payment market in China.
Sweden: e-krona
In 2017, the world's oldest bank, Swedish Riksbank, began its CBDC project called e-krona. In collaboration with Accenture PLC, a pilot took place from 2020 until February 2021, and the project was extended until February 2022. E-krona intends to offer a robust alternative in case of emergency or turmoil of private payment service providers, thereby ensuring the Swedish payment system remains stable.
Bahamas: Sand Dollar
In 2019, the Bahamas also began their CBDC project called "Sand Dollar'' ‚Äî it was fully deployed in October 2020. The project was initiated in two districts: Exuma and Abaco Islands. Each Sand Dollar constitutes an additional digital variant to the Bahamanian dollar, which is, in turn, kept at a 1:1 peg with 1 US dollar. The project delivers inclusive access to financial services and regulated payments.
Eastern Caribbean Area: DXCD
The monetary authority for the Organization of Eastern Caribbean States members, the Eastern Caribbean Central Bank, began their work on a CBDC project called DXCD to reach financially excluded parts of the population. Its prototype is being tested in Antigua, Barbuda, Grenada, Saint Lucia, St. Kitts, and Nevis. The main goal of DXCD is to be a retail payment system for citizens without credit cards, for merchant and e-commerce payments at low costs.
Marshall Islands: Sovereign
In 2018, the Republic of the Marshall Islands shared plans for launching a CBDC called Sovereign (SOV). Currently, the US dollar acts as legal tender on the island, mainly because its population is only 58,729 and the cost of cash printing exceeded its benefits. RMI plans to introduce SOV as alternative digital money as a legal tender to improve the efficiency of RMI's current payment systems.
CBDC's path to mass adoption

Decentralized blockchains today have a "user beware" attitude: all transactions are irrevocable once settled and assets cannot be recovered if the transacting party is unwilling to cooperate with legal authorities. This is not a viable option for a CBDC aiming for mass adoption.
Moreover, no existing non-banking payment system has this kind of treatment under existing laws. Nor is it possible for legal authorities to compel cooperation, since the recipient could be an unknown person from overseas not subject to abide by the laws and regulations of the CBDC's jurisdiction.
Instead, CBDC issuers should modify their state under banking, property, payment, and contract law. This is possible only with hybrid two-layer designs that address two distinct problems:

A CBDC needs a proper management infrastructure, wherein transactions can be first verified and then modified under the jurisdiction's law.
A dispute resolution system has security, compliance, and auditing processes. With Hedera acting as a publicly verifiable log of transactions for CBDCs that live in permissioned blockchain frameworks, banks that layer customer services on top of the payment layer can trust and verify that transaction information is accurate.
A CBDC also needs to report its state (account balances, transactions, etc.) to relevant regulatory bodies, which can be more than just the state of the transactions ‚Äî this includes things like information on IP addresses or account IDs.
This information is vital in dispute resolution for payment systems. Using Hedera as a public record, authorities could directly inspect a CBDC's state, including the details of how users have interacted with it.
Phoenix Moments,Why we should be grateful for all the low points in life.,Make each low point you have, worth the painThe view I have developed of the world is a one that is always in chaos. Thus, it is as important to cherish & be grateful for the low points in this rollercoaster I believe life is.This is how I ended up believing that winning every time is not a good thing as well. Sometimes, just a minor reality check through a failure, helps keep you level-headed in the game. A disrupting incident in an otherwise stable period. Stress sometimes get the better of us, making options that help you forget reality quite logical. The more high-performance your routine, the more likely you'll find yourself burnt out, spiralling towards this.I'm not just talking intoxication here; this could be anything that momentarily takes your head out of the game. For me, it used to be bing streaming - getting lost in stories of others to forget mine. And while that may not be wrong in helping rise over the tide, I'm still doubtful about how sustainable it actually is. Good or bad - I want to be a part of my reality - be it experiencing or enduring. The punches are going to keep coming, so might as well focus on levelling up your endurance index each time. For me, what works best, is describing these suffocating instances in life, where most things seem like they're falling apart - as "phoenix moments" since they feel like they burn you into nothingness, but given the right amount of courage & relentlessness, they force you to rise as a better version of yourself.Looking back, on somedays, these very moments will be looked-upon as the "life changing" or "character building" crossroads in your life, you'll be grateful for. Anecdote
Instead of capping this to moments from famous lives, I'll be sharing an anecdote from my life.I wanted to pursue some interesting Computer Science (CS) courses such as networking, algorithms, cryptography etc. but since I'd enrolled for a Bachelor's in Civil Engineering, each time, I'd always be ineligible for these electives. Semester after semester passed by, begging the Professors for just 1 shot at them. In semester 6 (out of 8), I finally got allotted a course on Computer Networking but the next day itself, I received an email from the Professor that since I'd missed out on some basic CS Courses in the previous semesters, my registration was disqualified, and frankly, just a lucky mistake. So I would have to drop the course. From the perspective of a 21 year old, who'd been always dealt with these cards, irrespective of the excellent profile I'd managed to build over the past 2 years of intensive application based learning, the declaration was suffocating since this put not just the chance of a minors in Computer Science in question, but the very aspect of graduation in jeopardy, since a massive credits defecit had mounted up, as I would continue waiting for this courses, semester over semester, not settling for anything else.We've all been there - the feeling of our world just crumbling down. Heartbreak, disbelief, loneliness seem to strike the worst then - sometimes all at once, making it difficult for us to focus & deal with such conclusive-seeming setbacks. Demoralized & directionless, he started experiencing a slump in performance at his internship & a general disinterest towards pretty much everything. Nothing seemed to make sense. All those efforts gone in building a powerful enough profile that seemed eligible, feeling just so futile. Classic phoenix moment.But here's where the power of will comes into play. I started talking to other friends who faced similar problems, regarding course registration & once enough feedback had been registered, I started working on a web-app that would make it easier for students to be self-aware about their academic choices, right from when they joined college. Day and night of constant body-breaking hard-work, failures, frustrations, loneliness plagued me. Didn't matter. The most transformative journeys are the difficult ones you embark on aloneThe mission was important & the passion, undying. The code was brilliant but he was running behind my launch date by a good 30 days. I once again started questioning whether it might work at all. The final day for course registration for students neared. My workload has also skyrocketed through the roof. Only a weekend remained between him & this all going to nothing. But I chose to battle this out since whether I won or lost didn't matter. I just didn't want to be down the path where I regretted not having tried my best. 36 hours - wired in. Everything blurred away. Bug bashes, server fires, failed requests - all the punches kept coming. But by now, I'd gotten used to the mishaps & gratefully embraced them with a rush of adrenaline & a rebellious reply.Be like a cockroach. Very hard to kill ~ Paul Graham The pilot launch drew in 2500 students, out of which around 50 or so, thanked me personally, for solving this problem. Professors from CS Department approached me to understand the working & offered their support in taking the solution further. I'd recognised his moment & lived through it with sheer will. Sleep came with a lot less restlessness after that. Take up the story of any successful person. You'll realise how they chose to rise from their phoenix moment in the most relentless way & build their legacyThe importance of the fire
When things seem tough, let willpower guide you & not instinctThe best part about such a moment is that you stand at an important crossroads here. Between good or bad. Difficult or easy. Adventurous or safe. Since you're most vulnerable at such a point, you have to be most careful about what you pick. There's a high chance it might get ingrained as a core, basic trait of you - which becomes very hard to change later. So whatever you choose, make sure you've absolutely thought consciously about it. Can be as simple as not opening that time-draining application, or that pack of cigarette. Constantly just fighting instinct with actual willpower. It'll be tough and painful, trying to control what your reflexes force you towards, especially since you're hurting. But its worth it. If you're already burnt, might as well make a bold bet . What more could you lose ?Helpful Hacks
2 things that I've found pretty helpful to deal in times like these are :Self-evaluation - Go on a solo walk, thinking about everything you're going through, patiently evaluate it. This helps you assess well how much of a problem you're really in & just being honest with yourself helps you to accept the situation & navigate your way out of it. I do this on a daily basis - good or bad, self-evaluation helps me correct the course to who I want to be.Long term thinking - In critical decision moments, such as our phoenix moments, if we just take 1 step back & think about say 30-40 years from now, would we regret doing what we're about to do ? This is what Jeff Bezos (Founder - Amazon) terms as the "regret minimization" framework where you make decisions that you don't regret later. Thinking long term and then backtracking it to how what we're doing now will affect it helps us make a more informed decision at the crossroads we stand.This is all I had to say about these really pivotal checkpoints in one's life. All we have to do is - "consciously decide" - when navigating through these.
The more of these you encounter, the more you'll be grateful for such moments as they're often the best teachers
‚ÄúWhen you come out of the storm, you won‚Äôt be the same person who walked in. That‚Äôs what this storm‚Äôs all about.‚Äù ~ Haruki Murakami
Using ML Algorithms,An explainer on when to use which ML algorithms,A brief introduction

The simplest 2 word explanation of ML - Curve Fitting

Basically, given a dataset, is a computer able to learn & interpret, what it ideally should i.e. learn it like a human would.
This is achieved by mathematically encoding everything from images, text, analytics into vectors that can be passed through a function that we want to come up with, that would actually give an output close to what a human brain would do for the same input information.
The accuracy of-course would be measured by drawing curves & see how well the output by the machine, matches real-world thinking i.e. how well does the predicted curve, fit the actual curve.
Basic Algorithms
I've personally been a developer, most of my career, hence most technologies I learn or stumble across new technologies across are actually when I run into a business use case that demands a new solution or approach as the most optimal way forward.
Hence, in this post, instead of explaining in depth what each algorithm does & the mathematics of it, I will primarily be focussing on explaining the basic algorithms used in Supervised (we have given inputs & outputs values) & Unsupervised (we have input values only) Learning & when to use which algorithm.
We will be speaking in short about :

Linear Regression
Logistic Regression (Classification)
Neural Networks
Support Vector Machine
K-Means Clustering - deriving structure from data
Principal Component Analysis

For each algorithm, discuss :

Optimization Objective
How to actually use it
When to use it

Also, as a bonus, I'll point you to some resources on actually debugging learning algorithms so that you can improve upon an implemented solution. üöÄ
Some basic definitions before we get started :
Say my dataset is the collection of characteristics of "houses in London" & their prices

Parameters aka Weights - The characteristic/trait associated with a given value in the dataset. eg: number of square feet, water supply hours
Features - The actual values in the dataset. Eg : 1200 square feet, 24 hour water supply.

Basically, our learning algorithm (especially in supervised learning) mostly is always assigning the right values to these weights associated with each value, to come up with a function (aka "hypotheses function") that always gives the good predicted value.
Eg : water supply hours should be given a higher weightage compared to other features, since most people want that, hence such houses will be in high demand, hence the prices for these houses will be more
1. Linear Regression
Regression Multivariate Gradient Descent Normalization Regularization
Used when - We have a labelled dataset & we wish to devise a function that uses the features + parameters & predicts the most accurate output. Since, the results are continuous & not into classified buckets, hence, the term "regression".
Eg application - Guessing the price of a house, starting with a labelled dataset of various houses‚Äô features such as area, locality etc. & their actual prices
"linear", since we mostly only deal with features independent of each other. Eg : size of a house does not ideally depend directly on how near its location is to a school. In case we do have dependent features we use - "Multivariate regression" a technique used to measure the degree to which the various independent variable and various dependent variables are linearly related to each other
We optimize the function we come up with using the "Gradient Descent" approach, by which we come up with better coefficients for each feature in our function.
Also, we do something called "regularization" to prevent underfitting or overfitting the predictions we give to the dataset we have. Get it just right :)
Lastly, we can also use something called "Stochastic Gradient Descent" for improving our hypotheses function, in case our dataset is too large & iterating over all training examples for each step of gradient descent is too time & compute consuming
This approach looks at only 1 random training example in each iteration of gradient descent, as opposed to all examples in the training set, & improves parameters just for that 1 example in that step, before moving on to the next random example in the next step. This drastically brings down number of iterations we have to perform for each step of gradient descent
2. Logistic Regression
Classification Sigmoid Gradient Descent One-vs-all
Use case - We have a labelled dataset & we wish to devise a function that uses features + parameters & predicts the most accurate classified output
Eg application - Looking at a series of emails & classifying as "spam" or "not-spam". eg of multiclass - Looking at a series of vehicle images, predict which of them is a bike, car, ship, bus etc
We basically predict the probability (between 0 to 1) of a given data point to lie in one of the classes. We use a "sigmoid" function as part of the function we come up with outputs lying in the range 0 to 1
For multiclass classification, we predict for each data point, what is the probability that it lies in 1 class vs its probability of not lying in that class (i.e. probability of it lying in any class except that class). This approach is called "one-vs-all" approach & we calculate this for all classes for all data points
3. Neural Networks
Complex non-linear hypotheses Forward Propagation Backward Propagation
Use case - We have a labelled dataset & we wish to learn interesting features starting from initial features, & perform classification. Basically, cases where we can‚Äôt use logistic regression for complex hypotheses as it is only a linear classifier
Eg application - Handwriting recognition
Each set of interesting features, that we learn, using the initial input features & training examples & building up on those - constitute a layer
Eg : Using 32x32 pixels black & white images & from a collection of pixels, learning what a line, arc, curve is & then using those, understanding how Arabic Numbers are constructed as a combination of these newly learnt, interesting features.
We use "Forward Propagation" algorithm to predict output based on the hypothesis function the layers of the neural network collectively. We use "Backward Propagation" algorithm to optimize the accuracy of the neural network
4. Support Vector Machine
Kernels Gaussian Kernels
Use case - Reducing feature set for labelled training data
Gives a more powerful & cleaner way of learning complex non-linear functions as compared to regression, neural networks - by learning only on basis of most important features in the given set.
Can be used to increase & decrease feature set size
Eg application - Feature set optimization in literally any machine learning implementation
We use kernels i.e. predefined methodologies to use existing features & come up with a smaller but more impactful feature set that can be less compute-heavy but more important to the actual output
5. K-Means Clustering
Cluster Assignment Move Centroid
Use case - Given an unlabelled dataset, that we have to transform into discrete classes/clusters/groups of data
Eg application - Grouping of Tshirts, by using dimension values, into S, M, L, XL categories
Basically we start by initializing the random points as "cluster leaders" (aka centroids) & then based on the similarity of other data points, with respect to these, add the other points to the clusters. This is known as "cluster assignment" step
Once this is done, we recompute the "cluster leaders' " values to be the average of values of all points in the cluster. This is known as "move centroid" step as on an actual graph, this average would be a centroid that would move upon recomputation
We keep alternating "move centroid" & "cluster assignment" step for a couple iterations till the "cluster leaders' " values aren't changing anymore or changing negligibly
6. Principal Component Analysis
Dimensionality Reduction Eigen Vector Reconstruction
Use case - Reducing feature set for unlabelled & labelled training examples aka "dimensionality reduction"
It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation
Eg application - Reduce memory needed to store data & to speed up learning algorithm
It uses the concept of "eigen vectors" which basically helps us extract the most impactful features i.e. the desired vector from a given feature set vector
Also, we use "reconstruction" of feature set, from the output that this algorithm gives, & use it in comparison to the actual feature set & assess & improve this algorithm.
7. Anomaly Detection
Gaussian Distribution
Use case - Collecting skewed datasets (almost as good as unlabelled) & learning from it to figure out a way to predict whether something is an anomaly or not
Eg application - Fraud detection based on user‚Äôs web activities, Manufacturing, Malfunctioning of computers in a data center
We basically assume that the data set follows Gaussian Distribution (basically data is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean) & use that to come up with a metric that helps us perform binary or multi-class classification of the data point we're looking at
8. Recommender Systems
Content Based Recommendation Collaborative Filtering
Use case - Build a system that recommends products to users based on previous feedback & also uses this same feedback to tag products better for an improved experience the next time around.

Basically, we treat each user as a linear regression model of their own (i.e. own parameters + features, so hypotheses function at an individual level) , based on their previous actions.

Eg application - Building recommendation engine for a streaming servive (eg : Netflix) or a e-commerce website (eg : Amazon)
Content Based Recommendation - Predicting/Improving parameters keeping features fixed.
This is more like summation of linear regression over a bunch of users. It addresses a problem of type - If Alice liked (gave a rating of 4 and above out of 5) movie 1 (which was tagged as "romance" & "action" ) & she also liked movie 2 (which was tagged as "romance" & "comedy"), to what extent will she like movie 3 (which was tagged as "romance" & "tragedy" ). Is movie 3 good enough to be recommended to Alice ?
Collaborative Filtering - Predicting/Improving features, keeping parameters fixed.
This is yet again a summation of linear regression over a bunch of users. It addresses a problem of type - If Alice likes "romance" & "tragedy" movies & John likes "action" & "romance" movies & Emily likes "drama" & "romance" movies, can a movie X, that all three have seen & liked, be tagged as "romance" ?
Thus, we use Content Based Recommendation & Collaborative Filtering in conjunction in most cases, to improve our own training dataset & the UX as our service is used more & more.
9. Online Learning
Large Dataset
Use case - You have continuous large, streams of data & you want your ML model to just learn from this data & then discard/throw it
Eg application - Product search algorithm showing the user only the most relevant, popular items, based on the latest learnings
9. Reinforcement Learning
Goal Oriented Algorithms Rewards Punishments
Use case - Enables an agent to learn in an interactive environment by trial & error based on feedback from its own actions & experiences.
Uses rewards & punishments for positive & negative behavior.
Usually modelled as a Markov Decision Process
Eg application - Find a suitable action model that would maximize the total cumulative reward. Building a bot that can play a game really well. Other use cases - Robotics, Business Strategy Planning, Traffic Light Control, Web System Configuration
Debugging Machine Learning Algorithms
The best way to go about this is :

Start with a quick, simple implementation of a ML model, chosen by the business use case you have, using just a small subset of the entire training dataset you have.
Plot "learning curves" to get an idea of how well your ML model addresses your use case.
Calculate metrics that you care about - such as Precision, Recall, F1 Score, Accuracy etc. & figure out which ones you want to improve upon
Then, bring into account things like regularization, adding more features etc. to fix any underfitting/overfitting problems you might have
Lastly, bring in your full dataset, use methods like dimensionality reduction, PCA, SVM, MapReduce (Basically splitting your training dataset into chunks & computing each part in parallel in a seperate core), Online learning to scale your ML model.

Word of advice
Sometimes, by just looking at a ML problem, we might not know what ML algorithm to use. More than the algorithm itself, the following play a big part in training a model :

How much data do you have
How skilled are you at error analysis, debugging learning algorithms, designing new features, figuring out what features give you learning algorithms & so on

Afterword
This is it. A quick runthrough to most important, fundamental concepts of machine learning & why to use them. Massive thanks to Andrew Ng's exhaustive Machine Learning course that forms the bedrock of this blog.
Hope you don't use this to build the AI that will conquer humanity one day :)
Education on a Map,The origins of the journey of visualizing educational opportunities as nodes on a graph,It's like "a decentralized insti fundae session right ?"~ Abhiram ShenoiThe journey for the CourseMap platform began with a simple idea -> What if courses in college are actually like nodes on a graph that you can study the interactions between ?
It has been a crazy journey of building this product from ground-zero to support a 1700 student base for now, with a vision to go much higher & further than thatThe idea
It all started the day I got rejected from a Computer Networking course on a minor complication of course allotment protocols in place. It just hit me that organized information of courses in their true nature - as being linked entities was much required owing to 2 reasons :

The massive plethora of options in courses that students can explore & pursue in college. Also, the fact that how each course interacts & compares with others in terms of prerequisite matching, difficulty, slot clashes
The distributed nature of skillsets that most people acquire from multiple sources, that leads to the creation of their roadmap as a collection of steps taken over years in college.

I read "The Mom Test" as my reference (because Eric said so......) & interviewed around 40 people in total, mainly from my college
Hence, was born the idea for a platform that would treat each course as a node in a graph to begin with, and build on top of this presumption to eventually proceed towards better networks between students, professors & projects.

Getting the Data
It was a problem I said I would solve by providing structured information - the only flaw in the plan - I didn't even have unstructured information to begin withüòÜ
I remember that the first approach I had was interviewing people, asking them to share their course details, then scribbling that in a notebook, going on academic.iitm.ac.in, & actually then manually relating courses to one another in my own notion db
What seemed like a stupid idea, kept me going strong to setup a backend, come up with a simple way to interact with Notion's API & support the entire architecture just on the presumption that more interviews would mean more courses being actually manually added in the Notion DB, so I could bootstrap the MVP with a simple 150 courses to begin with.
So with that, I set out with a 1.5 month product journey, starting in February, with a launch date set for 8th March. I lost hope by 15th February itself. Part of the reason is the lack of availability of actual course data that I could work upon. So my journey then on was pretty stagnated with lagging deadlines, failed implementations & just an air of de-motivation around.

Maintaining motivation is 2/3rds of the game
~ Michael Seibel, (YCombinator, Twitch)

I started off with a simple implementation using Notion as my database. The early days actually involved me just calling up my users for interviews, asking for their course lists, noting it down on paper, using the institute website to discover relations & finally storing it all, as structured as I could, by copy-pasting stuff manually in my Notion DB.
This went on for full of February & March. I was getting nowhere.
When I returned to campus, I started to use the opportunity of being in the network to web-scrape the entire website. The only barrier - I don't know how to do web-scraping XD
That's how after a fortnight of being demoralized, web-scraping only the names of 2 entries by mimicking button clicks, was I able to finally arrive at the decision that this could not scale.
I was 22 days away from launch day, at this point, with just barely enough to go on. By now I was even starting to experience loss of faith but the only thing that I strongly agreed was the urgency of the problem statement. It really used to keep me up at night. Then......eureka !
One day, as I was randomly exploring the Chrome Dev Tools while trying a web-scraping script, I happened to observe the Network calls the institute website was making. Luckily, I had been working on Khatabook's website performance optimization & was just learning to observe each network call. Some basic digging actually led me to the call that IIT Madras was using for its own PHP backend.
Then what ? a simple script to mimic the network calls for each course number (there were 2964 !) that would extract the HTML returned, parse it into structured objects & store it in my Mongo. Lucky me, the system did not have CORS or a rate limiter on the API so a simple script was all I needed to feed all the data into my Mongo.
50 consecutive query calls, 5 hours coding, 15 minutes IITM server downtime & 5 database backups later - the database was powerful enough to start powering the app üöÄ
This was the day I saw a potential in CourseMap as a product being actually used by the students. Now the hiccup was building an ecosystem of services on top of this data right before the course registration for the next semester happened.

This was the closest this project got to an illegal action :)


The Designing
The UI had to atleast qualify certain base functionalities that I had in mind so that the MVP would atleast have a navigable graph, sorted department wise. It had to support some basic search functionality & it also had to convey the details of each course atleast in a readable manner.Thus, I used a basic React Ant Design Library to offload some part of the design system for the frontend.
The final layer

I had already passed the the deadline by 6 days

It was 24th April. I was just now 2 more days that would finalize the ultimatum of whether CourseMap was a dream or reality. I felt a kind of final-swan-song like feeling for the entire project & that's when the best work happened
Straight 20 hours coding. Barely any breaks. 1 all nighter & relentless trials later, I had a working MVP that satisfied:

Rendering the graph
Drawing prerequisite<->course relation paths
Search
A lame Firebase authentication
Analytics

Speaking of engineering decisions, I went with

react-graph-vis & ant-design wrapped in NextJS for the frontend - The reason was a great community in the web & predefined UI components, saved me a tonne of time, enabling me to build actual features instead of reinventing the wheel with custom JS & CSS. NextJS will later come in handy as a good strategy for PWA & SEO features.
NestJS for a microservice backend - NestJS feels like Django equivalent for JS. The modular, app-based way of building backend really opened doors for multiple mini-apps actually nested in my main backend. This will also help me for migrations, ORMs & further scaling my application with more & more services
Algolia for search - Industry managed "search as a service" so I wouldn't have to manage "textsearch" myself
Mixpanel for analytics - Since it's seamless to integrate & when coupled with the kind of insights it enables you to derive from your actions with Reports, Funnels, Flows - it's really kind of a one-stop-shop for analytics & studying the behavior of users using the app
Firebase for authentication - Cause I had done it before. No other reason :)

With that, I was ready to go live on 26th April, 7AM. However, I just couldn't seem to figure out the deployment due to increased browser insistence on HTTPS becoming the protocol for communication.
What followed is 1 last dance of trials with Dockerizing backend using networks, using AWS -> ELB=> ECS => Fargate to run my own containers, even trying something with AWS Lambda but then I was just not going anywhere.
One thing I was sure of but, this thing would not fail due to lack of trying from my end
Launch Day

Fun thing is, the closer I felt to completing this, the worse & more frustrating the problems got.

I consulted the engineers at a company (Khatabook) I was interning with, for their opinion on this - I got what was quite simply my last lead - a barebone implementation of TLS certificate using NGINX as a reverse proxy
I just couldn't seem to wrap my head around this concept. I left the lab 2AM, disheartened but still driven enough to regroup on pizza slices & give this 1 last shot. I was out cold on the floor by 2:45am.
The next morning dawned at 7am, with yet again rigorous attempts at the last thing that I had faith in. And then the certificate worked.......and then the TLS succeeded......then the DNS worked, and just like that, thae app was brathed into life. I remember smiling that entire day, just with feelings peace & fulfilment.
But flashback to 10:30am, when the app went live. I used college email & just spam advertising on all college WhatsApp groups I'd been a part of & communities I'd developed, right for this moment. By 11:15am, the app had crashed due to overloaded. I couldn't even SSH into the damn EC2 üòÇ

This was a big problem, but one I liked & dreamt of having. Within the next 15 minutes, I set up 15 running Docker containers of the application, load balanced with sticky sessions, by my NGINX reverse proxy & ready to give 3sec latencies to requests. As I finally pulled back from the continuous typing at 2pm, I'd never felt so content.

The nodes had come together & the graph was truly quite beautiful.

By evening, Mixpanel had blown up with inspiring analytics, people were coming up to me & thanking me for the impact the tool had in course planning - few even reached out with feature requests, suggestions & I got even good response on my engagement channels. If anything, I'm more excited to build the next set of features I've set out for now
As I shut my eyes that night, laying on the cold floor of Alaknanda Hostel (Room 365), looking at the scribblings of CourseMap UI & architecture on the whiteboards (stolen from the trash :) ) of my room, I grinned at what a relentless & rollercoaster journey it had been. And I was so ready for round 2.

The power of cheap creation, mass distribution - that I most like about software


CollegeMap a.k.a CourseMap 2.0

The next goal is to make share the journeys of past students, as seamlessly as possible, with everyone by treating each PoR, Course, Semester as nodes that are part of a bigger graph.

I've started exploring the concept of Ontology & Graph Databases now. It is exciting, it is scary & the possibilities are once again, keeping me awake at night.

Whether or not CollegeMap will succeed or not, is uncertain, but it's going to be different building this time. In fact, every time building will feel different now. The thing is, once you create something from ground up, more than the final product, you resonate with the process of always getting back up after each fall & after a point, the blows too start to feel like checkpoints to the next level. I smile a lot more now, having realized that I have found what I love doing in my life. And if that doesn't fill the soul with a fire, I don't really understand what can....

Hell, I can't already wait for the next chapter  üöÄ
Project WhatShopp,A simple attempt I made at Product Designing to understand how designers resa,Powering SMEs for ecommerce through WhatsAppThe Indian online grocery market is estimated to reach US$ 26.93billion in 2027 from US$ 3.95 billion in FY21, expanding at a CAGR of 33%. Personally, I've been quite bothered by how much of this market is addressed by the tech giants like Amazon, Flipkart, BigBasket, Grofers (now BlinkIt) & other players. I've always enjoyed great relations & friendships with the local stores I used to rush down to buy everything from fruits & vegetables, to sweets, condiments & other items of everyday use. Hence the urge for a solution that would take the pioneer of this decade's communication & embed into it, a business solution, where the MSMEs win.The Story
What started as a simple dream to work at the company who had given this as an internship ¬†problem statement, as I waited for a bus, across its massive HQ on Hosur Road, it's blazing logo shining disruptively against the silent night, led me down to a journey of 1 month, of finding the solution to find what would define a "e-commerce system, where everybody wins"The Interviews

Supposing is good, but finding out is better.

The Mom Test, by Rob Fitzgerald, beautifully explains how you can get validation about your problem statement & product, without even talking about it.
I brushed up the book on a lazy Saturday morning, and then started talking with the waiters about their grocery shopping habits. From then, I went cafe-hopping across town speaking to restaurant managers, home makers & calling friends from all over the country to understand how they'd been consuming these goods.
From their mixed buying pattern, spread across somewhat 12 mobile applications & local interactions, I was able to draw the following conclusions
Where do small shops beat large stores

Delivery radius - Large companies may not deliver in smaller village areas but a shop in village can deliver intra village
Trust - People find it easier to trust a physical person in a shop whose grain they can see and gauge quality of and hold this person accountable if quality fails. Don‚Äôt have to trust unknown 3rd party for quality
Communication- Local sellers speak local language (may or may not work to benefit) and hence it is easier to communicate the needs and understand the names of listed products (eg : Onion in english, Kaanda in Marathi, Pyaaz in hindi).
Quality Assurance- Can hold a physical person you‚Äôve met accountable for orders like grain, pulses and get it return/replaced fast as opposed to customer support and TATs in most large companies
Small items- Small items like bread, egg, stationery etc. is where these
Human touch- Feels nice to step out of house, talk to familiar faces at the pretext of still working to buy stuff. Also, these sellers have a friendly relationship with buyers.
Customer contact saving- Local shops don‚Äôt have customer details stored so difficult to get in touch with later
Minimum order- For delivery, this is a deal breaker
Discoverability is based on how well-placed your shop in terms of geography

Where do large companies beat small stores

Product discovery - Listed items are inventory-managed, kept in stock, categorized and easy-searched for easy discovery as opposed to shelves and shelves of unlisted products just in the mind of the seller.
Product availability - Since inventory is managed customers know what is available and what is not so TAT is less. For SMEs, not having inventory managed and lack of customer‚Äôs knowledge of whether item is available or not causes the buyer to make multiple rounds for purchasing an item.
Delivery - Most SMEs can‚Äôt afford extra person for delivery whilst companies can
Ingress - Platforms can handle 100000+ customers at the same instant while local seller will queue them up and take lots of time to process sale
Partial shopping- You get the best products shopped across multiple stores
Amazing UI - navigate shopping options seamlessly
High Storage capacity- Physical store takes up lot of space. Instead a godown can be optimized to just store products efficiently and deliver to buyers as orders arrive.
Uni-lingual- Interacting with a machine so no issues of a language the machine can understand like English. Can‚Äôt exactly order, in broken English from an non-fluent seller
Item packs- Offered on grofers where they package commonly required items together

After this, was an interesting phase where I went ahead & tried out all the existing solutions to experience & interact with the UI to understand design at the lowest level. This led me to actually realize some decisions that the other market players had taken, much better & ¬†come up with my own design philosophy.The Core Design Principles

For any strong vision to manifest, a powerful design should be at the core driving it.

After the conclusions from the user interviews & trying out the products myself, I went on to formulate some fundamental principles I felt should be reflected in the design of the application.
They were - Trust, Simplicity, Communication & Community

Trust

Trust is of paramount importance as quality of food is an important decision of health
People really prefer to have quality of grocery, vegetables, fruits verified so this is something that can‚Äôt be compromised
Frequency creates trust - People use WhatsApp very frequently. So they‚Äôre comfortable with the UI system. Also, they‚Äôre connected to people through this app so having purchases with acquainted sellers feels a lot more safer and faster
People in tier 2, 3 cities trust local vendors who they‚Äôve maintained friendship with as opposed to an online-platform they know nothing about, where money disappears from their accounts but they barely can use UI to track what‚Äôs happening and can‚Äôt even speak to someone for doubt.
Human on the other end, speaking in a local language instead of chatbots, answering machine feels a lot more reassuring
Payments should be seamless and of all kinds to introduce inclusivity of sellers.
Small sellers in tier 3 cities prefer COD
Option for payment after goods QA test- After quality of goods has been verified on delivery, payment should be made. Existing approach between local vendors and buyers. This beats the drawn out online QA failures, TAT for return/refund
System of ‚Äúkhata-keeping‚Äù can be kept between seller and buyer as the relationship is between a per-shop basis.  Concept of part payments over a course of time


Simplicity

Flow must be very simple so that items can be discovered easily, added to cart and paid for
Language inclusion must be provided
Too many offers, carousels, page-redirections will be very confusing - keep no. of screens involved to minimum
Product discovery must be really easy -

Powerful search
Effective filters
Powerful recommendation - To help plan orders
Comparative pricing - since in local market we often see competitive rates being the deal breaker for vendors
Setup reminders to refill stock based on shelf life




Communication

Human on the other end of a call instead of an answering machine or a chatbot.
Since small shops will have smaller customer base, they can provide the call and answer personalized service.
Result - very low TAT
The customer support and contact between the seller and buyer must be very seamless

Call to seller must be available. Video call is also nice if seller is demoing a product
WhatsApp chat will be easier to track orders with respect to a seller
Better customer support in local language


Personalized service since customer base is small so special requests can be made on whatsapp chats.

Parcel delivery timings can be configured and post-order service can be super fast.
Special requests can be made to seller to bargain, configure certain quantities
Customized rates and offers as per the seller‚Äôs convenience.


Customers are more in sync with the inventory of seller - so no returning empty handed from shop
Incentives to stay connected - No minimum order, free delivery, discounts - the driving factor behind retention of customer
Seller notifies latest store updates via whatsapp status - can be shown in story UI as a seperate category in whatsapp status.
So zero-cost advertisements. Anyways in non-urban community, trust and word-of-mouth always work a lot better


Community

A more, well-connected community with humans interacting with each other. More friendship and conversations brewing.
Discoverability to all local businesses per geography, as opposed to being at the mercy of online marketplaces for the same.
Inclusivity of sellers as opposed to an ad-manipulated environment like online product-level marketplaces Unlike them entry of new players is not difficult as  there‚Äôs no paradox of lesser orders due to lesser ratings.
Here, trust is made on conversation with an actual person living probably 2 lanes away from you.
A more equity based approach to society since instead of coming on a common platform subject to restrictions, sellers can manage their own mini-online-marts subject to their own capacities and rules
This is the only difference between a bullying intermediary platform having full control over small business, full contact with the customer and a common equitable marketplace where customers are allotted to businesses based
Celebration of culture, community and region specific goods such as local foods, fashion and daily choices instead of americanization of marketplaces.
The purpose is to also celebrate community and regional foods. By purchasing from local sellers, you build lifelong relationships of trust and friendship
Promote a culture of home-cooked meals due to managed ingredients



The Design
The design for the entire project can be found in the Figma File below :FigmaFigma Community file ‚Äî Empowering Indian SMEs‚Äô ecommerce through WhatsAppFigmaAlso, here's a simple Kanban view of the most important design decisions I took, why I took them to align with my core design philosophy

The Conclusion
This was the 1st project where I handled all the pre-engineering part & it gave me a holistic sense of how a product is designed & why some decisions like this, can be game changers when catering to a large user base.
In a way, it shaped me to perceive products very differently since :)
How to HTTPS your API,Expose your API over HTTPS for free using an NGINX proxy server,Serving your API over HTTPS is becoming more and more crucial by the day, especially from a usability perspective since the browsers are shedding all the HTTP and other unsecure, legacy support nowadaysIn this blog, I will teach you by example, how to generate an SSL/TLS certificate for any API you have & then use a proxy server (developed using NGINX) to basically start serving your simple HTTP API over HTTPS Step 1 - The Domain Name Server (DNS) Resolution
You need a domain record (A or AAA) that basically acts as the entity that the certificate will be generated for
The only step to do here is to use for DNS service create a simple A record that points to the server you are going to use to host the API.
For me, I did it like this for a project :

Before you start the next step, make sure the DNS has propagated properly by running the following commands
Replace api.domain.com with your own API_DOMAIN_NAME
dig api.domain.com
nslookup api.domain.com

Next, we will generate a certificate attached to this domain.


Step 2 - Certificate generation
You can generate a public certificate & private key pair for free using letsencrypt.
For this, you 1st install the package on your machine
sudo apt install letsencrypt

Next you generate a simple certificate for the domain you want to certify using the certbot API that you can now access
For this step you need your port 80 unblocked so get that done by simply running
sudo fuser -k 80/tcp
Then you can safely run
sudo certbot certonly --standalone

Now, the CLI will prompt you to enter the domain name (here, it is api.domain.com) & then it will generate a pair of a public certificate & a private key
This is what will be actually used by our proxy server to even respond to requests to this API_DOMAIN_NAME


Step 3 - Proxy server setup
This is a simple nginx server we will setup that will listen on port 80 (HTTP) & port 443 (HTTPS) of our server & redirected it to our api's port.
We just pass the location of the generated certificate-key pair to NGINX in its configuration file
For writing a nginx.conf while, first install nginx
sudo apt install nginx

First stop running nginx service by running
sudo nginx -s stop

Next you should go to the directory where this configuration file is stored & override the code with the simple proxy code you have
cd /etc/nginx
sudo nano nginx.conf

In there, we specify a simple rule to pass the 80 (HTTP) & 443 (HTTPS) traffic to our backend. Here, I have even gone ahead & made this a load balancer with stickiness by specifying my api resolvers running on ports 8080, 8081, 8082 & add a #ip_hash configuration option so that once the client gets associated with one api, its requests are always directed there:
And we pass the locations of the generated public certificate & private key pair to the nginx rule that is resolving port 443 using SSL.
Here's my configuration file
For you api.domain.com would be whatever your API_DOMAIN_NAME is
http {
    
    upstream allbackend {
        #ip_hash;
        server 127.0.0.1:8080;
        server 127.0.0.1:8081;
        server 127.0.0.1:8082;
    }


    server {
          listen 80;
          listen 443 ssl;
          ssl_certificate /etc/letsencrypt/live/api.domain.com/fullchain.pem;
          ssl_certificate_key /etc/letsencrypt/live/api.domain.com/privkey.pem;
          location / {

              proxy_pass http://allbackend/;
           }   
     }



}

events { }

Then simply, clear any pending services on ports 80 & 443 restart nginx on them using :
sudo fuser -k 80/tcp

sudo fuser -k 443/tcp

sudo service nginx restart

That's that ! You've made your API HTTPS now
Distributed Graph Backend - Logistics System,What if all the stakeholders in a logistics system were just nodes in a graph ?,Development of a distributed graph backend for a logistics system
Overview
In today's world Logistics is considered as the backbone of an economy. Being the fastest evolving industry the Indian logistics sector is currently growing at a rate of 10.5% CAGR since 2017 and estimated to be of $225 Bn by the end of 2022.
Hence, there is an upsurge in demand of efficient algorithms for packing & routing in the several logistics systems running at a national, state, local & hyper-local level
Along with the need of such ground-breaking algorithms, that the Logistics Lab at IIT Madras is working upon, also arises the need to create a software layer that interacts with the users & the developers of the application in the most seamless manner possible.
The logistics sector is quite convoluted with multiple stakeholders involved in it interconnecting with each other on multiple levels. Hence, the approach was picked to view each entity in this system as a node in a graph to create a software layer that can adequately study the relations between them.
Goals
The project was divided into 2 main goals

Develop a framework for modeling various entities in the graph-based network proposed, & simulate the interactions between them with generated data that abides certain constraints observed in real world scenarios.
Set up a distributed system infrastructure that is redundant, scalable, cost-optimized & highly performant.

Apart from these there were several other objectives such as forming & training a team, setting up development environment & CI/CD pipelines & documentation of the entire process that I undertook as part of this project.
A High Level Design Overview
A high level overview of the system would be as follows :

Databases -

1 Master database that stores all the data for the graph entities in a structured format.
1 Database that stores the information coming from Maps API for hitting the paid API endpoints only when new data is asked for
1 key-value database that handles authentication, authorization & user session management.


Business logic server - The main server that :

Sets up the graph based connectivity across various tables in our database & exposes a set of endpoints to granularly understand the relations between these data points & perform calculations & manipulations over them,
A server for managing authentication & authorization with inbuilt role-based access control set up for finer management of roles
Communicates with other microservices in our architecture


Miscellaneous services used :

API documentation - For easy understanding of the available endpoints & mock request-responses to integrate easily
Containerization - For extracting server logic into reusable software chunks so that scaling the system is as easy as spinning up another container running similar code.
Cloud provider - For hosting our servers & website in the initial days of building the MVP.
UI Prototyping - To mock each design & debate feature utility before developing the same.
Remote Version Control System - To collaborate on developing different features with different developers in parallel,
CI/CD - For automation of repititive tasks, via usage of webhooks over events on code repositories. This also involved setup of cron jobs over remote servers.



Below img displays a systems diagram for entire software stack developed over the project course

Tech Stack

React UI library used for developing our frontend - a responsive web-application
React Query - A client for connecting the React state changes with the API endpoints written in GraphQL. Basically it‚Äôs the intermediary to interpret UI component functions performed and pass them to the APIs for resolving.
TypegraphQL APIs - APIs written in GraphQL (with added type definitions) :

GraphQL is self documenting - Helps save developer effort in documenting each API endpoint
Helps reduce latency and load by selective querying
Scales very well with data points
Scale really well with wide column data entries


TypeORM - A simple Object Relational Mapping (ORM) to convert the CRUD requests from our API into the format that the connected database can understand, in this case, it means converting the queries (functions to read data) and mutations (functions to manipulate data) of our GraphQL endpoints into SQL queries that our Postgres database can interpret
Postgres - An open-source simple SQL based database with a fast, powerful engine.
Redis - Redis is an open source (BSD licensed), in-memory data structure store used as an auth database here owing to its high speed.
Docker - The backbone of the entire CI/CD pipeline that makes setting up the entire architecture very seamless with fast software updates & production deployment rollouts
GitHub Actions - For CI/CD with docker image tagging & publishing on feature releases
Docker-compose - The Infrastructure as Code solution that makes the entire architecture easily understandable & scalable to infinite extent
MongoDB - A noSQL database used here as the spatial database holding all the variable information that is received from Google Maps API & storing it for fast, easy & scalable access.
Express - A node JS HTTP framework used here for creating APIs for authentication, session management & creating spatial API endpoints.
Language used for development is TypeScript

This convoluted, yet beautifully intricate tech stack was achieved as a result of multiple iterations over the problem statement & each blocker we had run into while we were developing the same.
Through this report, I will be going over the key software engineering decisions taken over 4 major areas encompassing the architecture of this entire pipeline :
PART 1 : API structure of stakeholder entities with TypeGraphQL, TypeORM
A : Sample CRUD Endpoints


In this section, we discuss how the below portrayed interactions between various stakeholders of an atomic Logistics System network was put on a graph based network, & the engineering benefits associated with it


Database connections are made through environment configuration variables that differ for development & production. In this section, the main databases we‚Äôre connecting to is :

Postgres - For storing structured, well-defined that each entity constitutes of



The documentation for the entire backend can be found in a Postman collection with multiplie environment configuration & mock request/response setups that I have done while I was testing each API endpoint


We start of with a sample code of an entity ‚ÄúCoordinates" where, just by defining a simple class in TypeScript, we are able to actually create a table called coordinates that has the columns as defined as properties in the class
// src/entity/Coordinate.ts

import { Entity, BaseEntity, PrimaryColumn, Column } from "typeorm";
import { ObjectType, Field, ID, Float } from "type-graphql";

@Entity("coordinates")
@ObjectType()
export class Coordinates extends BaseEntity {
  @Field(() => ID)
  @PrimaryColumn()
  readonly id:string;

  @Field(() => Float)
  @Column({ type: "float" })
  latitude: number;
  
  @Field(() => Float)
  @Column({ type: "float" })
  longitude:number;

}



We now write an API resolver over this model/entity/schema that helps us query this specific table with standard Create, Read, Update, Delete (CRUD) features + any miscellaneous functionalities that we wish to add


Witnessing a repeated need for the same CRUD functionalities required for each of the 15 stakeholder entities, I created an EntityBaseResolver function that simply takes in the details of the entity & returns a BaseResolver class with CRUD endpoints enabled over it.
The CoordinatesResolver now simply extends this EntityBaseResolver and if required, provides additional features other than CRUD over it


Here is the code for the EntityBaseResolver file. I have also added Authentication & Role Based Access Control over each endpoints inside the @UseMiddleware  method decorator of this Resolver class
import { getConnection } from 'typeorm';
import { Arg, ClassType, Mutation, Query, Resolver, UseMiddleware } from "type-graphql";
import { Middleware } from "type-graphql/dist/interfaces/Middleware";
import { v4 as uuidv4 } from 'uuid';
import { isAuthn } from '../middleware/isAuthn';
import { isAdmin, isDriver } from '../middleware/isAuthz';

function EntityBaseResolver<X extends ClassType,Y extends ClassType>({
    suffix,
    tableName,
    entity,
    createInputType,
    updateInputType,
    middleware
  }: {  
        suffix: string,
        tableName:string,
        createInputType: X,
        updateInputType: Y,
        entity: any,
        middleware?: Middleware<any>[],
    }
) {
    @Resolver({isAbstract:true})
    abstract class BaseResolver {
        @Query(() => [entity], { name: `getAll${suffix}` })
        @UseMiddleware(isAuthn,...(middleware || []))
        async getAll():Promise<typeof entity[]> {
            let connection = await getConnection()
                .createQueryBuilder(entity, tableName)
                
            return connection.getMany()
        }

        @Query(() => entity, { name: `get${suffix}ById` })
        @UseMiddleware(isAuthn,...(middleware || []))
        async getById(@Arg('id') id:string):Promise<typeof entity> {
            let connection = await getConnection()
                .getRepository(entity)
                .createQueryBuilder(tableName)
                .where(`${tableName}.id = :id`, {id})

            const entry = connection.getOne();
            
            if (!entry) throw new Error(`${suffix} not found!`);
            return entry;
        }

        @Mutation(() => entity, { name: `create${suffix}` })
        @UseMiddleware(isAuthn,...(middleware || []))
        async create(@Arg("data", () => createInputType) data: typeof createInputType):Promise<typeof entity> {
            const id = uuidv4(); 
            const entry = {['id']:id, ...data}
        return entity.create(entry).save();
        }

        @Mutation(() => entity, { name: `update${suffix}` })
        @UseMiddleware(isAuthn,...(middleware || []))
        async update(@Arg("data", () => updateInputType) data: typeof updateInputType, @Arg('id') id:string):Promise<typeof entity> {
            const entry = await entity.findOne({ where: { [`id`]: id } });
            if (!entry) throw new Error(`${suffix} not found!`);
            Object.assign(entry, data);
            await entry.save();
            return entry;
        }

        @Mutation(() => entity, { name: `delete${suffix}` })
        @UseMiddleware(isAuthn,...(middleware || []))
        async delete(@Arg('id') id:string): Promise<typeof entity> { 
            const entry = await entity.findOne({ where: { [`id`]:id } });
            if (!entry) throw new Error(`${suffix} not found!`);
            await entry.remove();
            return {id};
        }
    }
    return BaseResolver;
  }
  
export default EntityBaseResolver;



All our CoordinateResolver does now is extending this BaseResolver returned to provide simple CRUD endpoints.


Thus, we have succeeded in defining & handling the table definition with TypeORM, CRUD endpoints with TypeGraphQL, all made possible through defining just some basic decorated TypeScript classes.


Here‚Äôs the sample Postman response for a POST request our Coordinates  to create a new row in the coordinates table in our database


B : The Graph Relations


Relation mapping between different tables of our database was an engineering challenge that I made structured & streamlined using TypeORM‚Äôs relation mapping & a custom code dependency injection framework I developed as part of my research, specifically for handling the interactions between these entities more structuredly & intuitively


Say for instance the Address entity, which has a ManyToOne mapping against my Coordinates entity. We mention this using the TypeORM relation mapper with the Foreign Key. Also, this is created as a unidirectional relationship as per the product requirement
// src/entity/Address.ts

import { Field, ID, ObjectType, registerEnumType } from "type-graphql";
import { BaseEntity, Column, Entity, JoinColumn, ManyToOne, PrimaryColumn, RelationId } from "typeorm";
import { Coordinates } from "../coordinate/Coordinate.entity";

export enum AssociatedEntity{
    Consignor = "consignor",
    Consignee = "consignee",
    Depot = "depot",
    Operator = "operator",
    Driver = "driver"
}

registerEnumType(AssociatedEntity, {
    name: "AssociatedEntity", // this one is mandatory
    description: "The entity associated with the transport activity", // this one is optional
});
  
@Entity("addresses")
@ObjectType()
export class Address extends BaseEntity{
    @Field(() => ID)
    @PrimaryColumn()
    readonly id: string;

    @Field(() => String)
    @Column("varchar", { length: 45 })
    address_line_1: string;
    
    ........

    @Field(type =>Coordinates)
    @ManyToOne(type => Coordinates)
    @JoinColumn({ name: "coordinate_id", referencedColumnName:"id" })
    coordinates: Coordinates;
    @RelationId((address: Address) => address.coordinates)
    coordinate_id: string;

    @Field(() => AssociatedEntity)
    @Column({
        type: "enum",
        enum: AssociatedEntity,
        default: AssociatedEntity.Driver
    })
    associatedEntity: AssociatedEntity
    
    // Write a custom pattern for this
    @Field(() => String)
    @Column("varchar", { length: 45 })
    pincode: string;
}



The above snippet also shows how gracefully Enumeration types have been handled in the backend & the database so that we have no data validation issues on integrations later


Next, we define the AddressResolver , equipping basic CRUD functionalities with our core EntityBaseResolver & then using our custom inversion-of-control framework to inject connection to the Coordinates entity & return joint data.
These joins also have been equipped with thorough validation checks & error fallbacks to prevent any data inconsistency in the database
import { Repository } from 'typeorm';
import { Coordinates } from '../coordinate/Coordinate.entity';
// src/resolvers/AddressResolver.ts

import { FieldResolver, Resolver, Root } from "type-graphql";
import { Address } from "./Address.entity";
import { CreateAddressInput } from "./AddressInput/CreateAddressInput";
import { UpdateAddressInput } from './AddressInput/UpdateAddressInput';
import { InjectRepository } from 'typeorm-typedi-extensions';
import { Service } from 'typedi';
import EntityBaseResolver from '../../abstractions/EntityBaseResolver';

const AddressBaseResolver = EntityBaseResolver({
  suffix: "Address",
  tableName: "address",
  entity: Address,
  createInputType: CreateAddressInput,
  updateInputType: UpdateAddressInput,
})

@Service()
@Resolver(of => Address)
export class AddressResolver extends AddressBaseResolver {
  constructor(@InjectRepository(Coordinates) private readonly coordinatesRepository: Repository<Coordinates>) {super() }

  @FieldResolver()
  async coordinates(@Root() address: Address): Promise<Coordinates> {
    return (await this.coordinatesRepository.findOne(address.coordinate_id))!;
  }
}



Here‚Äôs how, we were able to develop the 1st in-house technology of the lab, a seamless, scalable & data-consistent framework for creating configured API modules for various entities in a graph-based backend system


The codebase for developing each entity is 90% lesser than any other competing framework out there, making the abstractions we‚Äôve used light, powerful & abiding with the SOLID principles of Object Oriented Design


The true power of this system is seen in executing complex queries.  Below attached is the screenshot of the custom detailing of information we can get while executing a POST request to CreateConsignment for the Consignment entity in the consignments table in our database, & getting the granular level information about each of its related entity mappings in the Address, Coordinate & Operator entity



And yet the code for heavy lifting CRUD on such a wide column table, inter-related to multiple other tables is under 50 lines (90% less than any other marketframework)!
import { ContainerOnPlatform } from '../containerOnPlatform/ContainerOnPlatform.entity';
// src/resolvers/ConsignmentMovementResolver.ts

import { Resolver, Query, Mutation, Arg, FieldResolver, Root } from "type-graphql";
import { Service } from "typedi";
import { InjectRepository } from "typeorm-typedi-extensions";
import EntityBaseResolver from "../../abstractions/EntityBaseResolver";
import { Address } from "../address/Address.entity";
import { Coordinates } from "../coordinate/Coordinate.entity";
import { ConsignmentMovement } from "./ConsignmentMovement.entity";
import { CreateConsignmentMovementInput } from "./ConsignmentMovementInput/CreateConsignmentMovementInput";
import { UpdateConsignmentMovementInput } from "./ConsignmentMovementInput/UpdateConsignmentMovementInput";
import { Repository } from 'typeorm';

const ConsignmentMovementBaseResolver = EntityBaseResolver({
  suffix: "ConsignmentMovement",
  tableName: "consignment_movements",
  entity: ConsignmentMovement,
  createInputType: CreateConsignmentMovementInput,
  updateInputType: UpdateConsignmentMovementInput,
})

@Service()
@Resolver(of => ConsignmentMovement)
export class ConsignmentMovementResolver extends ConsignmentMovementBaseResolver{
  constructor(
    @InjectRepository(Coordinates) private readonly coordinatesRepository: Repository<Coordinates>,
    @InjectRepository(Address) private readonly addressRepository: Repository<Address>,
    @InjectRepository(ContainerOnPlatform) private readonly containerRepository: Repository<ContainerOnPlatform>
  ) { super() }

  @FieldResolver()
  async start_coords(@Root() consignment: ConsignmentMovement): Promise<Coordinates> {
    return (await this.coordinatesRepository.findOne(consignment.start_coord_id))!;
  }

  @FieldResolver()
  async end_coords(@Root() consignment: ConsignmentMovement): Promise<Coordinates> {
    return (await this.coordinatesRepository.findOne(consignment.end_coord_id))!;
  }

  @FieldResolver()
  async start_address(@Root() consignment: ConsignmentMovement): Promise<Address> {
    return (await this.addressRepository.findOne(consignment.start_address_id))!;
  }

  @FieldResolver()
  async end_address(@Root() consignment: ConsignmentMovement): Promise<Address> {
    return (await this.addressRepository.findOne(consignment.end_address_id))!;
  }

  @FieldResolver()
  async container(@Root() consignment: ConsignmentMovement): Promise<ContainerOnPlatform> {
    return (await this.containerRepository.findOne(consignment.container_id))!;
  }
}



B : The Data specificity & the speed

TypeGraphQL allows us to query for only the data that is required from the endpoint, hence,

Response handling on client side is much smoother during Continuous Integration
Data packets downloaded & transferred are lesser, making the load on the server & database extremely optimized


This data specificity also helps with the speed of query execution, giving a latency of 52 miliseconds which is 300% faster than any contemporary technology
Lastly, this technology also sped up development cycle of data-specific CRUD endpoints on our models from a day to 10 minutes (with API testing) which accelerated the feature delivery.

PART 2 : Authentication, Authorization, MFA & Session Management with Redis
A : Choosing Redis

Redis is the in-memory database solution in the current architecture that handles the entire authentication, authorization, role-based access control & session management of user using the SaaS
All Redis data resides in memory, which enables low latency and high throughput data access. Unlike traditional databases, In-memory data stores don‚Äôt require a trip to disk, reducing engine latency to microseconds. Because of this, in-memory data stores can support an order of magnitude more operations and faster response times. The result is blazing-fast performance with average read and write operations taking less than a millisecond and support for millions of operations per second.

B : The System Setup


The Logistics Lab authentication kicks off with the user registering & their entry being created in the Postgres database, as well as Redis token that gets generated for enabling Multi-Factor Authentication


The user receives an email with the URL containing the token that when submitted & verified by Redis, unlocks the user account for Login & other subsequent activity


For the email sending, an SMTP server was configured with the necessary credentials



Post-login, the user‚Äôs Role [ Operator, Admin, or Driver ] restricts the endpoints they can be consuming, as per the middlewares passed to the endpoints.


Following is sample where Authentication was setup & Roles were defined, with master access for the Admin role
export const isAuthn: MiddlewareFn<AppContext> = async ({ context }, next) => {
  const userId = (context.req.session! as any).userId
  const user = await getRepository(UserAccount).findOne(userId);
  if (!userId) {
    throw new Error("not authenticated");
  }
  return next();
}

const isAuthz = (role: Role): MiddlewareFn<AppContext> => async ({ context }, next) => {
  const userId = (context.req.session! as any).userId
  const user = await getRepository(UserAccount).findOne(userId);
  if (!userId || (user?.role !== role && user?.role !== Role.Admin) ) {
    throw new Error("You are not authorized to perform this action");
  }
  return next();
};

export const isDriver = isAuthz(Role.Driver);
export const isAdmin = isAuthz(Role.Admin);
export const isOperator = isAuthz(Role.Operator);



To pass middleware on an endpoint, we simply use the @UseMiddleware() decorator on the method & pass in first, the criteria for authenticating, & then an array of middlewares which are the roles that are authorized to access given endpoint.
Here‚Äôs a snippet of securing a Query defined in the EntityBaseResolver using our authentication & authorization middleware
@Query(() => [entity], { name: `getAll${suffix}` })
        @UseMiddleware(isAuthn,...(middleware || []))
        async getAll():Promise<typeof entity[]> {
            let connection = await getConnection()
                .createQueryBuilder(entity, tableName)
                
            return connection.getMany()
        }



By default, Redis tokens for user sessions end after a month. Can be modified as suited.


Systems have also been incorporated to help user regain & reset their password using their email address & a separate Redis token for the same. The code for the same is found in the [UserAccountResolver](https://github.com/Logistics-Lab/typegraphql-api/blob/master/backend/src/modules/userAccount/UserAccount.resolver.ts) file


PART 3 : Spatial Data Generation & Storage


A big part of this project was data generation.


Sample data generation for the Postgres database tables was done using https://mockaroo.com and a bunch of NodeJS scripts & stored in a Google Sheets spreadsheet.


Apart from that, as per the algorithms team requirement, a system needed to be developed for returning points of interest in a given search radius, for working of routing & packing algorithms for the same


After much research on Google Maps API & MapMyIndia, Google Maps API was chosen owing to its accuracy, ease of integration & pricing model


However, for repeated queries, to prohibit extra cloud spend, each document for each location we get back from Google on the first query, was cached into a MongoDB database so that :

subsequent similar query would be faster,
Subsequent similar query would be free of cost
With more research, more valuable spatial data would be easily available & structured in our own custom database



To combat the pagination Google Maps API enforces, a intelligent debouncing service was written for each call to the next page & for the same request, all results from all pages were aggregated & stored into MongoDB
Following is the code for the intelligent service that the our spatial endpoints consume to :

fetch data directly from MongoDB (for repeat queries) OR
populate MongoDB with data from Google Maps & then fetch from it

const locationService = async(city:string, buildingType:string, resultMapper:(results:any[])=>any[]) => {
    try {
        await mongoose.connect(`mongodb://${process.env.MONGO_INITDB_ROOT_USERNAME}:${process.env.MONGO_INITDB_ROOT_PASSWORD}@mongodb:27017/spatial-db?authSource=admin`);
        const results = await Location.find({ city: city, buildingType: buildingType }).exec()
        if (results.length > 0) {
            console.log('coming from mongodb directly', results.length)
            return resultMapper(results)
        } else {
            console.log('saving to mongodb first')
            storeResultsInMongo(city, buildingType);
            await sleep(10000);
            const results = await Location.find({ city: city, buildingType: buildingType }).exec()
            console.log('coming from mongodb indirectly', results.length)
            return resultMapper(results)
        }
    } catch (error) {
        console.log('error connecting to mongodb, so proceeding with google maps API', error)
        throw new Error('could not fetch results');
    }
}

const sleep = (ms:number) => {
    return new Promise((resolve) => {
      setTimeout(resolve, ms);
    });
  }

const storeResultsInMongo = async (city: string, buildingType: string) => {
    const queryToGMapsAPI = `https://maps.googleapis.com/maps/api/place/textsearch/json?key=${process.env.GOOGLE_PLACES_API_KEY}&query=${buildingType}%20in%20${city}&type=${buildingType}`
    const resFromGmaps = await axios.get(queryToGMapsAPI) as any
    let results = resFromGmaps.data.results;
    results.forEach(async (result: any) => {
        const location = new Location({ city: city, buildingType: buildingType, ...result });
        await location.save();
     })
    let nextPageToken = resFromGmaps.data.next_page_token
    while (nextPageToken && nextPageToken.length > 0) {
        try {
            await sleep(1000);
            const nextQuery = `${queryToGMapsAPI}&pagetoken=${nextPageToken}`
            console.log(nextQuery, 'is ready')
            const nextResFromGmaps = await axios.get(nextQuery) as any
            let newRresults = nextResFromGmaps.data.results;
            let newStatus = nextResFromGmaps.data.status;
            newRresults.forEach(async (result: any) => {
                const location = new Location({ city: city, buildingType: buildingType, ...result });
                await location.save();
            })
            nextPageToken = nextResFromGmaps.data.next_page_token
            console.log(newStatus, nextPageToken)
        } catch (error) {
            console.log('No more paginated responses', error)
        }
    }
}
export default spatialAPIRouter;



MongoDB was chosen to provide the flexibility of noSQL since spatial data is still quite inconsistent & a column-rigid SQL option would prevent the use case from scaling


On top of MongoDB various Express API endpionts were exposed for querying different fields of the collected data, as per the utility. Proper documentation, environment setup & test requests for the same have been provided in our this API collection


PART 4 : Continuous Integration / Continuous Deployment


Any software at scale is equipped with this key engineering milestone to accomodate exponential growth in usage, feature development cycles & fault-tolerant product rollouts


The diagram shown above takes exactly 2 minutes to setup for any developer onboarding the code, irrespective of the machine it is being operated upon since the entire business logic, environment configuration & failure orchestration



The diagram shown below helps setup a fresh server for handling production load for our entire system in 10 minutes. This is the power of containerization technology & Infrastructure as Code concepts well-integrated into this architecture



A : Containerization with Docker

To ensure maximum flexibility and replication of the code across development, staging & production environments, the business logic server was containerized into a movable block of code containing all the configuration details of its function inside it
With any machine running the docker-daemon, the code will function exactly the same irrespective of the OS, File System management or other services that differ across computers.

B : Multi-Container Management with Docker-Compose


All the systems mentioned - Business Logic Server, Redis Database, MongoDB database & Postgres Database - run as containers through their respective Docker images


All these Docker containers run in the exact same network making the communication between them seamless & replicable very easily in any environment



Name
Phase
Requirement
Solution




Business Logic Server
Development
Code should be mapped to a file system volume inside development machine & support hot module reloading.
Created a bind mount that symlinks the container to the host development machine so that developer can immediately observe their changes & how they impact the entire system




Imported packages of the Container must be specified on host when developing
An anonymous docker volume was created to handle imported packages inside container work well during development with local code changes



Staging
Docker image must be built & pushed to mimick production behavior
Docker image with production configuration is built & pushed to container registry at Docker Hub after simulating production behavior on local



Production
Code running must be equipped with the latest feature rollouts
Cron job running at 2AM IST everyday that reboots the system with the latest code (downtime - 45 seconds, healthcheck alerts in place)



All
Environment configurations must be stored separately from business logic
Dedicated environment files providing configuration during container build time to switch between mimicking environment behavior seamlessly


Redis DB, Mongo DB, Postgres DB
All
Data persistence across container reboots in case of container crashes
Setup using a named volume between the database & the host machine so data will never be lost











Here‚Äôs the code sample for the docker-compose file that orchestrates this entire setup in around 2% of the time taken by a traditional system setup & software update rollout.
version: '3.8'
services:
  postgresdb:
    image: postgres
    restart: always
    volumes:
      - postgresdata:/var/lib/postgresql/data
    env_file:
      - '../../env/postgres.env'
  redisdb:
    image: bitnami/redis
    ports:
      - '6379:6379'
    volumes:
      - 'redisdata:/bitnami/redis/data'
    env_file:
      - '../../env/redis.env'
  mongodb:
    image: 'mongo'
    volumes: 
      - mongodata:/data/db
    env_file: 
      - ../../env/mongo.env
  api-server:
    build: 
      context: '../../backend'
      dockerfile: 'dev.dockerfile'
    restart: always
    ports:
      - '8080:8080'
    volumes:
      - '../../backend:/app'
      - /app/node_modules
    env_file:
      - '../../env/dev.api-server.env'
      - '../../env/postgres.env'
      - '../../env/redis.env'
      - '../../env/mongo.env'
    depends_on:
      - postgresdb
      - redisdb
volumes:
  postgresdata:
  redisdata:
    driver: local
  mongodata:



GitHub actions  were set in place to trigger updating the latest tagged container on our Container Registery everytime a major version of the API was released


Webhooks were setup to auto-generate documentation & update Postman Schema everytime the code on the remote was updated.


Thus, using this well-crafted end-to-end system for feature delivery, the entire project encompassing such a high-intensity & widespread use case was bootstrapped & architected for scale in a matter of 3 months.


Afterword
As a kid, I'd always been amazed by how stakeholders in a system can be treated as nodes in a graph & here, just creating something so pure, scalable & powerful was a really cool journey.
I remember my heart thumping as I executed the complex graph query on the Consignment entity in the system, or when running docker-compose up in my production server, and the smile that came after months of pouring my soul into architecting something I just conceived of as a cool project.
Fun thing is, I can't wait to do it all over again !
Web Push Notifications,A working knowledge understanding sending notifications in web-apps,And just like that, now web apps are 1 step closer to mobile apps

What & why
Web push is the ability of a web-application to send the user notifications as a background process, just like a native mobile application
What happens is that your browser runs scripts in the background that fire a notification to the user on the behalf of the target web-app, when these browser scripts receive a message from a notification server
Explaining it to a 5 year old

There are 3 main entities involved in this setup :

App notification server - sends notifications to a registered subscriber
Browser push service - A feature provided by the browser which serves as a medium of communication between the app notification server & the service worker on the client
Service worker on the client - Responsible for setting up subscription object & displaying the notifications of messages it receives




I will explain this process in 2 phases
Making the client web-app subscribe to the notification server

When the service worker (just a JS file, running in the background in the browser) is registered, we get a registration object that contains the details of this registered service worker.
On this object, we call the pushManager.subscribe() function to get a subscription object. The client will see a ‚Äòdo you want to allow notifications‚Äô banner in their browser. If they say yes, the subscription is successful & we get a subscription object. If they say no, the process just exits here.
In this subscription object, we have the subscription endpoint + publicKey.
The subscription endpoint is the the exact address of the client web app on the given browser in the given device being used.
The public key is just for encrypted communication between client web-app & app notification server.
We convert this subscription object to JSON & send it to the our app notification server.

Displaying messages from the notification server as notifications on the client web-app

Now, the app server, whenever it sends a message, it is received by the Browser‚Äôs push service, which resolves it to the respective client using the subscriptionId param inside the endpoint URL & then it directs the decoded message (using the public key) to the necessary client application with a push event.
This push event is then picked up by the service-worker registered on the client, who parses the title & body of the message & shows it as a notification on the client using the  registration.showNotification() function.
And thus, the client is able to register for notifications & then receive them.
The subscription object might change from time to time on the client so we should always check for this object & send it to the app server accordingly
For any web push notification feature with a browser there are 3 main components :

App notification server - eg : CleverTap, MessageBird
Push service on the browser - eg : Google Chrome browser managed push-service, Mozilla Firefox's browser push-service, Microsoft Edge's browser based push service.
Service worker of the app - Just a JS file on the client side that will help make the client become a subscriber of the notifications & show notifications whenever received.



Conclusion
This is the basic procedure of how web push notifications work in Progressive Web Apps, hypercharging them with the ability to send notifications even when the web app is not open in the user's device.
Why Work at a Startup ? - Part III,The chapter on finding family, facing loss, holding onto faith no matter what.,You should be a pebble in the pond that creates the ripples for change.~Tim Cook (Apple Inc.)NoidaLarge political banners. Stone monuments. Thousands of epic tales and a millenia of heritage. Delhi woke to a fresh morning as my flight hit the tarmac of Indira Gandhi International Airport, New Delhi, on 1st March."Chapter 2", I exhaled as I heaved my luggage of the conveyor belt and texted Aman Bhaiya about my arrival .This time I wasn't nervous. The Hackers had taught me better. Yet like always, I did manage to screw up. I called cab to the wrong pickup point, gave the wrong destination and after about 3 hours and ‚Çπ1500 down, I finally found the office space (Noida, Uttar Pradesh, India). A couple of painters and workers were doing some renovation work. I got into an argument with one when I claimed I'd come to stay, but he wouldn't listen saying this villa had already been booked and I couldn't get it.Hearing this commotion, one of the people inside walked out. Signature black Van Heusen T shirt, knee-length jeans, an ever-dead Apple Watch, square spectacles, gruff beard and an air of confidence and focus that's kind of unique to him. My 2nd meeting with Aman Bhandula, CEO, Farmako.Building a homeYou can dream, create, design, and build the most wonderful place in the world. But it requires people to make the dream a reality. ~Walt DisneyThe office space is fondly referred as farm house by everyone. It's a spacious 3 storey villa with a beautiful backyard and lake view. The founders had envisioned the ground floor to be an office space and the upper 2 floors for residence. This phase speaks more about building a home and creating an environment where team members would feel excited to work on something cool.(Left to Right) Aman Bhandula and me setting up the "hackers" section in office¬†A lot of the initial days mostly involved setting up equipment. Initially, it was just me and Aman Bhaiya. We ate off of a simple picnic setup of takeaways and towel-tablecloths. We oversaw delivery, management and setup of common household and office equipment. Aman Bhaiya is what Kaishu Bhaiya jokingly calls a "hackist" - combination of a hacker and an artist. Nikhil Bhaiya will just tease him about dropping out and currently doing managerial and design work, basically non-programming work ("Alt + Click is barely any work" ~Nikhil Kumar). I personally think he's a great leader, honest friend and most important - a good person. Like the Hackers, he carries his unique entrepreneurial spark coupled with a clear vision, an extremely high pain and patience index and a great heart, fair to everyone.Case in point : Aman Bhandula won't care about wearing the same design T Shirt (classic black Van Heusen) everyday (hacker) but he will be super skeptical about tiny details such as the font size or the padding when it comes to UI of the website (artist) or say something like the accuracy of the Machine Learning voice recognition model he trained for the SaaS product for doctors.A week later, Kaishu and Nikhil Bhaiya also showed up from Goa after a series of comic incidents on fighting with packers and movers and a small funny scooter accident to cap it off. To this day, you'll hear Nikhil Bhaiya chuckle - "I always knew travelling with Sahu would someday end in an accident".The Founders in the conference room at farm houseBuilding a culture starts from the place you're expected to do your best work. A lot of thought had been put into the entire setup, right from the principles hung up in the conference room to the design of the bean bags in the break room. We'd be moving around all day - heaving around things, installing equipment, wiring up an efficient internet connection, ¬†mounting bookshelves and a lot more. I still remember the day the work tables and office chairs showed up and the amount of effort and dis-assembling and re-assembling it took to get this massive conference table through a small door.It is not the beauty of the building you should look at: it‚Äôs the construction of the foundation that will stand the test of time. ~David Allen CoePost attending my online college lectures during the day (which earned me the reputation of school kid) ¬†I'd return to code by late afternoon. I'd get my tasklist from Aman Bhaiya and start shipping code. In late evening, we'd take a small break and play squash, then dinner and then back to ideating on the whiteboard and shipping some more code. The most amazing part about this was that it always felt energizing all the time. And some crazy moments also did happen. We majorly drank juices and soda for about the first 10 days, since due to some packers & movers issue, our water purifier's delivery was messed up. So barely drank water, and at one point Nikhil Bhaiya got so frustrated that at 11:30 he used the a food delivery service to simply order 3 bottles of water. Another story involved me breaking into farm house from a neighboring villa after locking us out. There's also the incident of me and Aman Bhaiya almost setting fire in the lawn as we tried to barbecue food using wood shards, instead of coal (talk about learning thermodynamics), as fuel. ¬† ¬†(Left to Right) Aman Bhandula handling the drill, Kaishu Sahu doing tech support, Nikhil Kumar defrosting CokeWe even visited Indian Institute of Technology (IIT), Roorkee, the college which brought the founders together, birthplace of Farmako. I had a blast glimpsing into their life, sharing their friends, dreams and memories. Kaishu Bhaiya completed the promise he'd made to clean his room once they got accepted into Y Combinator. Aman Bhaiya met old friends and relived nostalgic moments. Cool things I did involved breaking a lock , playing the guitar on a rooftop and slipping into the library or any-place on campus, posing as a student the entire time.Around this time, I met Ankita Didi, this brilliant, cheery graphic designer in the team, who'd masterminded a lot of the UI/UX work at Farmako whilst simultaneously pursuing her dissertation in engineering. Ankita Didi is the artist in the team. She possesses a natural inner compass of figuring out what looks cool and constitutes great design, kind of like her own crazy Spidey-Sense. Apart from that, she's this badass elder sister who'll keep advising on following rules and taking care of health, but will joyfully accompany expeditions on doing something crazy, out of bounds (stealing a miniature sauce pan as a memento from her favorite restaurant or trespassing on the most well-kept, off-limits gardens of IIT Roorkee). All things said, she carries a pure heart, sheer willingness to learn, explore and adapt quickly and a dream to build something of her own. It's just crazy and fitting how this team came together. Birds of a feather.Each of us is carving a stone, erecting a column, or cutting a piece of stained glass in the construction of something much bigger than ourselves.~ Adrienne ClarksonAn unsettling worm was eating me on the inside. I'll admit thinking really selfishly about personal growth at that time. So, as much as I enjoyed it, I wanted all of this amazing programming work, minus the other interactions. Seems crazy now, but at that point all I thought was - "What do I get from this ?". Hence, at several points, I thought of quitting and even came real close this one time when under the pressure of backlogged academics and other commitments, I was about the book a flight that very evening and leave all of this behind. Harsh ? I know.Aman Bhaiya made it very clear that working remotely wouldn't work out hence I had decided to call it quits. Rationally that would have been a proper decision. But I decided to trust my instinct on this one. Sometimes, you just can't leave people, that's what made me stick and give it 1 final shot. One of my best life choices ever. ¬†A lot of qualities I carry as a person today, stem from that one moment of choosing to stick with the idea and the people I'd fallen in love with.Have the courage to follow your heart and intuition. Somehow, they already know what you truly want to become. ~Steve Jobs (Apple Inc)Also, funny story, the same night, a burglar broke into my parent's house from the balcony where I usually code during the same time. Boy would alternate-universe-me have gotten into deep trouble that night had I quit and returned home Building a companyWrite code. Talk to users.~Erik Migicovsky (Pebble, Y Combinator)Cool office ? Check. Insanely passionate team ? Check. Brilliant product vision ? Check. Time to make something people want. And it did go brilliantly. The website was launched, the web app was in final UI stages, the mobile app was gaining users. We were exploring new UI development lifecycle technologies and considering a database migration to a more graph oriented approach with Neo4jWebsite launch dayNikhil Bhaiya was working on the automation of SMS sending and Whatsapp business status. Kaishu Bhaiya was iterating on the mobile app, installing Indian fonts on a receipt machine and some miscellaneous backend work. Aman Bhaiya was coordinating with the setup of our medical store, co-designing UI and company standees with Ankita Didi; managing product, personnel, onboardings and planning the next steps for scaling up the company during summer. I was working on the website's SEO and social-media-bot-tagging, along with design reiterations using valuable feedback from my co-intern Hardik. On free days, I'd accompany Aman Bhaiya on product pitches to nearby laboratories and clinics. Funnily, I was the only person in the team with a driving license and experience for quite some time.The characteristics of great innovators and great companies is they see a space that others do not.They don't just listen to what people tell them; they actually invent something new, something that you didn't know you needed, but the moment you see it, you say - "I must have it !"~Eric Schmidt (ex-Google CEO)Farmhouse bustled with activity. You could hear Aman Bhaiya on his calls from the conference room, roll your chair over and you'd find Nikhil Bhaiya cursing at some existing software service or API and how he'd one day replace it with his own. I feared that someday, the hacker actually might. Kaishu Bhaiya was quiet most of the time. He'd write some code, walk around to debug some issue, think of product features and then speak little to us about our progress.Food at a startup is the one thing that is always a great experience, across all startups. After a stressful day, we'd dig into great takeout food, share stories and mock each other the entire time. Kaishu Bhaiya is super into fitness so we'd joke about how he ate dinner by the calorie, only to later gorge on the stocked food as "midnight snacks". Nikhil Bhaiya is super into non-veg so he'd be happy with his own special meal. Aman Bhaiya and I were just too hungry to care. ¬†A new homeWhat I enjoyed most, were the late night walks we went on, in the society, freely talking about problems, dreams, opinions and feelings. Even the aggressive dogs, swarm of mosquitos and impending time pressure would fade away for some time as 4 engineers from different walks of life rediscovered a different kind of family in each other.Which was a good thing, since what followed would shake us to our core ¬† Troubled watersGood things fall apart. Good people don't though.It all started falling out of hand beginning with when Aman Bhaiya had to urgently leave, since his family was not doing great, health-wise. Kaishu Bhaiya left a couple days after. Same reason. Ankita Didi was quarantined back in IIT Roorkee. Nikhil Bhaiya and I spent around 5 days before he too had to leave due to some medical emergency in the family. Just all of a sudden, the pandemic had knocked out our breath. What once was a home, buzzing with constant ideas, products and energy felt like an empty vessel of nothingness.Programming suddenly felt dry. Most things in life did. I couldn't tell the founders this since they were struggling with their own problems and worrying about me would burden them further. So I'd call up Ankita Didi. Her crazy energy would make me forget about most problems and worries about the founders' families. We'd come up with stupid challenges to co-learn new software platforms for UI designing and then laugh by trashing each other's work.Nikhil and Aman Bhaiya sometimes called and we'd laugh together as they said - "You thought this was going a programming internship......"Things came around for a short period when Madhvendra Singh, (aka Madhu Bhaiya) joined. The founders' families were recovering at great rates and living with Madhu Bhaiya is also real fun.Madhu Bhaiya is the typical I-know-people-and-get-shit-done kind of guy. He rocks a classic gold chain and hearty laugh. He's the go to person to discuss when you screw up something really bad. Also he's a a good people-reader and daring risk-taker. We shared a great couple of days of guitar jamming, barbecuing and working on the recruitment of new hires for the summer.Aman Bhaiya showed up soon after, with his family, having rented 2 more villas in the same society. ¬†He'd lost a lot of weight but still managed to pull of his classic energetic, confident, ever-black-Van-Heusen T-shirt-wearing personality. For a brief moment things seemed to stabilize after this long overhaul. And then everything spiraled out of control.Ruin and RedemptionSometimes life hits you in the head with a brick. Don't lose faith.~Steve JobsThe same night, one after another, 2 medical emergencies struck our team. This time, both cases, it was critical. I scrambled around at both ends to help neutralize the emergencies. It involved a lot of inter-city driving at odd hours, rushing to places to fetch medicines and reports, continuously being on call to arrange for some sort of solution or another, barely catching up on sleep and being constantly worried all the time. Madhu Bhaiya was facing the same situation. Nikhil and Kaishu Bhaiya were COVID positive and barely supporting themselves and their families. Aman Bhaiya and his sister, pushed their pain index to another level, hardly eating or sleeping, working round the clock.The worst part about this crisis is we lost to it. I remember breaking down uncontrollably at a 6AM quiet morning at farm house. Helplessness, guilt and regret engulfed me. I doubt it'll leave. Sometimes even the thought of it sucks the breath out of me.The following day, my grandfather and the day after, my aunt, both were hospitalized in Mumbai. Every family I've cared about was crumbling.I boarded the flight to Mumbai, wearing a stuffy PPE kit and a really fogged mind about when things would get better.As the city of Delhi faded away, all I could think of, and can think of, to date, is a promise I made, a promise on my soul, to decode healthcare no matter what.Never give up. Today is hard, tomorrow will be worse, but the day after tomorrow will be sunshine.~Jack Ma (Alibaba Group)
Why Work at a Startup ? - Part II,The chapter of of friendship, rediscovery and relentless efforts,If I'm going to be young and stupid, it better be with an idea to solve something that helps millions of people. Touchdown4:30AM. Touchdown at Panaji Airport, Goa. The air was cold, felt colder since I had taken a chance for the first time, entirely on my own. There were hardly any people at the airport despite the wonderful weather, thanks to the pandemic.The office was a 50 minute drive but the rates were scathingly exorbitant. I remember texting Nikhil Bhaiya at 4:30 in the morning (my first Slack message) inquiring whether the cab rates were actually this high or if I was being duped. I reach by 6 and that's when I learnt my first lesson, a lesson in patience. For over an hour I kept ringing the office's doorbell respectfully, and no one answered. I started to worry if this was some messed up prank or worse, as my mother feared, a setup with a nefarious ulterior motive. Fun fact : My mother actually thought that this company was a front to terrorist activities and I would be kidnapped and transported via boat to some cell which was why I had been called alone to an Goa.Locks clicked and I came face to face with a barely awake, well-built, baby faced guy, wearing a plain grey t-shirt, boxers and a "what the heck's going on ?" look. Kaishu Sahu, CTO, Farmako. I remember thinking "He looks barely a year older than me !". After an awkward exchange of introductions, we stepped into the most messed up apartment ever I had ever seen in my life.To call it unclean would be undermining the degree to which this place had been trashed. The dining table was cluttered with uncountable meals of food and unwashed plates, giving the apartment what it's unique ¬†smell . The office space and tables were littered with soda cans, Dorito bits, used tissue papers, wires and spilled drinks. There was even a hole in the wall for some reason and a dark circular spot on the ceiling (some in-house cricket tragedy I guessed). ¬†Every other breathing space was sucked up by electronics, wires, routers, multiple testing mobile devices, a broken printer, a destroyed microwave, large whiteboards scribbled with product ideas and printouts (credits : Nikhil Bhaiya) of pop-singers and trending memes ¬†I think the maid had clearly given up by this point. A door handle had been broken, kitchen cabinet doors had unhinged, fridge was packed with food that was no longer edible (1 open beer dated 6 months), and there were scattered piles of groceries, spices and massive amounts of some body building protein mix (credits : Kaishu Bhaiya)"It's just 1 month", I breathed as I picked the guitar, something that always calmed my nerves. It had stickers of the company logo and Y Combinator's logo on it. I shared a quality conversation and a plate of rice-cakes with Kaishu Bhaiya. We talked about college life, programming journey, interests, ideas and the history of the company. I'd totally fund the guy too ! The amount of technical knowledge he possessed was a lot more than most other people I'd met in life. Funnily, that's where most his expertise ended too. As I saw him hesitantly wash the dishes, I could guess that this was definitely a rare occasion.I crashed on the couch for about 2 hours. I woke up to the smell of chicken that the cook was preparing. Being a vegetarian, I recoiled and I looked around to find a friendly, spectacled, bearded face grinning broadly at me. Aman Bhandula, CEO, Farmako Healthcare. We exchanged introductions, jammed over the guitar and then entered Kaishu Bhaiya's room to wake up the lean dead body sleeping on a mattress beside the bed; AirPods plugged in, bedsheet from chin to toe, dead-asleep after what I guessed must've been a stretched out code-tear. Nikhil Kumar, Technical Lead, Farmako Healthcare. He greeted me with a slight nod and a ‚úåÔ∏è and went back to sleep again.I onboarded the website and web application project that evening. Aman Bhaiya was leaving the next day for a last rafting adventure with his college friends, so that evening, we shared a bowl of Maggi and an amazing conversation about entrepreneurship, networking, morals and how Farmako went from being a barely surviving project out of Aman Bhaiya's hostel room, where they had to save up to purchase 1 printer, to the rocketship startup it had become 2.5 years later. "They're great people, but I'm just an intern." - I said to myself as I lay down to sleep, planning on quitting after a month of programming, Goa and learning the inner workings of a startup. ¬†I was going to be proven wrong.......big time.Living with the HackersIf you're changing the world, you're working on important things. You're excited to get up in the morning.~Larry Page (Co-Founder of Google, Alphabet)A classic morning at office started with me waking up to enjoy a walk nearby and then a quick cereal breakfast. Some college and programming work would follow. The cleaning helper, Nikita Didi, showed up at 9 and we shared light conversation. I'd cleaned most of the apartment a day after onboarding so even she felt happy to go the extra mile in wrapping up the rest of the trash. She was the unofficial tour guide who taught me a lot ranging from how to identify ripe fruits to finding a watch repair shop around.The Hackers would wake up by afternoon, mostly after the cook showed up. They were in a separate league of their own. Based off of Paul Graham's interpretation of people, given a chance to choose between being a hacker or a painter, I'd definitely go for the former. It feels like a complex personality on the outside, but on the inside, it's just an innocent, inquisitive, relentless child, with a yearning to build something great.Hackers are actually the most interesting people. They're a beautiful combination of contrary idealogies. They'll know how to tackle some of the most complex problems but have difficulties overcoming basic obstacles in life.Kaishu and Nikhil Bhaiya are pure hackers by soul. They learn and implement new technologies at breakneck speed but take hours to decide what to order for dinner. They trust revealing their identity to no one online and have the most complex authentication setup on their machines, but throw the apartment keys on the floor. They claimed to be busy with new ideas all the time but they also managed to find time to install Linux OS on my PC (three times over, at this point). They have their socially awkward moments with the people around, but not a day went by when they failed to ask whether I'd eaten enough. They keep arguing on countless occasions, blaming each other for bugs, but somehow reconcile and deep down, even though they never acknowledge it face-to-face, carry great respect each other.They can't express it, but they do care.(Left to Right), Nikhil Kumar, Kaishu SahuAnd I just don't mean the word "hacker" in the computer programming fashion. These fighters can hack any problem. On a classic weekend, you'd see Nikhil Bhaiya rewiring the WiFi connection in the office by hand or Kaishu Bhaiya dismantling electric sockets to fit iOT devices to automate lights and fans via voice recognition. They're your classic open-it-up-and-see-how-it-works engineers.And then you'd find these same people microwave frozen Coke bottles or descending on an ascending elevator or speeding past cops when asked to pull over. (we came close to an arrest twice)The more I got to know about them, the more I learnt, not just about the tech stack we were using or the brilliant system architecture they'd designed, but about how to bravely explore a new idea, get version 1 out quickly and keep punching at the problem, with undying hope for a breakthrough. Pure entrepreneurs. Relentlessly resourceful (as Sam Altman would say).Launch, Inspect, Iterate, Repeat. Pretty inspiring, if you ask me."...the round pegs in the square holes, the ones who see things differently - they're not fond of rules - you can quote them, disagree with them, glorify or vilify them, but the only thing you can't do is ignore them because they change things....."~Steve Jobs (Apple Inc.)Office space, Farmako Healthcare, Calangute, Goa, IndiaRediscoveryThe days are long but the decades are short~Sam Altman (Open AI, Y Combinator)I was on a work-ethic path that would have eventually killed me at some point. I didn't even realise this since I always felt pretty cool about learning some new skill instead of spending some time with friends and family or even exploring and experiencing new avenues. Even if I'd spare 5 minutes, I'd run off to pursue some new programming skill. This wasn't sustainable. I was clearly missing out on a lot and not even understanding how important it all was. I was running away from a truth but all this would have eventually led to nowhere.Writing code is probably one of the easiest things you'll be dealing with in life.~Nikhil Kumar (Farmako Healthcare) Nikhil Bhaiya has a way to extract what he wants to know from you, no matter how hard you try to hide it. Friggin hacker. It seemed intrusive at first but later it did feel nice to share problems with someone as uptil then, I've a habit of playing it all pretty close to the chest. He gave me an hour long advice on why it was actually really easy to always be busy with work in life, the hard part was gratefully acknowledging all the people who stood beside you and giving them time too. He'd snatch me off to trips to nearby forts, cafes, ice-cream parlours, beaches and slowly I began to comprehend, the depth of why however stupid it seemed, it was really important to go off-plan and try something new out. If you ask Kaishu Bhaiya his biggest regret in college life, he'd tell you he wished he'd learnt the guitar. Nikhil Bhaiya wishes he'd spent time on learning some non-technical skill too.It was a great couple of days, beginning to understand and learn from these unlikely mentors on what life actually seems like, out of college, in the real world.I learnt how to drive a scooter, the 1st time at 2am outside Anjuna Fort, Goa, on Nikhil Bhaiya's relentless persuasion. He kept joking about kidnapping me. Once when the electricity went out, the 3 of us spent an afternoon pencil-carving and the evening, programming out of a KFC fast-food joint. We'd go to Artjuna Cafe, 20 kms away, to code, mainly because Nikhil Bhaiya really liked the cheesecake there. Or say drop by Gai Kajus, a cashew nut factory that was Kaishu Bhaiya's #1 spot to kick back with a class of dry-fruit milk.I spent a lot of time with Nikhil Bhaiya, since he was who I was mainly interning under. The lean, spectacled, piercing eyed hacker has a kind of Megamind like vibe. He knows all the hacks and glitches of the game, but chose to stay more on the good side. As Aman Bhaiya says, he's an ocean of knowledge, and just from talking to him, you learn a lot, although it does get overwhelming after a point as he just won't stop talking. He's different from everyone I know. He'd kick back with a Coke at 6AM, sleep till 3PM and blazingly ship code afterward till the next 6AM shot of caffeine. Funny thing is, from his attitude it looks like he's laid-back, but has the most control over his mind and time, always slamming deadlines. ¬† We used to get lost couple of times near beaches, scrambling to find our scooter. He'll still always blame me for the first time when we went on one of these walks and by mistake my hand knocked out his iPhone from his, resulting in a scratch. I still carry recordings of him scolding his heart out at a food delivery executive, when our order delivery got cancelled midway. Boy was it a satisfying 2AM dinner at some random bar after about 3 hours of argument with the food delivery management team.Kaishu Bhaiya seems like he's chasing the goal of being the best hacker. He has infinite hunger to learn something new and build something cool with it. And boy does he always manage to surprise.He mostly keeps to himself and is always worried about something all the time. It may ranges from anything like a new bug to not getting a credit card approval from some bank. You'll never see him anywhere without a backpack containing his laptop, strapped to his back. The craziest adventure we shared was an unfruitful 3 hour, 50 km drive across Goa to purchase guitar strings, everytime being redirected to a closed shop or even in one case, a private bar (thanks Google Maps !) for some reason. Like I said, he couldn't say or show it, but he did care deeply even about trivial things such as this. Removing a trash version now and iterating it to perfection is a lot better than waiting to build the perfect code.~Kaishu Sahu (Farmako Healthcare)(Left to Right) Kaishu Sahu, Nikhil Kumar, meI'll admit it wasn't always easy and amazing all the time. Like I said, being vegetarian, I took time to get used to the smell of chicken in the apartment. I remember feeling left out when they went to the gym. Their disjoint, nocturnal work schedule did bother me a lot, since we rarely had meals together. Kaishu Bhaiya always claimed eating less food so it did bug me when he stole food we'd bring( later, Nikhil Bhaiya and I ¬†secretly just bought extras :) ). Working with Nikhil Bhaiya also requires some next level dedication since he'll continuously keep barraging questions and thoughts whilst you try to concentrate and code. Also, it took infinite amount of patience to keep the office clean and organized and regulate food wastage.But they were worth it. All of it. ¬†I just wished I'd told all of this to them much sooner as it got sorted really quickly when we discussed it later. I guess that's how most friendships work, you must carry no secrets or emotional debt. You fight but you never forget the underlying connection you always carry.I don't know how exactly most of these detours or experiences shaped me or what I learnt from them, but it just made me a lot stronger to face any challenge that came my way. It gave me enough hope that no matter how formidable a problem seemed, there always had to be a hack around it, and you could always find it. ¬†I even learnt a thing or 2 about responsibility, selflessness and a scientific mindset from these unconventional teachers.Levelling up
Study the unusually successful people you know, and you will find them imbued with enthusiasm for their work which is contagious. Not only are they themselves excited about what they are doing, but they also get you excited.~Sam Altman (Open AI, Y Combinator)Aman Bhaiya, Ankita Didi and Arun Bhaiya were working remotely but somehow they'd managed to collaborate and pull off a really minimalist but beautiful design for the website and the blog. We coded it out.A month flew by. The website was ready. Aman Bhaiya had finalized an office-cum-living-space in Noida, India and we were about to scale up with new recruits, products and customers. He called us to start shifting, but I chose to go home for a week to spend some time with family whilst the new office was being setup. On 1st March, I pushed my last bit of code, hailed a cab and barely made to the flight in time, almost forgetting my laptop at the scanning area (this seems funny now, but could have become a really bad experience had I not felt the bag was missing something as I was walking down the final passage to the flight doors.)As the flight climbed higher, a sense of fulfilment gripped me. This wasn't your standard "beachside retreat" story but it was soothing and transformative in its own unique way. I smiled. Only a few hours back, 3 crazy engineers were scribbling product ideas on an office whiteboard 15000 ft below. The cabin lights dimmed and as my eyes started drooping, I remember feeling really optimistic about the next chapter. Little did I know, what was oncoming would introduce me to a new family and then break us all to unimagined levels.Life is like a book. If you don't turn a page, you will never know what the next chapter holds.
Why Work at a Startup ? - Part I,The first of many chapters in my journey of working at the startup - Farmako Healthcare,It's like jumping of a cliff and assembling a plane on the way downOn February 10th, 2021, through a LinkedIn connect, I got the opportunity to work as a React Developer at Farmako Healthcare, a healthtech startup, then based out of Calangute, Goa, India.I am not going to lie, the thing that initially drew me to the company was the brand value it had achieved by securing a backing from Y Combinator, an amazing startup incubator and accelerator led by some of the most inspiring visionaries I've come to idolize.However, as I write this article, I'm 3 months into this, and not a day goes by when I don't smile about jumping at this window - against the wishes of my family, an increasingly challenging online semester, amidst a global pandemic and a couple other imbalanced things in life.In my 3 months as an intern (as of May 2021), I have not written a lot of code, maybe lesser than what I would have if I stayed put with my routine, but I have grown as a person and earned experiences and people, that pretty much dwarf what most certifications, accomplishments and compensations, any other work would have yielded.This journey has been a tale of friendship, sacrifices, losses, dreams, chaos but for the most part, it resonates a glimmer of undying hope, to help society, to make something people want.About FarmakoDoing what we already know how to do takes the world from 1 to n, adding more of ¬†something familiar. But every time we create something new, we go from 0 to 1.The act of creation is singular, as is the moment of creation and the result is something fresh and strange.~Peter Thiel, (Paypal, Facebook, Palantir)Farmako is the brilliant brainchild of 3 insanely crazy, visionary engineers out of IIT Roorkee, India :Aman Bhandula - CEO, Graphic Designer, Machine Learning Engineer, ever-dead-Apple-watch wearer.Kaishu Sahu - CTO, Software Developer, guitarist and fitness enthusiast.Nikhil Kumar - Technical Lead, omniscient hacker of code and people, caffeine connoisseur.Farmako is a healthtech startup that takes the 4 main components of any healthcare system i.e.PatientDoctorLaboratoryMedical Storeand digitally integrates them all in a privacy regulated manner in a cloud of software services, such that the speed and effectiveness of communication between these components would make the healthcare pipeline - right from symptom to diagnosis to testing prescription to recovery - cheaper, faster and more accessible for anyone."Everything these days, right from banking to booking a cab these days is digitized in a unified manner. There seems no logical reason why healthcare hasn't undergone this transformation yet"~ Aman Bhandula, (Farmako Healthcare)Also, once enough records are safely stored in the database, Farmako can actually perform machine learning to predict future disease outbreaks, effective medications and potential hotspots where most healthcare resources and expertise needs to be diverted to. Also, the thing that sets apart Farmako from most startups I've seen is that most discussions were not on monetizing and profiting from the idea, but on helping make healthcare technology, a real support system for everyone. ¬†Also, funny thing is these post titles and job divisions of CEO, CTO etc. are nothing but names to the outside world as with the kind of amicable culture inside Farmako, literally everyone is doing everything. The most interesting kind of workplace I could possibly ask for. First ImpressionsBe a cockroach. They're much harder to kill~Paul Graham (Y Combinator, VistaWeb)My first introductions to the company started with a LinkedIn post announcing for hiring a React Developer Intern. I was lucky to be tagged onto it by a friend. As researched more about the people, I got amazed more and more about how 3 extraordinarily talented people, from different walks of life, had gotten together on an idea and stood by it, since 2018 (3 years back), against all odds.I even went through one of their blogs on why something like this had to be done.The more I researched, the more I was convinced that the problem, the idea, the people and the approach all seemed to somehow fit perfectly into actually solving one of the biggest problems of the 21st century. I was drawn to it quite strongly Hiring...If you give a good idea to a mediocre team, they will screw it up. If you give a mediocre idea to a brilliant team, they will either fix it or throw it away and come up with something better.~Edmund Catmull, (Pixar)Applied. Interview scheduled. I was battered with questions about software development and in fact, the project I was most proud of upto that point in my journey as a programmer was hacked within a minute by Nikhil Bhaiya. ("Bhaiya" means "brother", in Hindi language). In those 30 minutes I actually experienced the diversity in thoughts and approach of Aman Bhaiya and Nikhil Bhaiya. Someday later, Aman Bhaiya, drawing from Paul Graham's observations, would explain to me, how 2 distinct kinds of people - hackers and painters, come together to create marvellous pieces togetherCase in point : 1st April, 1976, California, USA. A Reed college dropout, brilliant artist meets a University of Colorado expellee, elite hacker and the 2 set up a small venture selling home computers from the artist's parents' garage. As of 2021, this venture has grown to a $2 trillion company, we know by the name Apple Inc. - brainchild of hacker Steve Wozniak and artist Steve Jobs.A funny anecdote that comes to mind is that Aman Bhaiya started conversation with how life was going and at 1 point Nikhil Bhaiya just barged into the talk with - "Is Node JS single threaded or multi-threaded ?". After a short answer, Aman Bhaiya realised that they hadn't introduced themselves to me yet so after about 5 minutes of awkward questioning, I hear - "Hi ! I'm Aman, and these are my co-founders Nikhil and Kaishu (who was wired into code, and just greeted me with a classic ‚úå)."Aman Bhandula and Nikhil KumarPost-interview, one thing was for sure though. Something was really different about these entrepreneurs. I knew I had to get to know them better hence I kept spamming Aman Bhaiya with text messages asking for the technical assignment that would be the last evaluation in my application for an internship. Funny thing is, first 2 hours of starting the assignment, I am dead spirit-broken because of some small bug because of which the project wouldn't even initialize. At this point I start having 2nd thoughts. I mean - I had a full-blown online semester going on, I had institute club commitments, I wished to continue playing around in shallow waters of being a student developer and enjoy the luxury of being a child in a family and there was 1 other integral reason as well which was yearning me to give up on this chance and move on."You always regret the things that you don't do"~Jeff Bezos (Amazon, Blue Origin) Somehow, deep down, I knew I had to do this. 20 years down the line, I didn't want to face myself regretting the question - "When you had the chance to work with a bunch of great people on a transformational idea, where did you go ?"That was it. 22 hours. Wired in. At the end, I was able to create a simple dashboard that could be used by a doctor for viewing the medical records of the upcoming patient appointments. For the first time ever since I started programming, I felt a wave of confidence wash over me, that somehow, amidst the nights of staring frustratingly at errors for hours altogether, I'd managed to gather enough grit to disect a problem into code blocks and weave it all together in a functioning software solution that people could use.A shot in the darkWhen you find the right people, you don't have a choice, but to work with them. When you meet somebody who's that alter ego, don't let them go.Find a way, however crazy and however weird to make them part of your world.~Eric Schmidt (Google CEO, 2004-2011)A day after the submission, I get a call from Aman Bhaiya, and he told me that they really liked my code and quite casually, his next question was - "How soon can you move to Goa ?" Now see, I come from a family of order and security. The path I was expected to tread was one of a Masters from a college of repute, a high-paying engineering job at a tech giant and a settled life. So, a startup was a big red flag on this road. And upto this point, these were the exact footsteps I'd been taking. But this time, I somehow gathered enough confidence to go against my family and explore this uncharted territory, purely because my heart knew this was the place I had to be in. Sometimes it's okay to be stupid and instinctive. Sometimes you have to trust your gut and take a leap, however irrational it may seem. Flight ticket booked for the day after.That night, at around 11pm, I was introduced to the team in a more informal fashion. Aman Bhaiya was getting anxious about not having the monitor properly set up for a video call. Nikhil Bhaiya was getting teased about flexing his AirPods on call. Kaishu Bhaiya walked up from behind, sipping a chocolate smoothie. Hemil, the Flutter Developer intern was being laughed at, for turning his camera on for the 1st time on call. Arun Bhaiya, the graphic designer, smiled, not understanding most of what we were saying as we spoke in Hindi, a language that was not-native to the South Indian origins he hails from. ¬†Ankita Didi was laughing, complaining about being detained at Khosla House (I think it's a guest house) back in IIT Roorkee, where she was balancing her dissertation side by side with a graphic designing internship at Farmako.I remember laughing a lot without realizing at how this strangely messed up medley of people would become family to me in the upcoming weeks, and how we would embark upon a great journey on revitalizing the healthcare situation in India.As I lay down that night, I could barely believe how this had all happened so fast. Tomorrow, I'd meet the friend who was pivotal to this entire transformative journey, as I started a new chapter, and say my thanks, goodbyes and promise to keep in touch. Day after, I'd fly. Sometimes it's important to wake up and stop dreaming~Larry Page (Google, Alphabet)
Steering at 190kmph,Designing a lightweight, strong, customizable, comfortable steering wheel for a racecar,Driving a racecar is like dancing with a chainsaw~Cale YarboroughThe steering wheel is one of the most integral components in an automobile considering the handling of the driver is directly associated with it, more so, while desigining a race carRace car steering wheels are the most subtle yet most complex components when seen from a product designing point of view. They need to provide full accessibility and control over most of the car's units to the driver whilst ensuring that she/he doesn't strain her/his time or energy just trying to reach the right buttonAlso, there is the question of :At what angle will it be mounted ?How light but cost effective can it be made ?How can you customize the wheel for each driver ?How minimalistic yet effective can you make the wheel ?How strong can you make it to prevent under and oversteer at high speeds ?
Mulling over all these questions in mind, I was designing the 1st steering wheel for the final combustion car my institute motorsports team, Raftar Formula Racing (IIT Madras) would make, before migrating to an electric vehicle.Our car's top speed at that time was 190kmph and the engine was powered by that of a KTM 390 superbike. So you can only imagine the vibrational load that the steering wheel would encounter under high speeds ü§ØThe design
It took the team over 5 drawing iterations, 4 simulation loopbacks, 6 paddle shifter ideas and several other ways to make the wheel light, accessible and customizableWhilst I was handling creation of the parts and the assembly, this project could not have been possible without the help and support from my team members Kulma, Ithan, Karthik, Shriram, Sahoo, Keshav, Abhiram and Sailesh
This project was carried out in 3 main phases :Designing the steering wheel plateOptimizing steering wheel plate design via simulations and stress analysisPaddle Shifter designI started off by studying the previous steering wheel and initially just had the thought of optimizing it a bit but what resulted was something unexpected, arduous but astoundingly inspiring !PHASE 1 : Designing the wheel plate
As a reference, we studied RFR19 Steering Wheel and got the wheel plate outer dimensions that were preferred by the driversAlso, Formula Bharat 2021 latest rules were referred for compliance with Formula Student competition rulesSaurav and I started off with a pretty basic designs and both branched out with different design concepts.I designed v1.1 and Sahoo designed v2. Both designs were reviewed in a meeting with a meet with the drivers.V1.1 - A rough overview inspired from RFR19 wheel
The drivers made thermocol models of both designs for understanding the ergonomics well. They approved and suggested optimizations on v1.1Thermocol model test of v1.1
V1.3 - Holes and button positions changed
As a result, Sahoo and I started working on optimizing the same design ¬†(design v1.1 ) by studying ergonomics of several other teams‚Äô , companies‚Äô and other wheel manufacturers‚Äô designs.Google Docs - create and edit documents online, for free.Create a new document and edit with others at the same time -- from your computer, phone or tablet. Get stuff done with or without an internet connection. Use Docs to edit Word files. Free from Google.create and edit documents online, for free.After design v1.6, a decision was made to bend the wheel towards the driver for better force distribution. This was the most crucial and most unexplored design descision we took that was the single best feature to optimize the weight to strength ratio of the steering wheelv1.7 involved a uniform bend across the surface of the steering wheel. It was rejected as a design as it made attaching the wheel to the quick release quite a problemv1.8 involved a flat midsection and bending the wheel at the edges
From this point on, I proceeded further with designing paddle shifters and the associated actuation while Saurav proceeded with performing simulations on the steering wheel plate to study force distribution, safety factor and topology optimization.PHASE 2 : Optimizing the wheel plate
This majorly did 2 things

Made the steering wheel light
Made the steering wheel strong

The mastermind that is Saurav Sahoo....
The plate was decided to have 5 layers of Carbon Fiber on either side of an Aluminium Honeycomb core.The boundary conditions are a fixed support where it's attached to the quick release, and the forces applied were F1:155 N(y), -269 N(z) and F2: 217 N(y), -376 N(z) and a moment of 88 N-m.
Viability of bending the plate was checked ergonomically, but it still had to be checked mechanically.So a comparison test for Safety Factor and Equivalent Stress distribution was done between a flat plate and a curved plateIt was found that the Inverse Reserve Factor (more this value, less the safety factor ) decreased and so did the Maximum Equivalent Stress
So an ACP simulation was done on the partial bent plate decided on v1.8 and the results were compiled and compared to the results of the RFR 19 Steering Wheel.The maximum deformation obtained was 2.129 mm down from 2.94 mm in RFR 19 Steering WheelThe IRF obtained was 0.945, which deems it safe to usePHASE 3 : Paddle Shifters design
This was the most gruelling and difficult design part. We had to use the least number of components whilst making it all work like clockwork for the driver using it
We chose this as a component because of the driver comfort it would give as opposed to stretching out thumbs to hit the buttons to shift up and down
We went through so many models and iterations that it was pretty surprising when we decided to scrap all that we'd seen and put up our own simple, unique design as the solution to it allInitially, I, was assigned the task of improving upon Shriram‚Äôs design. I managed to change the shape of paddle shifter to a more ergonomically optimized one and made changes to present actuation to make this new paddle shifter design compliant with the newly designed wheel plateInitial designHowever, this was rejected due to 3 main reasons :It is difficult to manufacture.It didn't seem right to actuate 2 shifters with the same spring.This component was monolithic so it would be difficult to dis-assemble for improvement or repair purposes.I was now assigned the task of coming up with my own idea for the entire design and actuation of paddle shifter.After some research, I came across a YouTube video that showed the entire process of making paddle shifter using some small parts (CADs were available as STL files) and regular linear springs‚ÄôMy version of paddle shifter and wheel plate inspired from YouTube
I tried to copy this design to a great extent but finally it was rejected due to the following reasons :Too many parts were involved in the actuation and there was no need to involve so many moving parts and attach them when we could come up with lesser, static partsThe parts would have been too complicated to manufactureThus, after much brainstorming with Shriram and Karthik, I now started working on a simpler design that would comply with the current assembly (new wheel plate + quick release), involve actuation buttons and contain a lesser number of parts that were easy to manufacture.This phase involved some trial and error to make the parts as small, as simple and as less in number as possibleEg: The support boxes dimensions were reduced from 53*39 mm to 46*31 mm
We were able to finally arrive at a working model of the steering wheel assembly involving these parts :


PART
SPECS
FUNCTION




Paddle Shifter
A 66 mm high, 92 mm long paddle shifter with a oblique cylindrical protrusion to actuate the switch 2 paddle shifters on either side of wheel plate center for upshift (Right) and downshift (Left).
Follow an arc-like motion constrained by the torsional spring. When pulled towards oneself, the driver forces the paddle shifter (against the spring force) to actuate the switch and thus, shift gears


Mounting Plate
A 46x31 mm plate with protruding 3x27 mm screws. Houses actuation switches on its surface
Mounting the support box onto the wheel plate. The paddle shifter hits these actuation switches to change gears


Support Box
A 46x31 mm cuboid with 2 3mm holes for rotation axis of paddle shifters.It also has 4 other 3 mm holes for mounting
Provide a space for the motion of the paddle shifter along with axes of rotation.  Mount the actuation on the mounting plate


Torsional Spring
A dual coiled 18 mm long spring. Each coil has 4mm diameter and 1mm cross-section diameter
Constrains motion of paddle shifter through it‚Äôs resistive spring force


Actuating Switch
A 6x6x8mm switch with a 4mm wide button
Gets actuated by protrusion on paddle shifter and converts actuation into an electrical signal that will stimulate a gear shift


Back Lid
A 46x31mm rectangular plate
Protects the components inside the support box


Wheel Plate
A 220x140 mm wheel plate slightly curved at the edges.Houses 4 buttons. Houses 3 holes for mounting quick release
Turning the tires.Actuation of upshift/downshift, brake, accelerator



Compact fitting and motion of various parts involved without any overlaps. Enough space has been provided to mount the PCB and the wiring accompanying itSmaller, lesser and functionally optimized parts protected wellSlight optimizations (eg: Bending of wheel plate, positioning of buttons, shape of paddle shifters)An enhanced view of the paddle shifter assembly
Lastly, as an afterthought we disassembled the paddle shifter itself into 2 parts for better customizability for each driver's hand : 


Part
Function




Functional
Actuates the button in a controlled fashion due to spring constraint


Interactive
The customizable, personalized part interacting with the driver's hand and mounted onto the functional part



Afterthought
That's how we came up with the design of the coolest,cheapest and most comfortable and customizable steering wheel design for the drivers of the RFR21 car that we virtually built and simulated, owing to the COVID19 pandemic in 2021 that had us all under lockdown.
It was one heck of a journey designing this beaut, right from sketching out plate diagrams, making thermocol models, brainstorming designs and even arguing to no end about the minute tweaks each design iteration would bring.üí™

The rough mockup design


The final design


This final image is something that always puts a smile on each of the associated project member's face and even got applauded by the judges in the national level Formula Bharat as a simplistic, unique approach, unexplored beforeüî•
Design of wheels ... a brief historyNext year, we go on to build our 1st electric vehicle, RFR22 and boy is that one gonna be an even more thrilling journey with all the chassis and powertrain being designed from scratch !üöÄ
I hope you enjoyed reading this blog and I'll see you next time with a high performance electric racecar steering wheel  ......
Observer Pattern,Understanding the roots of state management with the Observer Pattern,Used to notuify and update objects (observers) when the value of a specific object (observable) changesBasically, the Observer Pattern works beautifully when you have 2 things :One / more objects (observable) whose state you wish to monitorOne / more objects (observers) whose state you wish to change when the observable undergoes a state change The Observer Pattern is widely implemented in all places where state management is imperative like :Redux, for rolling UI changes on store updates ¬†Microsoft Excel, where, when you change the value of a cell, the column and say maybe a graph based off the column get updatedResourcesThe source code for all prominent design patterns has been properly implemented and documented in this GitHub repostitory by me.‚Äåcrew-guy/Design-PatternsA collection of the design patterns, implemented in java, as originally created by the GoF - crew-guy/Design-PatternsGitHubcrew-guyProject DescriptionHere, in this article, we will be building a simple application that mimics Microsoft Excel's simple functionality where if you change a value in a cell, the column and the graph dependent on this cell's value get immediately re-rendered/ updated !!üî•‚ÄåAlso, here, we will be working with something known as the Push style in the Observer Pattern so here, the Observers will be idle so it is the Observable that will push the value to notify them to re-render themselvesImplementing the Observer interface
It is an interface which lays out the skeleton of the methods each of the observers should haveIt helps maintain the Open-Closed Principle, something is fundamental to good Object Oriented Programming programs. It states that the "software entities must be open to extension, but closed for modification"Thus, we can scale no. of observers, easily without ever changing existing code.It lays out the prototype of the update( ) method i.e. what the observers (child classes implementing this interface) do when the observable changespublic interface Observer {
    void update(int value);
}
Child classes implementing the Observer interface. These are what bring about changes when the observable(s) change(s)They polymorph the method (viz the update() method) specified by the interface. To understand about working with polymorphism in detail be sure to check out this blog, I wrote sometime back
Template Pattern | JavaOne object many forms ! Here, we up our strategy game up by working with templates, inheritance and polymorphismAnkit Sanghvi | BlogAnkit SanghviIn our case, this happens - when a cell (observable) value changes :1. Chart (observer) => Get's re-rendered2. Column (observer) => Get's recomputedpublic class Chart implements Observer{
    @Override
    public void update(int value) {
        System.out.println("Chart was just updated with value :" + value);
    }
}Chart.javapublic class Spreadsheet implements Observer{
    @Override
    public void update(int value) {
        System.out.println("Spreadsheet was just updated with value :" + value);
    }
}Spreadsheet.javaImplementing the SubjectIt is a simple abstract parent class having the collection of observers and implementations of 3 methods on them



Method Name
Function
Example




addObserver
Adds a new observer to the list/collection
Adding a new column in Google Sheets which subscribes to an existing cell


removeObserver
Removes an observer from the collection
Removing a YouTube subscriber or a facebook follower


notifyObserver
Notifies all observers (by iteration over each one ) that value of observable has changed
Slack notification to all members in a channel listening onto the successful merge of their code to a GitHub repository



Here, all the subject does is exactly this only : public class Subject {
    public List<Observer> observers = new ArrayList<>();

    public void addObserver(Observer observer){
        observers.add(observer);
    }

    public void removeObserver(Observer observer){
        observers.remove(observer);
    }

    public void notifyObservers(int value){
        for(var observer : observers){
            observer.update(value);
        }
    }
}
Subject.javaImplementing the ConcreteSubjectsThese are observables extending the Subject class. They hold values that can be subscribed to by observersThese implement 2 methods



Method
Function




getVal()
In case the observers wish to use the updated value for something


setVal()
To set the value, thereby triggering a change among all observers



Here, we will simply have 1 observable called DataSource which contains the value the observers are watchingpublic class DataSource extends Subject {
    private Integer value;

    public Integer getValue() {
        return value;
    }

    public void setValue(Integer value) {
        this.value = value;
        notifyObservers(value);
    }
}
DataSource.javaUsing this pattern in our applicationImport all the necessary classes and modules into main.java for a final showdown of our application ! Here, I'll be simply adding 2 sheets and 1 chart as observer to our data and re-render/ ¬†all observers when I set the value in the ¬†var dataSrc = new DataSource();
var sheet1 = new Spreadsheet();
var sheet2 = new Spreadsheet();
var chart = new Chart();

dataSrc.addObserver(sheet1);
dataSrc.addObserver(sheet2);
dataSrc.addObserver(chart);

dataSrc.setValue(1);Main.javaConclusion
Congrats on unlocked the secret of how most modern implementations of state management work under the hood. You should feel pretty comfortable now developing the dynamic state functionalities in your applications. Until next time...‚ÄåReferences..
Whatever I have learnt and implemented here in code as well as the explanation is all from the teachings of the brilliant Mosh Hamedani and his course on Design Patterns which I would recommend you to take if you are really interested in exploring the beauty of Object Oriented Programming
The Ultimate Design Pattern SeriesAce your coding job interview. Learn to write maintainable and extensible code.Code with MoshMosh Hamedani
Undoing "commands",Implementing undo mechanisms with the optimization benefit most intensive, modern apps require,Here, we will talk about undoing commands to change to course of history......The Command Pattern, as described in one of the earlier blogs, has a wide variety of applications, one of which is the most famous text-editing command - Undo. Usually, to implement Undo mechanisms, the Memento Pattern is used but here's the thing - Memento stores the history objects as they are without any manipulation or processing or abstraction. So, once the size of history objects starts to become large (eg : video, images) , the space and time complexities of this pattern become hugely inefficent.Which is why, in most modern applications, Command Pattern is used to implement undo because what Command Pattern stores as history object is just some logic that helps restore the actual history object when the Undo command is executed. This logic is pretty lightweight so space and time complexities are both optimized.To know more about the fundamentals about the Memento and the Command Pattern, be sure to check out these blogs by me on the same !Command Pattern | JavaUsing the command pattern to understand how most modern frameworks implement the right balance of abstraction and controlAnkit Sanghvi | BlogAnkit SanghviMemento PatternAn excerpt on implementing undo mechanism in software applications using the Memento PatternAnkit Sanghvi | BlogAnkit SanghviProject Unified Modelling Language (UML) DiagramUML Explanation....At its core, the command pattern consists of 4 interacting bodiesBodyFunctionInvokerInteracts with the interface to call a specific function on the receiver sideCommandIt's the interface¬†establishes and controls communication¬†between the sender and the receiver. It declares an¬†execute( )¬†methodReceiversClasses that expose methods that hold the actual implementation logic of a receiver service (eg : AddService, AuthService etc.)ConcreteCommandA child class of the interface that defines the¬†execute( )¬†method and which receiver service it shall call.ResourcesThe source code for all prominent design patterns has been properly implemented and documented in this GitHub repostitory by me.‚Äå‚Äåcrew-guy/Design-PatternsA collection of the design patterns, implemented in java, as originally created by the GoF - crew-guy/Design-PatternsGitHubcrew-guyProject DescriptionHere, in this article, we will be building a simple application that implements the Undo Mechanism the way most modern applications like Google Suite, Adobe Creative Cloud and many more services use, using the Command Pattern method.You will understand how this abstraction not only makes code more scalable, but also more optimized in terms of space and time

Implementing the Command (Interface)The top level parent interface that establishes and manages communication between the sender and receiverOn some request from sender, it creates a Command object Also, it declares the execute( ) method which shall be defined on the receiver end to run the logic that answers the sender's request.public interface Command {
    void execute();
}
Command.javaImplementing the UndoableCommand classThis is just a child interface of the Command class that simply extends the functionality of the prototype any classes extending this class (i.e. classes having undo feature on them) must have.public interface UndoableCommand extends Command{
    void unexecute();
}
UndoableCommand.javaTracking the history of commandsA simple class that is just tracking the history of the current Command object, with respect to which commands were run on it, so we can undo commands easily.public class History {
    private Deque<UndoableCommand> commands = new ArrayDeque<>();

    public void push(UndoableCommand command){
        commands.add(command);
    }

    public UndoableCommand pop(){
        return commands.remove();
    }

    public Integer size(){
        return commands.size();
    }
}History.javaImplementing the Receiver - BoldCommandThese is a simple class that exposes methods holding the implementation logic to change history of the current command object based on a do or and undo



Method Name
Function




execute( )
Method for implementing 2 things - (1) Making modifications to the 'content' string of the current object (2) Pushing the newly modified command object to the current object's history (3) Tracking the previous state of the 'content' string


unexecute( )
Method for implementing the authentication of an existing user



public class BoldCommand implements UndoableCommand {
    public HtmlDoc document;
    public String prevContent;
    public History history;

    public BoldCommand(HtmlDoc document, History history) {
        this.document = document;
        this.history = history;
    }

    @Override
    public void execute() {
        prevContent = document.getContent();
        document.makeBold();
        history.push(this);
    }

    @Override
    public void unexecute() {
        document.setContent(prevContent);
    }
}
BoldCommand.javaImplementing the ConcreteCommand - UndoCommandA child class of the Command interface.It does 2 things : It keeps track of the History of commands and changes it if an UndoableCommand's execute( ) method is runpublic class UndoCommand implements Command{
    private History history;

    public UndoCommand(History history) {
        this.history = history;
    }

    @Override
    public void execute() {
        if (history.size()>0){
            history.pop().unexecute();
        }
    }
}
UndoCommand.javaImplementing the InvokerA simple class that gives out an object that developers using this pattern use to execute some receiver function.Here, we simply create a HTMLDoc class that manages the state of a simple string ('con) - whether it is in bold or not.makeBold( ) - Command that calls makes the string content bold.On activating this makeBold( ) function object (instance of our HTMLDoc class), a Command object is created and the execute( ) method is called on itpublic class HtmlDoc {
    public String content;

    public void makeBold(){
        this.content = "<b>" + this.content + "</b>";
    }

    public String getContent() {
        return content;
    }

    public void setContent(String content) {
        this.content = content;
    }
}
HTMLDoc.javaUsing this pattern in our applicationImport all the necessary classes and modules into main.java for a final showdown of our application ! Here, I'll be show the adding the ¬†! var document = new HtmlDoc();
var historyOfCmds = new com.ankit.command.undo.History();
var boldCommand = new BoldCommand(document,historyOfCmds);
document.setContent("Namaskar !");
System.out.println(document.getContent());

boldCommand.execute();
System.out.println(document.getContent());

var undoCommand = new UndoCommand(historyOfCmds);
undoCommand.execute();
System.out.println(document.getContent());ConclusionCongrats on unlocked the secret of how most modern apps reverse your actions. You should feel pretty comfortable now developing the your own simple apps like text editors, with undo functionalities in a that contain the right balance of abstraction and control. Until next time...‚ÄåI shall also be writing some content on using the Command Pattern to implement Composite Commands and Undo, Redo functions so do check them out !!‚Äå‚Äå‚ÄåReferences..Whatever I have learnt and implemented here in code as well as the explanation is all from the teachings of the brilliant Mosh Hamedani and his course on Design Patterns which I would recommend you to take if you are really interested in exploring the beauty of Object Oriented ProgrammingThe Ultimate Design Pattern SeriesAce your coding job interview. Learn to write maintainable and extensible code.Code with MoshMosh Hamedani
React FontAwesome,A simple article enlisting the steps to use Font Awesome icons in React,This is more of a snippet than a blog.....It is always a pain to find the right documentation and application principle to implement such small functionalities seamlessly in our frontend applicationsHence, this blog. Nothing much....just a simple set of instructions to follow to start using fontawesome icons as React Components.Markdown
STEP 1 : Install these packagesnpm i --save @fortawesome/fontawesome-svg-core
npm install --save @fortawesome/free-solid-svg-icons
npm install --save @fortawesome/react-fontawesomeSTEP 2 : Set up a FontAwesomeIcons.js file in root directory to add all reqd icons to the ‚Äúlibrary‚Äù. NOTE THE ¬†faIcon	syntax (pascal cased) in the icon namesimport {library} from '@fortawesome/fontawesome-svg-core'
import {faCarBattery, faBatteryHalf, faBolt} from '@fortawesome/free-solid-svg-icons'

library.add(faCarBattery, faBatteryHalf, faSpinner, faDirections, faAtom, faBolt)
Importing all the icons into a library that we will use app-wideSTEP 3 : Call this FontAwesomeIcons.js file in your topmost parent component (preferably use App.js as it is the root parent component) to beam down all icons added to library to child componentsimport {library} from '@fortawesome/fontawesome-svg-core'
import {faCarBattery, faBatteryHalf, faBolt} from '@fortawesome/free-solid-svg-icons'

library.add(faCarBattery, faBatteryHalf, faSpinner, faDirections, faAtom, faBolt)
STEP 4 : Use the FontAwesomeIcons in any child component inheriting the library by simply Importing the FontAwesomeIcon componentimport React from 'react'
import { faBolt } from '@fortawesome/free-solid-svg-icons'Bolt	

const ChildComponent = () => {
    return (
        <div>
            <FontAwesomeIcon icon='bolt'></FontAwesomeIcon>
        </div>
    )
}

export default ChildComponent
Using the icons added to library. Note the ‚Äúicon‚Äù syntax here for calling icons added to the libraryConclusion
That's all there is to this folks ! 4 steps....its that easy. Hope this saved you some unnecessary roaming around for the right answer on Stack Overflow or finding the right docs
To now implement further customization on our imported icons....I recommend you to check out these official docs by Font Awesome
Font AwesomeThe world‚Äôs most popular and easiest to use icon set just got an upgrade. More icons. More styles. More Options.Font Awesome
Command Pattern,Using the command pattern to understand how most modern frameworks implement the right balance of abstraction and control,Used to decouple a sender (aka invoker) from a receiver during a command execution.The Command Pattern has a wide variety of use cases since by decoupling the sender and the receiver in such a way that they only interact through a completely customizable interface so there is actually a lot that can be done due to this decoupling just by tweaking the interface.Application : Literally every modern framework has Command Pattern implemented in some form or the other due the versatile application of this logic. It gives the developers (invoker) that use this framework complete customizability without actually needing to care what runs on the receiver side.Other uses :Implement a middleware functionality in framework using Composite Command architectureImplement undo, redo mechanisms by storing command objects that contain the logic to restore a particular state.Project Unified Modelling Language (UML) DiagramUML Explanation....
At its core, the command pattern consists of 4 interacting bodies



Body
Function




Invoker
Interacts with the interface to call a specific function on the receiver side


Command
It's the interface establishes and controls communication between the sender and the receiver. It declares an execute( ) method


Receivers
Classes that expose methods that hold the actual implementation logic of a receiver service (eg  : AddService, AuthService etc.)


ConcreteCommand
A child class of the interface that defines the execute( ) method and which receiver service it shall call.



ResourcesThe source code for all prominent design patterns has been properly implemented and documented in this GitHub repostitory by me.‚Äåcrew-guy/Design-PatternsA collection of the design patterns, implemented in java, as originally created by the GoF - crew-guy/Design-PatternsGitHubcrew-guyProject DescriptionHere, in this article, we will be building a simple application that decouples customer handling of any major services, be it Amazon or Netflix or Uber or any other company that has user authentication and management set up !!üî•‚ÄåImplementing the Command (Interface)An interface that establishes and manages communication between the sender and receiverOn some request from sender, it creates a Command object Also, it declares the execute( ) method which shall be defined on the receiver end to run the logic that answers the sender's request.public interface Command {
    void execute();
}
Command.javaImplementing the ReceiversThese are simply class that expose methods holding some implementation logic (eg : adding a customer, authenticating a customer etc.), to anyone that instantiates them 



Class Name
Method exposed
Function




AddCust
addCust()
Method for implementing the addition of a new use to a database


AuthCust
authCust()
Method for implementing the authentication of an existing user



public class AddService {
    public void addCustomer(){
        System.out.println("Adding a customer.....");
    }
}AddService.javapublic class AuthService {
    public void authCustomer(){
        System.out.println("Authenticating this customer.....");
    }
}AuthenticateService.javaImplementing the ConcreteCommandA child class of the Command interface.It does 2 things : It instantiates all the Receiver classes so that it can access the methods they exposeIt defines the execute( ) method by populating it with the method call to a Receiver service using the objects of the Receiver classesHere, we simply execute the logic for the addCust( ) or authCust( ) method by simply calling them on the objects of the AddService or AuthService class that we instantiated here.public class AddCommand implements Command {
    private AddService addService;
    private AuthService authService;

    public AddCommand(AddService addService, AuthService authService) {
        this.addService = addService;
        this.authService = authService;
    }

    @Override
    public void execute() {
        addService.addCustomer();
        authSerive.authCustomer();
    }
}
AddCommand.javaImplementing the InvokerA simple class that gives out an object that developers using this pattern use to execute some receiver function.Here, we simply create a Button class that calls the execute( ) method on the Command object.On clicking this Button object (instance of our Button class), a Command object is created and the execute( ) method is called on itpublic class Button {
    private String label;
    private Command command;

    public Button(Command command) {
        this.command = command;
    }

    public void click() {
        command.execute();
    }

    public String getLabel() {
        return label;
    }

    public void setLabel(String label) {
        this.label = label;
    }
}
Button.javaUsing this pattern in our applicationImport all the necessary classes and modules into main.java for a final showdown of our application ! Here, I'll be show the addition as well as authentication part ! var addService = new AddService();
var authService = new authService();
var command = new AddCommand(addService, authService);
var button = new Button(addCommand);
button.click();Here, I have done 2 things that can be changed when scaling up :I have called both the services for adding and authenticating a customer but you may choose to use an input from sender coupled with a switch statement inside the ConcreteCommand to conditionally call whichever receiver service you wantYou can make the receiver services extend a main Receiver interface or parent class to introduce more abstraction of the instances created in the ConcreteCommand to avoid any confusions as the size of your codebase grows because you start offering more receiver servicesConclusionCongrats on unlocked the secret of how most modern frameworks work under the hood. You should feel pretty comfortable now developing the your own frameworks functionalities in a that contain the right balance of abstraction and control. Until next time...‚ÄåI shall also be writing some content on using the Command Pattern to implement Composite Commands and Undo, Redo functions so do check them out !!References..
Whatever I have learnt and implemented here in code as well as the explanation is all from the teachings of the brilliant Mosh Hamedani and his course on Design Patterns which I would recommend you to take if you are really interested in exploring the beauty of Object Oriented Programming
The Ultimate Design Pattern SeriesAce your coding job interview. Learn to write maintainable and extensible code.Code with MoshMosh Hamedani
Template Model Pattern,One object many forms ! Here, we up our strategy game up by working with templates, inheritance and polymorphism,Used to manage and execute different functions for different objects, where functions share some common codeThe Template Pattern is implemented to perform different behaviors represented from context object using the template object(s).It is what is being currently used in a lot of places, substituting the State Pattern to follow the modern practice of decentralization of state into smaller components for a more robust component architecture in most frameworks like React JS, Next JS etc.‚Äå‚ÄåIn fact, React JS Hooks, which got introduced in 2016 and took the component driven framework world by a storm, implement the Template Model and Strategy Pattern logic under the hood !Feel free to browse through this detailed blog on Strategy Pattern for reference ¬†:‚ÄåStrategy PatternLearning the OOP way to execute different algorithms in different situations using the Strategy PatternAnkit Sanghvi | BlogAnkit SanghviTemplate Model Pattern is like a version of strategy pattern where the implementation is done through a parent abtract class instead of an interface to give the 2 options to the child classes - Polymorph the required methodsInherit the required methods right from how the parent defines themOk, but what is Polymorphing ?Polymorphism is one of the fundamentals of Object Oriented Programming. What it lays down is this - "Child classes extending an abstract parent class have to implement the methods (functions) prescribed by the parent class. This is other done by inheriting i.e. copying the method implementation from the parent, or by coming up with  their own implementation of the same method (aka polymorphing), which shall override the parent's implementaion."
Yeah sure, but what exactly is an abstract parent class ?In polymorphism, we use the concept of an abstract parent class where the parent class just provides a base template with some variables and functions definedChild classes extending this template can either inherit this template's definitions or come up with their own custom implementations of the methods, variables declared in the template (aka polymorphing the methods, variables).Salient features :Abstract parent classes can't be instantiated (can't make objects directly from these classes)Any abstract class that contains abstract methods, concrete methods (methods that can't be polymorphed) or bothAny class which extends this abstract class must override the abstract methods.Abstract methods can be declared only in abstract classes.
So how is this any different from the Strategy Pattern ?So how is this any different from the State Pattern ?




STRATEGY PATTERN
TEMPLATE MODEL PATTERN




The base layer is an interface
The base layer is a parent abstract class


Child classes compulsorily have to implement all the logic declared in the base layer themselves
Child classes have the flexibility to either implement all the logic declared in the base layer themselves (polymorphing) or simple inherit it from the parent




‚ÄåResourcesThe source code for all prominent design patterns has been properly implemented and documented in this GitHub repostitory by me.‚Äå‚Äåcrew-guy/Design-PatternsA collection of the design patterns, implemented in java, as originally created by the GoF - crew-guy/Design-PatternsGitHubcrew-guyProject DescriptionHere, in this article, we will be building a simple application that mimics digital transaction processing like any payments gateway like Google Pay, PayTM or any bank in the sense that we will layout the structure of how an audit trail is implemented by classes for transfering money and generating reports üî•‚Äå‚Äå
Setting up the Context
This class simply generates our context object i.e. our audit trail for the task method to do some functions on
public class AuditTrail {
    public void record(){
        System.out.println("Audit recorded !");
    }
}

AuditTrail.javaImplementing the TemplateA template is just common interface for exposing a multitude of functionalities offered by our softwareIt is implemented as an abstract parent class or as an interface in all OOP languages. It just has to enlist the methods that all classes extending it can polymorph or inherit in their own way for providing the - "same interface - different/same function" feature.‚Äå‚Äå‚Äå‚ÄåWe have defined 1 template - Task (which will be our Template class), with the following abstract and concrete methods defined.



Method Name
Method Type
Function of Method




doExecute()
abstract
Prototype of method for implementing the logic of the audit task being executed (eg :generate a report, transfer money )


execute()
concrete
Fixed method for running some startup middleware processing irrespective of whichever audit task you wish to execute (eg: generate a report, transfer money)



public abstract class Task {
    public AuditTrail auditTrail;

    public Task(){
        this.auditTrail = new AuditTrail();
    }

    public Task(AuditTrail auditTrail){
        this.auditTrail = auditTrail;
    }

    public void execute(){
        auditTrail.record();
        System.out.println("Starting execution of transaction......");

        doExecute();
    }

    protected abstract void doExecute();
}Task.javaHere, the protected keyword is used to ensure that only the child classes extending this class can access this method and no one else.It gives a sort of role based access control to our doExecute( ) method.Also, as visible here, the execute( ) method is what is called on each task to do 2 things- First, run some middleware that starts the transaction and records it- ¬†Run the task i.e. the crux of this transactionImplementing the Concrete classesChild classes that extend their respective Template class and implement the abstracted prototype methods listed by the interface ‚Äåthat they extend‚Äå



Concrete Class Name
Function




MoneyTransfer
Implementation of the logic for transferring money will exist here


GenerateReport
Implementation of the logic generating a report will exist here



public class TransferMoneyTask extends Task{
    @Override
    protected void doExecute() {
        System.out.println("Your money has been successfully transferred !");
    }
}
MoneyTransfer.javapublic class GenerateReportTask extends Task{
    @Override
    protected void doExecute() {
        System.out.println("Your report has been successfully generated !");
    }
}
GenerateReport.javaUsing this pattern in our applicationImport all the necessary classes and modules into main.java for a final showdown of our application ! Here, I'll be running both tasks just to show how some common code as well as some unique code, will run for both these tasks ‚Äåvar task = new TransferMoneyTask();
task.execute();
var task2 = new GenerateReportTask();
task.execute();
task2.execute();ConclusionCongrats on unlocked the secret of how most modern implementations of reactive applications work under the hood. You should feel pretty comfortable now developing the state responsive functionalities in a decentralized but structured manner in your applications. Until next time...‚Äå‚ÄåReferences..
Whatever I have learnt and implemented here in code as well as the explanation is all from the teachings of the brilliant Mosh Hamedani and his course on Design Patterns which I would recommend you to take if you are really interested in exploring the beauty of Object Oriented Programming
The Ultimate Design Pattern SeriesAce your coding job interview. Learn to write maintainable and extensible code.Code with MoshMosh Hamedani
Theory of Data Visualizations,An analysis into the design and thought process that goes into making beautiful data vizes that tell a story,Today it's all about storytelling and infographics and visuals are a key part of that. All researchers want to connect the reader with the data and that's what data visualization's and storytelling can do~Derek RicharHonestly, not everyone has the time in life to analyze each and every data point and extract it's significance and in combination with other data points, become aware of the bigger picture, whether it be the trend of a wildspread virus, or the income statement of a company or literally the interest of people in a certain programming language over pizza üòÇ
Everything is so data driven these days, that the need for interactive visualizations that help even non-mathematicians to understand trends, is of paramount importance
This is what this article will be about today. Not about libraries and frameworks and programming. Just about designing effective data visualizations (aka data vizes) in a beautiful manner, down to the most intricate details, such that the next pie chart you create will leap from the pages and tell the viewer a story ! Basic theory...
There are primarily just these 3 types of data vizes :



TYPE
ROLE




1. TABLES
To show separately indexed data in a systematic format


2.NETWORKS
To display connected data in the form of lines connecting data points


3. SPATIAL
To represent geographical or other areal data points in their raw form by giving importance to position/actual geo-location of data point



Also, there is the basic notion of marks and channels that just help us understand which sort of graph is the most accurate for our use case
MARKS AND CHANNELS

Marks
These are basically rows aka items aka entries in your database that you'll consider and use the attributes as properties whose quantities you wish to assess in several different ways (eg : over time, relative to other entries etc.) axis and plot your viz.
Channels
These are columns aka attributes each mark in your database is associated with and it is these properties that we wish to study of a single or multiple item(s) in the database with the help of vizes
These are divided as follows



TYPE
MEANING
EXAMPLE




Categorical
Comparison of same attributes of items in the database but different categories
Comparing the oil prices(Y-Axis) of India, China and Japan over time


Ordered : Ordinal
Comparison of like attributes of a discrete form in same category
eg : S, M, L, XL sizes in a T Shirt


Ordered : Quantitative
Comparison of like attributes of a continuous form
eg : Spread of a virus over time





Use cases
Following are the use cases of application of our marks-channels that I learnt while following this brilliantly taught dataviz course






Picking your graph....
For this one, I actually read through this amazing writeup with examples for picking a suitable template and injecting the most apt graph for your use case in it.






Next steps ...
Congratulations ! You have now mastered the most basic fundamentals that give you mastery over conveying your data in the most engaging manner possible !
Technology has provided us with so many amazing tools for dataviz that we honestly can't keep track of it. Nevertheless, as long as you get the basics right, all your charts and graphs will be on point !

If you wish to explore this domain more, I would highly recommend you to read my blog on creating a simple bar chart with Next JS and D3 JS, 2 JS based libraries that integrate together brilliantly to make next level performance enhanced, interactive datavizes
D3 and NextA brief explainer on creating fast, interactive dashboards and data visualizations with D3.JS and Next.JSAnkit Sanghvi | BlogAnkit SanghviThanks a lot for reading this content and I really wish you the best of luck on your journey to becoming a dataviz ninja ! Until next time....ü•∑
Strategy Pattern,Learning the OOP way to execute different algorithms in different situations using the Strategy Pattern,Used to represent and execute different kinds of behaviors (like algorithms, functions) based on different strategy objectThe Strategy Pattern is implemented to perform different behaviors represented from context object using the strategy object(s)It is what is being currently used in a lot of places, substituting the State Pattern to follow the modern practice of decentralization of state into smaller components for a more robust component architecture in most frameworks like React JS, Next JS etc.In fact, React JS Hooks, which got introduced in 2016 and took the component driven framework world by a storm, implement the Strategy Pattern logic under the hood ! ¬†Feel free to browse through this detailed blog on State Pattern for reference ¬†:State Pattern in JavaUnderstanding how the state-change-reactiveness of modern software is implemented by studying the state patternAnkit Sanghvi | BlogAnkit SanghviSo how is this any different from the State Pattern ?




STATE PATTERN
STRATEGY PATTERN




Used to change behavior of the context object in different situations
Used to change behavior of the context object in different situations


The context object can have only a single state and all its different behaviors are represented by sub-classes of the State interface so the state is centralised
The context object can have multiple states in the form of strategy objects implementing the Strategy interface, thus, different behaviors can be executed separately, independently due to the decentralised state


eg : state management in class components in React JS which changes the entire state together
eg : state management in functional components in React JS to change and manage individual state properties



‚Äå‚ÄåResourcesThe source code for all prominent design patterns has been properly implemented and documented in this GitHub repostitory by me.‚Äåcrew-guy/Design-PatternsA collection of the design patterns, implemented in java, as originally created by the GoF - crew-guy/Design-PatternsGitHubcrew-guyProject DescriptionHere, in this article, we will be building a simple application that mimics digital illustration softwares like Photoshop, Gimp etc. in the sense that when a different img processing format (eg : JPG, PNG) and a different filtering strategy (eg : Black and White filter, RGB filter ) is implement, different functions are fired !!üî•‚ÄåProject Unified Modelling Language (UML) DiagramImplementing the StrategiesA strategy is just common interface for exposing a multitude of functionalities offered by our softwareIt can be either implemented as an abstract parent class or as an interface in all OOP languages. It just has to enlist the methods that all classes extending it have to polymorph in their own way for providing the - "same interface - different function" feature.‚Äå‚Äå‚Äå‚ÄåThis choice is made simply by asking : Do I want my functionalities to have some code in common apart from the definition of their methods ? If yes, then implement state as an abstract parent class otherwise implement it as an interfaceWe have defined 2 strategies - Compressor and Filter, each with their own prototype method‚Äå defined for compressing and filtering images.



Strategy Name
Protoype Name
Function of Prototype




Compressor
compress()
Prototype of method for implementing the compression of an image


Filter
apply()
Prototype of method for implementing the filtering of an image



public interface Compressor {
    void compress(String fileName);
}
Compressor.java (Interface aka Strategy 1)public interface Filter {
    void apply(String fileName);
}
Filter.java (Interface aka Strategy 2)Implementing the class using these tools (Context) We will just define class that just calls and accesses different strategies based on different inputs from our sidepublic class ImageStorage {
    public void store(String fileName, Compressor compressor, Filter filter){
        compressor.compress(fileName);
        filter.apply(fileName);
    }
}
ImageStorage.javaImplementing the ConcreteStrategy classesChild classes that extend their respective Strategy interface and implement the abstracted prototype methods listed by the interface ‚Äåthat they extend



Strategy Name
Concrete Strategy Name
Function




Compressor
JpegCompressor
Implementation of the compression the image by the JPEG algorithm



PngCompressor
Implementation of the compression the image by the PNG algorithm


Filter
BandFilter.java
Implementation of the filtering the image by the Black and White filter



RGBFilter.java
Implementation of the filtering the image by the color filter



public class JpegCompressor implements Compressor{
    @Override
    public void compress(String fileName) {
        System.out.println("Compressing "+ fileName +" using JPEG format");
    }
}
JpegCompressor.javapublic class PngCompressor implements Compressor{
    @Override
    public void compress(String fileName) {
            System.out.println("Compressing "+ fileName +" using PNG format");
    }
}
PngCompressor.javapublic class BandFilter implements Filter{
    @Override
    public void apply(String fileName) {
        System.out.println("Filtering " + fileName + "  using the RGB filter");
    }
}
BandFilter.javapublic class RGBFilter implements Filter{
    @Override
    public void apply(String fileName) {
        System.out.println("Filtering " + fileName + "  using the RGB filter");
    }
}
JpegCompressor.javaUsing this pattern in our applicationImport all the necessary classes and modules into main.java for a final showdown of our application ! Here, I'll be storing images using just 2 permutations of selecting a Compressor and Filter options available but you can go with whichever option you prefer var imageStorage = new ImageStorage();
imageStorage.store("image1",new JpegCompressor(), new BandWFilter());
imageStorage.store("image1",new PngCompressor(), new RGBFilter());‚ÄåConclusionCongrats on unlocked the secret of how most modern implementations of reactive applications work under the hood. You should feel pretty comfortable now developing the state responsive functionalities in a decentralized but structured manner in your applications. Until next time...‚ÄåReferences..
Whatever I have learnt and implemented here in code as well as the explanation is all from the teachings of the brilliant Mosh Hamedani and his course on Design Patterns which I would recommend you to take if you are really interested in exploring the beauty of Object Oriented Programming
The Ultimate Design Pattern SeriesAce your coding job interview. Learn to write maintainable and extensible code.Code with MoshMosh Hamedani
Injecting some sass,Giving your CSS superpowers using the Sass language extension,A stitch in time saves nine.Sass (Syntactically Awesome Style Sheets) is the most popular solution to writing well-structured, cross-browser supported CSS in the most developer friendly way possible. Once you switch to this next level transpiling option, there is absolutely no chance that you're going back to writing styling of nested elements the way CSS forces you toSetting up a project in Sass..
It's just about following these 4 steps that'll save you tonnes of time and energy that CSS takes up


Create a separate folder called sass and create a main file called styles.scss (arbitrary name) inside of it. Also, create a folder called css and create a file styles.css (arbitrary name)


Install a compiler

This can be implemented by installing the NPM package for your project
npm i sass
Alternatively, depending on the IDE you use, you could simply install
a plugin / extension that does it for you. for VSCode : Extension, for
WebStorm : Plugin



Feel free to create more supporting files to compartmentalize your code into smaller CSS modules by following these 2 rules

The filename should be prefixed with an underscore i.e. '_' eg: _header.scss
This file must be imported in the scss directory using the @import option in scss.



Next, create a .vscode (for VSCode users) or .idea (for WebStorm users) folder where you set up a configuration file mapping the source file (styles.scss in sass folder) to the destination file (styles.css in the css folder).
This basically just tells your transpiler to take all the code imported from separate CSS modules and the code present in your source file to be compiled into the destination CSS file that can be globally imported into your project's top level component


Importing the _variables.scss module into the main styles.scss fileSignificant superpowers
Sass is basically CSS with superpowers. I've listed the most common ones that you'll be working with most of the time
1. VariablesVariables are a way to store information that you want to reuse throughout your stylesheet. You can store things like colors, font stacks, or any CSS value you think you'll want to reuse. Sass uses the $ symbol to make something a variable. Here's an example:SCSS SYNTAX$font-stack:    Helvetica, sans-serif;
$primary-color: #333;

body {
  font: 100% $font-stack;
  color: $primary-color;
}
CSS OUTPUTbody {
  font: 100% Helvetica, sans-serif;
  color: #333;
}



When the Sass is processed, it takes the variables we define for the $font-stack and $primary-color and outputs normal CSS with our variable values placed in the CSS. This can be extremely powerful when working with brand colors and keeping them consistent throughout the site.2. NestingWhen writing HTML you've probably noticed that it has a clear nested and visual hierarchy. CSS, on the other hand, doesn't.Sass will let you nest your CSS selectors in a way that follows the same visual hierarchy of your HTML. Be aware that overly nested rules will result in over-qualified CSS that could prove hard to maintain and is generally considered bad practice.With that in mind, here's an example of some typical styles for a site's navigation:SCSS SYNTAXnav {
  ul {
    margin: 0;
    padding: 0;
    list-style: none;
  }

  li { display: inline-block; }

  a {
    display: block;
    padding: 6px 12px;
    text-decoration: none;
  }
}
CSS OUTPUTnav ul {
  margin: 0;
  padding: 0;
  list-style: none;
}
nav li {
  display: inline-block;
}
nav a {
  display: block;
  padding: 6px 12px;
  text-decoration: none;
}


You'll notice that the ul, li, and a selectors are nested inside the nav selector. This is a great way to organize your CSS and make it more readable.3. MixinsA mixin lets you make groups of CSS declarations that you want to reuse throughout your site. You can even pass in values to make your mixin more flexible. A good use of a mixin is for vendor prefixes. Here's an example for transform.SCSS SYNTAX@mixin transform($property) {
  -webkit-transform: $property;
  -ms-transform: $property;
  transform: $property;
}
.box { @include transform(rotate(30deg)); }
CSS OUTPUT.box {
  -webkit-transform: rotate(30deg);
  -ms-transform: rotate(30deg);
  transform: rotate(30deg);
}

To create a mixin you use the @mixin directive and give it a name. We've named our mixin transform. We're also using the variable $property inside the parentheses so we can pass in a transform of whatever we want. After you create your mixin, you can then use it as a CSS declaration starting with @include followed by the name of the mixin.4. Extend/InheritanceUsing @extend lets you share a set of CSS properties from one selector to another. It helps keep your Sass very DRY (Don't Repeat Yourself). In our example we're going to create a simple series of messaging for errors, warnings and successes using another feature which goes hand in hand with extend, placeholder classes. A placeholder class is a special type of class that only prints when it is extended, and can help keep your compiled CSS neat and clean.SCSS SYNTAX/* This CSS will print because %message-shared is extended. */
%message-shared {
  border: 1px solid #ccc;
  padding: 10px;
  color: #333;
}

// This CSS won't print because %equal-heights is never extended.
%equal-heights {
  display: flex;
  flex-wrap: wrap;
}

.message {
  @extend %message-shared;
}

.success {
  @extend %message-shared;
  border-color: green;
}

.error {
  @extend %message-shared;
  border-color: red;
}

.warning {
  @extend %message-shared;
  border-color: yellow;
}
What the above code does is tells .message, .success, .error, and .warning to behave just like %message-shared. That means anywhere that %message-shared shows up, .message, .success, .error, & .warning will too. Each of these classes will get the same CSS properties as %message-shared. This helps you avoid having to write multiple class names on HTML elements.Note that the CSS in %equal-heights isn't generated, because %equal-heights is never extended.Evidence ..
This is how you can get upto 95% reduction in the number of lines you code by writing styling in Sass instead of CSS
Conclusion...
And that is how, in 5 easy steps, you go from a junior developer to an experienced frontend engineer writing extensible, reusable, readable code that is enjoyable to read and work upon !
If you're interested in checking out more of the CSS superpowers that Sass and other CSS language extensions like Less grant, I would highly recommend you to go read their own beautifully written and explained docs.
Sass: DocumentationSyntactically Awesome Style SheetsDevDocs ‚Äî Less 4 documentationLess 4.0.0 API documentation with instant search, offline support, keyboard shortcuts, mobile version, and more.
Autocomplete,A mini-project on implementing autocomplete functionality in your search bar,What is the point of having high performance computers accessible right in the palm of our hand if we need to type everyting verbatim, in forms that we fill ?Thankfully, simple sorting and filtering solutions like implementing autocomplete in frontend technologies save us the effort and time and also introduces a layer input validation by abstracting away most of the form selection logic away from the user.At its base...
Autocomplete is just about 3 things,

Re-fetching the data from our API endpoint as the user keeps changing the characters in the input
Filtering it as per the query typed
Sorting the filtered results in a form most convenient to the user
Re-rendering the UI to show the newly sorted, filtered results.

Let's start typing...
So right here, we are going to build a simple HTML input element using JSX and fill it dynamically with search results from the Pokemon API as the user keeps typing more characters
Obviously, you'll need a frontend technology to implement this. I am choosing React as I'm really comfortable with it but you can choose any technology or framework of your choice Understanding the concept ....
Pretty simple, nothing to worry about. Remember, we just gotta do 4 things. Nice and smooth, right ?
The following table is an easy explainer on how this feature is implemented using simple state management



STATE
ABOUT
SETTER FUNCTION




search
Stores the string that the user types in the input and which will be used for searching
setSearch - Get's called whenever the input element is changed and causes a re-render (which leads to a re-render)


options
Stores the array of items that the form suggests as the user types in the input element
setOptions - Updates the options state with data based on the new search value when the component is re-rendered


display
Nothing related to core autcomplete functionality.
Just provides that UI feature where the options list is only visible when the use is actually typing in the input element



 
The following table is an easy explainer on how React hooks (basically just getter-setter functions of state) help us connect state management to UI re-rendering




HOOK
FEATURE
IMPLEMENTATION




useState
State management
Provides getter-setter functions for our options and search state


useRef
DOM access
Simply helps us access the DOM input element and change our search state based on the value typed in this element


useEffect
Re-fetching and re-populating data
Updates the options state on each re-render with data based on the new search   value



The code implementation
Hardly a few paragraphs since the logic is just simple and pure
import React, { useEffect, useState, useRef } from "react";

const Auto = () => {
  const [display, setDisplay] = useState(false);
  const [options, setOptions] = useState([]);
  const [search, setSearch] = useState("");
  const wrapperRef = useRef(null);

  useEffect(() => {
    const pokemon = [];
    const promises = new Array(20)
      .fill()
    // Refetching
      .map((v, i) => fetch(`https://pokeapi.co/api/v2/pokemon-form/${i + 1}`));
    Promise.all(promises).then(pokemonArr => {
     // Filtering 
     return pokemonArr.map(value =>
        value
          .json()
          .then(({ name, sprites: { front_default: sprite } }) =>
            pokemon.push({ name, sprite })
          )
      );
    });
    setOptions(pokemon);
  }, []);

  // Simply closes the input when we click outside of it
  useEffect(() => {
    window.addEventListener("mousedown", handleClickOutside);
    return () => {
      window.removeEventListener("mousedown", handleClickOutside);
    };
  });

  const handleClickOutside = event => {
    const { current: wrap } = wrapperRef;
    if (wrap && !wrap.contains(event.target)) {
      setDisplay(false);
    }
  };

  const updatePokeDex = poke => {
    setSearch(poke);
    setDisplay(false);
  };

  return (
    <div ref={wrapperRef} className="flex-container flex-column pos-rel">
      <input
        id="auto"
        onClick={() => setDisplay(!display)}
        placeholder="Type to search"
        value={search}
        onChange={event => setSearch(event.target.value)}
      />
      {display && (
        <div className="autoContainer">
          {options
            .filter(({ name }) => name.indexOf(search.toLowerCase()) > -1)			
      		// Sorting
            .map((value, i) => {
              return (
                <div
                  onClick={() => updatePokeDex(value.name)}
                  className="option"
                  key={i}
                  tabIndex="0"
                >
                  <span>{value.name}</span>
                  <img src={value.sprite} alt="pokemon" />
                </div>
              );
            })}
        </div>
      )}
    </div>
  );
};

function App() {
  return (
    <div className="App">
      <h1>Custom AutoComplete React</h1>
      <div className="logo"></div>
      <div className="auto-container">
        <Auto />
      </div>
    </div>
  );
}

export default App;4 steps - fetching, sorting, filtering, re-renderingConclusion
That's all there is, to this amazing functionality. You can now do interesting things with the fetching, sorting and filtering parts by integrating smart technologies like Elasticsearch to filter through large database at lightning speeds and pinpoint accuracy to show the most relevant results to the user.
Alternatively, you could also use something like Algolia, that provides an out-of-the-box integration of "smart searching" with "UI re-rendering" and just exposes simple to implement components that help you do this.
Either ways, you now have the power to create powerful search features with enhanced UI/UX for your frontend apps !
Iterator Pattern,Using the iterator pattern to abstract unnecessary complications in iteration logic as type of collection changes,Abstract away the iteration over a collection of items.The Iterator Pattern is used basically when you have a collection of items you wish to iterate over, but don't exactly know how the collection has been implemented as (is it a list ? is it a stack ? etc).Thus, we simply abstract away the logic of iteration to an interface and make sure that the user's of this interface are exposed to the same set of methods for iteration, irrespective of what our collection is implemented asAnalogy....Think of the interface as your TV remote and the methods as the buttons. No matter what circuitry logic is implemented under the hood, inside the remote, you (the user of this interface) simply need to know that which button does what and it does it !


CODE
ANALOGY
FUNCTION




Interface(aka iterator)
TV remote
Exposes the logic in the form of easy to use methods


IteratorTypes
Wiring inside the TV remote
Classes extending the interface and containing implementation logic


Methods
Buttons on TV remote
Called by the user and implemented under the hood by iterator types


Caretaker
User of TV remote
Just uses the methods



Eg : Browser history, depending on your browser may either be stored as a List of Strings or an Array (implemented as a stack) of stringsThus, every time the implementation of the collection is changed, we just need to create a new Iterator Type which will handle the under-the-hood-logic of this new implementation of our collection, so users can again easily iterate over itResourcesThe source code for all prominent design patterns has been properly implemented and documented in this GitHub repostitory by me.‚Äåcrew-guy/Design-PatternsA collection of the design patterns, implemented in java, as originally created by the GoF - crew-guy/Design-PatternsGitHubcrew-guyProject DescriptionHere, in this article, we will be building a simple collection of browser history URLs implemented differently (as a list and as a stack) and their corresponding iterators for handling the same‚ÄåImplementing the CaretakerPerforms the state management



Type
Name
Function




Methods
push( )
Add a new item to the collection



pop( )
Remove the last item from the collection



createIterator( )
Creates an object of the Iterator (interface) and of whatever IteratorType (child class extending the interface), to iterate over the current implementation of the collection of items



    private List<String> urls = new ArrayList<>();
    private int index = 0;

    public void push(String url){
        urls.add(url);
    }

    public String pop(){
        var lastIndex = urls.size() -1;
        var lastUrl = urls.get(lastIndex);
        urls.remove(lastUrl);

        return lastUrl;
    }

    public List<String> getUrls() {
        return urls;
    }

    public void setUrls(List<String> urls) {
        this.urls = urls;
    }

    public Iterator createIterator(){
        return new ListIterator(this);
    }

    public class ListIterator implements Iterator{
        public BrowseHistory history;

        public ListIterator(BrowseHistory history){
            this.history = history;
        }

        @Override
        public boolean hasNext() {
            return (index < history.getUrls().size());
        };

        @Override
        public String current() {
            return history.getUrls().get(index);
        }

        @Override
        public void next() {
            index++;
        }
    }
}BrowseHistory.javaImplementing the IteratorAn interface with child classes that expose the basic methods required to iterate over a collection, whilst abstracting away the iteration logic



Type
Name
Function




Methods
hasNext( )
Returns a boolean that specifies whether we've reached the end of the collection



current( )
Returns an item of the collection



next( )
Increments iteration counter



public interface Iterator {
    boolean hasNext();
    String current();
    void next();
}Iterator.javaImplementing the IteratorType classesChild classes that extend the Iterator interface Each such class contains all the iteration logic over a particular implementation of the collection (eg : List, Stack) but abstracts away all this logic, serving up only 3 methods, as enliseted by the Iterator interface.‚Äåpublic class BrowseHistoryForArray {
    private String[] urls = new String[10];
    private int index = 0;
    private int count = 0;

    public void push(String url){
        urls[count] = url;
        count++;
    }

    public String pop(){
        --count;
        return urls[count];
    }

    public String[] getUrls() {
        return urls;
    }

    public void setUrls(String[] urls) {
        this.urls = urls;
    }

    public Iterator createIterator(){
        return new ArrayIterator(this);
    }


    public class ArrayIterator implements Iterator{
        public BrowseHistoryForArray history;

        public ArrayIterator(BrowseHistoryForArray history){
            this.history = history;
        }

        @Override
        public boolean hasNext() {
            return (index < history.count);
        }

        @Override
        public String current() {
            return history.urls[index];
        }

        @Override
        public void next() {
            index++;
        }
    }

}BrowseHistoryForArray.javapublic class BrowseHistoryForList {
    private List<String> urls = new ArrayList<>();
    private int index = 0;

    public void push(String url){
        urls.add(url);
    }

    public String pop(){
        var lastIndex = urls.size() -1;
        var lastUrl = urls.get(lastIndex);
        urls.remove(lastUrl);
        return lastUrl;
    }

    public List<String> getUrls(){return urls;}

    public void setUrls(List<String>urls){this.urls = urls;}

    public Iterator createIterator(){
        return new ListIterator(this);
    }


    public class ListIterator implements Iterator{
        public BrowseHistoryForList history;

        public ListIterator(BrowseHistoryForList history){
            this.history = history;
        }

        @Override
        public boolean hasNext() {
            return (index < history.getUrls().size());
        }

        @Override
        public String current() {
            return history.getUrls().get(index);
        }

        @Override
        public void next() {
            index++;
        }
    }

}
BrowseHistoryForArray.javaUsing this pattern in our applicationImport all the necessary classes and modules into main.java for a final showdown of our application ! I have used the BrowseHistoryForArray.java implementation here but you can go forward and try out the List implementation (BrowseHistoryForList.java) as well.var iteratorHistory = new BrowseHistory();
        iteratorHistory.push("a");
        iteratorHistory.push("b");
        iteratorHistory.push("c");

        Iterator iterator = iteratorHistory.createIterator();
        while(iterator.hasNext()){
            System.out.println(iterator.current());
            iterator.next();
        }What basically happens when the implmentation of collection changes :push( ), pop( ) methods in Caretaker are redefined as per new implementationcreateIterator( ) returns a different IteratorType, (if not existent, makes such a IteratorType) which knows how to iterator over this implementation of the collection.All changes in code are confined only to the CareTaker, and if required, IteratorType class, instead of dependent classes.ConclusionCongrats on unlocked the secret of how the most complex iterations are made simple by workings under the hood. You should feel pretty comfortable now developing the next level iteration functionalities in your applications. Until next time...‚Äå‚ÄåReferences..
Whatever I have learnt and implemented here in code as well as the explanation is all from the teachings of the brilliant Mosh Hamedani and his course on Design Patterns which I would recommend you to take if you are really interested in exploring the beauty of Object Oriented Programming
The Ultimate Design Pattern SeriesAce your coding job interview. Learn to write maintainable and extensible code.Code with MoshMosh Hamedani
D3 and Next,A brief explainer on creating fast, interactive dashboards and data visualizations with D3.JS and Next.JS,Talk about a match made in heaven üöÄD3 and Next are the perfect combination for seamlessly creating reusable, extensible and component-driven interactive dashboards that can be used in literally any industry you can imagine.With Next JS we can weild the high speed and performance powers to make the UI/UX even more pleasing due to prefetching of all the data that the dashboard is populated with. To learn more about Next JS, refer to my blog here :Next JS OverviewAn overview of the future of fast, SEO optimized frontend web software using the Next JS frameworkAnkit Sanghvi | BlogAnkit SanghviWhat is D3 ?Data Driven Documents JS aka D3.js is a javascript library to render the amazing charts based on data. It can be used to:Visualize charts Find ¬†patterns Show the comparison between two or more time-series data Draw the data in real time as it happens (for e.g. data generated by various sensors or internet traffic) Generate cool dynamic art on webpages. It just depends on user's imagination ‚ÄúD3 helps you bring data to life using HTML, SVG, and CSS. D3‚Äôs emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.‚Äù ~d3js.orgBut how does D3 work ?Well, charts are based on information coming from third-party resources which requires dynamic visualization during render time. SVG is a very powerful tool which fits well to this application case.What is SVG ?SVG stands for Scalable Vector Graphics which is technically an XML based markup language.It is commonly used to draw vector graphics, specify lines and shapes or modify existing images. You can find the list of available elements here.Pros:Supported in all major browsers;It has DOM interface, requires no third-party lib;Scalable, it can maintain high resolution;Reduced size compared to other image formats.Enough talk, let's get this party started....
Here, we are going to build a very simple data visualization using D3 and Next JS that shows the population of the 10 most populous countries in the form of a simple horizontal bar chart
Resources
The code for this app has been properly committed and document by me in this GitHub repo for your perusal
crew-guy/next-d3A collection of data visualizations created using react and D3 libraries of JS - crew-guy/next-d3GitHubcrew-guyData Fetching
Start by initializing the Next JS project and adding the D3 js NPM package npx create-next-app population-chart
npm i d3The data for all this visualization has been easily made accessible by me on this GitHub Gist, feel free to explore and tweak it; else just use the URL below in your projectGitHub GistThis is the UN World Population Prospect datasetThis is the UN World Population Prospect dataset. GitHub Gist: instantly share code, notes, and snippets.Gist262588213843476Direct URLhttps://gist.githubusercontent.com/crew-guy/e1ae0b5db6ace5eda68bc8fb9e903576/raw/UN%2520World%2520Population%2520Dataset%2520-%2520Sheet1.csvI have abstracted all the logic for data prefetching in - src/helpers/fetchData.js. Here, we have used the d3.csv() function to parse the data as per my liking and just extracted ¬†2020's population for the top 10 most populous nationsimport * as d3 from 'd3'

export const fetchData = async() => {
    const csvUrl = "https://gist.githubusercontent.com/crew-guy/e1ae0b5db6ace5eda68bc8fb9e903576/raw/UN%2520World%2520Population%2520Dataset%2520-%2520Sheet1.csv"
    
    // Using d3.csv()
    const row = (d) => {
        d.Population = +d['2020']
        return d
    }
    const fullData = await d3.csv(csvUrl, row)
    const data = fullData.slice(0,10)
    const text = d3.csvFormat(data)
    return data;
}src/helpers/fetchData.jsCreating the Chart from the Data
For this part, I have simply used the component development tech to create separate components for the data marks and both the axes and just integrated it all, along with the data fetching logic in my landing page file Next JS



Filepath
Function




AxisTop.js
./src/components/AxisTop.js
Logic for the X-Axis (showing population count) marks


AxisBottom.js
./src/components/AxisBottom.js
Logic for the Y-Axis (showing population country) marks


Marks.js
./src/components/Marks.js
Location of the actual plot points on the graph


index.js
./pages/index.js
File that integrates all these above components into 1 single chart



export const AxisTop = ({xScale, innerHeight, tickFormat}) => (
    xScale.ticks().map(tickValue=>(
            <g className="tick" transform={`translate(${xScale(tickValue)},0) `} >
                <line y2={innerHeight} />
                <text
                    key={tickValue}
                    style={{textAnchor:"middle"}}
                    dy='0.71em'
                    y={innerHeight}
                >{tickFormat(tickValue*1000)}</text>
            </g>
        ))
)./src/components/AxisTop.jsexport const AxisBottom = ({yScale}) => (
    yScale.domain().map(tickValue=>(
        <g className="tick">
            <text
            key={tickValue}
            x={-3}
            style={{textAnchor: 'end'}}
            dy="0.32em"
            y={yScale(tickValue) + yScale.bandwidth()/2}
            >
            {tickValue}
            </text>
        </g>
    ))
)./src/components/AxisBottom.jsexport const Marks = ({data, xScale, yScale}) => (
    data.map(d => (
            <rect
                x={0}
                y={yScale(d.Country)}
                width={xScale(d.Population)}
                height={yScale.bandwidth()}
            />
        ) )
)./src/components/Marks.jsimport Head from 'next/head'
import styles from '../styles/Home.module.css'
import * as d3 from 'd3'
import {useState, useEffect} from 'react'
import {scaleLinear, scaleBand, max, format} from 'd3'
import {AxisTop} from '@components/AxisTop'
import {AxisBottom} from '@components/AxisBottom'
import {Marks} from '@components/Marks'
import {fetchData} from '@helpers/fetchData'

export default function Home({data}) {
  const [height, setHeight] = useState(0)
  const [width, setWidth] = useState(0)

  useEffect(()=>{
    setHeight(window.innerHeight-100)
    setWidth(window.innerWidth-100)
  },[])

  const margin = {top:40, left:300, right:40, bottom:60}
  const innerWidth = width - margin.left - margin.right
  const innerHeight = height - margin.top - margin.bottom

  const xVal = d => d.Population
  const yVal = d => d.Country

  const yScale = scaleBand()
      .domain(data.map(yVal))
      .range([0,innerHeight])
      .padding(0.2)

  const xScale = scaleLinear()
      .domain([0, max(data,xVal)])
      .range([0, innerWidth])
  
  
  const xAxisLabelOffset = 40
  const siFormat = format(".2s");
  const tickFormat = tickFormat => siFormat(tickFormat).replace('G','B')
  

  return (
        <svg width={width} height={height}>
          <g transform={`translate(${margin.left},${margin.top})`} >
            <AxisTop xScale={xScale} innerHeight={innerHeight} tickFormat={tickFormat} />
            <AxisBottom yScale={yScale}/>
            <text
              className="axis-label"
              x={innerWidth / 2}
              textAnchor='center'
              y={innerHeight + xAxisLabelOffset}
            >Population</text>
            <Marks data={data} xScale={xScale} yScale={yScale} xVal={xVal} yVal={yVal} />
          </g>
        </svg>
)}

export const getStaticProps = async () => {
  const data = await fetchData()
  console.log(data)
  return {
    props:{
      data
    }
  }
}./pages/index.jsIn the main file, I have used the scaleBand, scaleLinear and max functions from d3 to setup the scale and the axis of the overall rendered SVG chart.
Also, I have written the getStaticProps( ) function to prefetch all the data (as defined in the code in the fetchData.js file) in advance for a performance boost.
Apart from that, this is just pretty basic React JS logic on rendering components using the array of data fetched

Thus, the final rendered chart looks like this !
Afterthought
This is just the sliver of the beginning of we can craft powerful and completely customized data visualization by the wielding the holy trifecta - D3, Next and SVG
There are no limitations to what you can create now. Maybe the next big stockbroking platform or the next Google Trends will be yours. Who knows ?
If you liked this article then be sure to check out these crazy amazing links containing tutorials as well as brilliantly designed data visualizations
The theory of Data VisualizationAn analysis into the design and thought process that goes into making beautiful data vizes that tell a storyAnkit Sanghvi | BlogAnkit SanghviAmelia WattenbergerData Visualization with D3, JavaScript, React - Full Course [2021]Learn Data Visualization with D3, JavaScript, and React in this 17-hour course.Part 2: https://youtu.be/H2qPeJx1RDIüé• Created by Curran Kelleher. His channel...YouTubeThe 52 Best ‚Äî And Weirdest ‚Äî Charts We Made In 2016In addition to a number of interactive graphics and updating dashboards, this year we published almost 1,000 charts and maps on FiveThirtyEight. Here are 52 of ‚Ä¶FiveThirtyEightAndrei ScheinkmanGraphic detail | The EconomistGraphic detail from The Economist. You‚Äôve seen the news, now discover the story.The Economistdata visualizationQuartz is a guide to the new global economy for people in business who are excited by change. We cover business, economics, markets, finance, technology, science, design, and fashion.QuartzInformation is Beautiful AwardsThe Kantar Information is Beautiful Awards celebrate excellence and beauty in data visualizations and infographics.Kantar Information is Beautiful AwardsUpshot ‚Äì FlowingDataFlowingDataFlowingDataNathan YauThe Upshot, Five Years InOur favorite, most-read, most distinct work since 2014.The New York Times The New York TimesThe Upshot Staff
State Pattern,Understanding how the state-change-reactiveness of modern software is implemented by studying the state pattern,Implement different actions on state changes in our programThe State Pattern is what is implemented under the hood in most of the popular web development, reactive frameworks like React JS, Angular JS, Next JS, Vue JS, Svelte JS and many others too !Basically....
Given a situation where you want part of the program to behave differently based on the state of another part of the program, you use the state pattern
Eg : On clicking a internal page's link in a reactive web app, the main page component gets changed whereas the navbar and footer stay constant.

ResourcesThe source code for all prominent design patterns has been properly implemented and documented in this GitHub repostitory by me.crew-guy/Design-PatternsA collection of the design patterns, implemented in java, as originally created by the GoF - crew-guy/Design-PatternsGitHubcrew-guyProject Description
Here, in this article, we will be building a simple application that mimics digital illustration softwares like MS Paint, Photoshop, Sketch etc. in the sense that when a different tool (Brush, Selection, Eraser), a different function is executed when the mouse is moved up or moved down

Implementing the State A common interface for a multitude of functionalities offered by our softwareIt can be either implemented as an abstract parent class or as an interface in all OOP languages. It just has to enlist the methods that all classes extending it have to polymorph in their own way for providing the - "same interface - different function" feature.This choice is made simply by asking : Do I want my functionalities to have some code in common apart from the definition of their methods ? If yes, then implement state as an abstract parent class otherwise implement it as an interface



Type
Name
Function




Methods
handleMouseUp
Prototype of method for handling a "mouse up" event



handleMouseDown
Prototype of method for handling a "mouse down" event



public interface Tool {
    void handleMouseDown();
    void handleMouseUp();
}
Tool.javaImplementing the class using these tools (Context)
We will just define class that just calls and accesses different tools based on different inputs from our side
Also, we will be using an enum to specify the different kind of tools that are available for use.public enum ToolType {
    SELECTION,
    BRUSH,
    ERASER
}ToolType.javapublic class Canvas {
//  USING THE STATE PATTERN
    public Tool currentTool;

    public void mouseUp(){
        currentTool.handleMouseUp();
    }

    public void mouseDown(){
        currentTool.handleMouseDown();
    }

    public Tool getCurrentTool(){
        return currentTool;
    }

    public void setCurrentTool(Tool requiredCurrentTool){
        this.currentTool = requiredCurrentTool;
    }

}
Canvas.javaImplementing the ConcreteState classesChild classes that extend the State interface by polymorphing the enlisted methods



Type
Name
Function




Methods
handleMouseDown
Tool-wise implementation of the "mouse down event". eg : brush tool will draw a downward arrow



handleMouseUp
Tool-wise implementation of the "mouse down event". eg : eraser tool will erase something below it



public class BrushTool implements Tool{

    //@Override
    public void handleMouseDown() {
        System.out.println("Downward arrow drawn");
    }

    //@Override
    public void handleMouseUp() {
        System.out.println("Downward arrow drawn");
    }
}

BrushTool.javapublic class SelectionTool implements Tool {
    //@Override
    public void handleMouseDown() {
        System.out.println("Selection Icon");
    }

    //@Override
    public void handleMouseUp() {
        System.out.println("Rectangular area selected !");
    }
}

SelectionTool.javapublic class EraserTool implements Tool{
    //@Override
    public void handleMouseDown() {
        System.out.println("Eraser Icon");
    }

    //@Override
    public void handleMouseUp() {
        System.out.println("Erased some stuff !");
    }
}
EraserTool.javaUsing this pattern in our applicationImport all the necessary classes and modules into main.java for a final showdown of our application ! Here, I'll be using the Eraser Tool for demo but you can go with whichever tool you prefer out of the 3 that we have builtvar canvas = new Canvas();
canvas.setCurrentTool(new EraserTool());
canvas.mouseDown();
canvas.mouseUp();main.javaConclusionCongrats on unlocked the secret of how most reactive applications work under the hood. You should feel pretty comfortable now developing the state responsive functionalities in your applications. Until next time...References..
Whatever I have learnt and implemented here in code as well as the explanation is all from the teachings of the brilliant Mosh Hamedani and his course on Design Patterns which I would recommend you to take if you are really interested in exploring the beauty of Object Oriented Programming
The Ultimate Design Pattern SeriesAce your coding job interview. Learn to write maintainable and extensible code.Code with MoshMosh Hamedani
Memento Pattern,An excerpt on implementing undo mechanism in software applications using the Memento Pattern,Implement undo mechanisms in our classesThe Memento Pattern is one of the most well-known and commonly used Structural Design Patterns.It uses Object Oriented Programming principles in a 3 class setup to easily implement undo mechanisms used in most software editors, ranging from VSCode and Google Docs to Photoshop and most browsers' go-back features. 
Resources
The source code for all prominent design patterns has been properly implemented and documented in this GitHub repostitory by me.

crew-guy/Design-PatternsA collection of the design patterns, implemented in java, as originally created by the GoF - crew-guy/Design-PatternsGitHubcrew-guyImplementing the History class

Also known as the Caretaker





Type
Name
Function




Variable
'states'
An array of objects (implemented as a stack) where each object extends the EditorState class


Methods
pop
Returns the most recent aka state object stored in the states  array. (top of the stack)



push
Adds a new state to the states list



public class History {
  public List<EditorState> states = new ArrayList<>();

  public void push(EditorState state){
    states.add(state)
  }
  public void pop(){
    var lastIndex = states.size() - 1;
    var lastState = states.get(lastIndex);
    states.remove(lastIndex);
    return lastState;
  }
}Implementing the EditorState class

Also known as the Memento





Type
Name
Function




Variables
state
An object having a content property


Methods
getContent
Retrieves the content (string) of the state object passed to it from the states list in the History class



public class EditorState {
  private final String = content;

  // Constructor
  public EditorState(String content){
    this.content = content;
  }

  public String getContent(){
    return this.content
  } 
}
Implementing the Editor class

Also known as the Originator





Type
Name
Function




Variable
content
Simple string that is passed from the user input


Methods
createState
Creates an EditorState object using the content string



restore
Retrieves the content as a string from the History class using EditorState's getContent() method



getContent
Getter for the content variable



setContent
Setter for the content variable



public class Editor {
  private String content;

  public String restore(EditorState state){
    this.content = state.getContent();
  }

  public EditorState createState(){
    return new EditorState(this.content);
  }

  public String getContent(){
    return this.content;
  }

  public void setContent(content){
    this.content = content
  }Using this pattern
Import and call the following in your main class :class Main {
  public static void main(String[] args) {
    var editor = new Editor()
    var history = new History()

    editor.setContent("a")
    history.push(editor.createState())

    editor.setContent("b")
    history.push(editor.createState())

    editor.setContent("c")
    history.push(editor.createState())

    editor.restore(history.pop())
  }Conclusion
Congrats on unlocked the secret of how a computer reverses time. You should feel pretty comfortable now developing the undo and redo functionalities in your applications. Until next time...

References..
Whatever I have learnt and implemented here in code as well as the explanation is all from the teachings of the brilliant Mosh Hamedani and his course on Design Patterns which I would recommend you to take if you are really interested in exploring the beauty of Object Oriented Programming
The Ultimate Design Pattern SeriesAce your coding job interview. Learn to write maintainable and extensible code.Code with MoshMosh Hamedani
Next JS,An overview of the future of fast, SEO optimized frontend web software using the Next JS framework,Create fast, SEO optimized frontend web apps with zero configAllows to build the content on the server so the 1st thing a user or bot sees on landing is fully rendered HTML.Hence, amazing SEOAfter that traditional CSR takes over and it works just like a regular web app Hence, amazing UX
Key concepts :What is Client Side Rendering (CSR) ? A traditional React App is rendered client sideBrowser starts with a shell of an HTML page (empty) lacking any rendered contentFrom there, the browser fetches the app.js file containing the React code to the page and make it more interactiveWhat is Static Site Generation (SSG) ? Pages are rendered at build time 
FEATURES :Generates HTML pages at build time.HTML is rendered on the server and uploaded to a storage bucket or static hostDelivered with high performance over a CDNDRAWBACKSData may become stale ‚áí need to rebuild and redploy site when server side data changes, to implement the changesHard to scale ‚áí **Difficult to render all pages if website has too many pagesAPPLICATIONData that doesn't change oftenSites that have relatively low number of total pages (eg : blog ‚áí few 100 pages that don't change on a daily bases)What is Server Side Rendering(SSR) ? Pages are rendered at build time 
FEATURESGenerate each page at request timeIdeal for data that changes constantly as end user always gets the latest data from whatever data source it exists onDRAWBACKSSlower ‚áí Far less efficient as we gotta respond to requests instead of caching it all on a Global CDNInefficient data cachingWhat is Incremental Side Rendering(ISR) ? Regenerate pages in the background
By simply adding a revalidate option to the SSG function (atleast in Next JS), ¬†a page can be regenerated after a fixed time intervalA server rendering strategy midway between SSG and SSR.Main features1. ¬†"pages" directoryA file structure that mimics the routing setup in the appDirectory inside a Next projectEach JS file defined here exports a React component that represents a route in the appNext exposes its own Router to make navigation seamless_app.js ‚áíexists at highest levelmain entry point into the appevery individual page will start from this templateWhen a user navigates to an internal route URL (specified by the PAGE_NAME.js or DIR_NAME/PAGE_NAME.js files), Next will look for the default export component in the app and render it as a component Thus, every file here must have a default export componentDYNAMIC ROUTING ‚áí [ PAGE_NAME.js] - " [ ] " make the route dynamicuseRouter( ) ‚áí hook in Next JS to access the query parameters from the URL2. "api" directorySpecial part of Next for setting up routes that'll act as an integrated serverUseful since the code written here won't increase the client side JS bundle that needs to be ultimately sent over the netUseful when :Work is to be done on the backendYou wanna expose on an API for your end users3. Stylingglobals.css ‚áí Styles apply to the whole Next app*.module.css ‚áí Component specific stylesImport the stylesheet into a component simply as an object and call styles as if referencing properties on a JS objectCan write actual CSS syntax inline4. Server Rendering StrategiesImplementing SSG1. ¬†getStaticProps( ) ‚áí ¬†tells Next to pre-fetch the props for the component's prerendering2. ¬†getStaticPaths( ) ‚áí ¬†tells Next to pre-fetch the dynamic paths for the component's prerenderingSince we're working with dynamic routes, we gotta inform Next about the pages we gonna associate with this route/URLeg : To prerender (SSG), all the "id" routes associated with the "cars" route on a website ( Route - website.com/cars/paths ), we gotta inform Next about all these ids in advance3. getServerSideProps( ) ‚áí Fetches the props for the component's rendering at request timeCan copy all the code in a getStaticPaths( ) function and use it here. Next will implement the data handling under the hood for usDon't need getStaticPaths( ) here as each page is generated at request time so we don't need to inform Next in advance about any routes as there is no prerendering done5. SEOCan import Head from "next/head" and easily apply custom meta tags for each and every single page in the pages directoryAnything inside the Head component will be rendered at the head of the doc as title, meta-tags, etc.Custom MetaTag componentPass props to it from every single page to get next level custom SEO, for each pageimport Head from 'next/head'

const Meta = ({ title, keywords, description }) => {
  return (
    <Head>
      <meta name='viewport' content='width=device-width, initial-scale=1' />
      <meta name='keywords' content={keywords} />
      <meta name='description' content={description} />
      <meta charSet='utf-8' />
      <link rel='icon' href='/favicon.ico' />
      <title>{title}</title>
    </Head>
  )
}

Meta.defaultProps = {
  title: 'WebDev Newz',
  keywords: 'web development, programming',
  description: 'Get the latest news in web dev',
}

export default Meta
va
Export a static websiteAdd a script for next export ¬†or just integrate it into the build scriptWhen we run next build it runs for production (.next folder includes our server and stuff) and when we deploy to host, we deploy everythingHowever, when we export, it just goes into a folder called out ¬†and it is just a static websiteWhen it is deployed to server, as seen in the .gitignore file, it is the out ¬†directory is NOT deployed.NOTE : "serve" is an NPM package to serve static sites, SPAs, static filesserve -s out -p 8080
Conclusion
Technologies embracing these performance and SEO optimizing technologies are already taking over the software development decisions by a storm and soon they'll be industry standard.
It's gonna be a exciting future of saying goodbye to slow and irrelevant clickbait webapges
Git Squash and Rebase,Understanding grouping of small, related commits into more significant commits,As the size of the codebase increases...
more and more developers start collaborating and the entire branching in the project becomes one big, complex pile of commits flying in and out from branches and suddenly, understanding of the significance of a group of commits from a branch or by a developer starts to disappear.
This is where the squash and rebase features shine and help the version control become a lot easier to develop and follow
The basicsOut of the gate, the goal of both merging and rebasing is to take commits from a feature branch and put them onto another branch. Let‚Äôs start with how a quote-on-quote ‚Äúnormal‚Äù merge makes that happen.MergingSay I have a graph that looks like this. As you can see, I split off my feature branch at commit 2, and have done a bit of work.
If I run a merge, git will stuff all of my changes from my feature branch into one large merge commit that contains ALL of my feature branch changes. It will then place this special merge commit onto master. When this happens, the tree will show your feature branch, as well as the master branch. Going further, if you imagine working on a team with other developers, your git tree can become complex: displaying everybody else‚Äôs branches and merges.Rebasing
Now let‚Äôs take a look at how rebase would handle this same situation. Instead of doing a git merge, I‚Äôll do a git rebase. What rebase will do is take all of the commits on your feature branch and move them on top of the master commits. Behind the scenes, git is actually blowing away the feature branch commits and duplicating them as new commits on top of the master branch (remember, under the hood, commit objects are immutable and immovable). What you get with this approach is a nice clean tree with all your commits laid out nicely in a row, like a timeline.Rebasing caveatsAt this point, I think I better mention some caveats. Rebase doesn‚Äôt play super well with open-source projects and pull requests since it can be hard to trace, especially small changes that are introduced to a codebase. This point is a bit nuanced, but here is an article that does a good job of explaining why.It can also be dangerous if you‚Äôre working on a shared branch with other developers because of how Git rewrites commits when rebasing; however, in the workflow example below, I‚Äôll show you how to mitigate this risk.In practice: the actual commandsOn the development team I work with, we‚Äôve successfully adopted the workflow I‚Äôm about to show you and it works well for us. When I start development I always make sure the code on my local machine is synced to the latest commit from remote master# With my local master branch checked out
git pullNext, I‚Äôll check out a new branch so I can write and commit code to this branch ‚Äì keeping my work separated from the master branchgit checkout -b my_cool_featureAs I‚Äôm developing my feature, I‚Äôll make a few commits‚Ä¶git add .
git commit -m 'This is a new commit, yay!'Note: while I‚Äôm developing it‚Äôs likely that my fellow developers will have shipped some of their own changes to remote master. That‚Äôs ok, we can deal with that later.Now that I‚Äôm done developing my feature, I want to merge my changes back into remote master. To begin this process I‚Äôll switch back to local master branch and pull the latest changes. This ensures my local machine has any new commits submitted by my teammates.git checkout master
git pullWhat I want to do now is make sure my feature will jive with any new changes from remote master. To do this, I‚Äôll checkout my feature branch and rebase against my local master. This will re-anchor my branch against the latest changes I just pulled from remote master. Additionally at this point, Git will let me know if I have any conflicts and I can take care of them on my branchgit checkout my_cool_feature
git rebase masterNow that my feature branch doesn‚Äôt have any conflicts, I can switch back to my master branch and place my changes onto master.git checkout master
git rebase my_cool_featureSince I synced with remote master before doing the rebase, I should be able to push my changes up to remote master without issues.git pushRebase vs. MergeANALOGY - Writing a book 

Merging - Publishing the original draft of the book indicating the thought workflow from start to finish in its raw form


Rebasing - Publishing a more concise, edited and reviewed draft of this book in a way that is best for future readers.


Merging justification - Repository‚Äôs commit history is a record of what actually happened. It‚Äôs a historical document, valuable in its own right, and shouldn‚Äôt be tampered with. From this angle, changing the commit history is almost blasphemous; you‚Äôre lying about what actually transpired. So what if there was a messy series of merge commits? That‚Äôs how it happened, and the repository should preserve that for posterity.Rebasing justification -The opposing point of view is that the commit history is the story of how your project was made. You wouldn‚Äôt publish the first draft of a book, so why show your messy work? When you‚Äôre working on a project, you may need a record of all your missteps and dead-end paths, but when it‚Äôs time to show your work to the world, you may want to tell a more coherent story of how to get from A to B. to tell the story in the way that‚Äôs best for future readers.Squash - the basicsSay I have a graph that looks like this. You can see I split off some commits onto a bug fix branch:
I‚Äôll need to get my commits back onto mainline, and I can use merge or rebase to do that; however, with either solution, all my local branch commits would be preserved.Usually, preserving commits like this is a good idea, but let‚Äôs say you made a typo somewhere and had to create another commit to fix the spelling error. Or you have a bunch of local commits related to a bug fix, but you‚Äôd really rather just have all of those related commits under one roof. Enter squashing.Squashing allows you to rewrite history and combine together commits.
In practice: the actual commandsNow that you know what squash is, let‚Äôs take a look the actual commands. Again, we‚Äôll say my starting point is my bug fix branch with 3 commits.It would be nice if I didn‚Äôt have to preserve these extraneous commits as separate entities since they are all related to a bug fix. I‚Äôd rather combine them together into one clean commit.With my bug fix branch checked out, I‚Äôll start by running the interactive rebase command with HEAD~3. This lets Git know I want to operate on the last three commits back from HEAD.git rebase -i HEAD~3Git will open up your default terminal text editor (most likely vim) and present you with a list of commits:pick 7f9d4bf Accessibility fix for frontpage bug
pick 3f8e810 Updated screenreader attributes
pick ec48d74 Added comments & updated README
 
# Rebase 4095f73..ec48d74 onto 4095f73 (3 commands)
#
# Commands:
# p, pick <commit> = use commit
# r, reword <commit> = use commit, but edit the commit message
# e, edit <commit> = use commit, but stop for amending
# s, squash <commit> = use commit, but meld into previous commit
...There are a couple options here, but we‚Äôll go ahead and mark commits we‚Äôd like to meld with it‚Äôs successor by changing pick to squash. (If you‚Äôre using VIM, type i to enter insert mode)pick 7f9d4bf Accessibility fix for frontpage bug
squash 3f8e810 Updated screenreader attributes
squash ec48d74 Added comments & updated READMEPress ESC then type :wq to save and exit the file (if you are using VIM)At this point Git will pop up another dialog where you can rename the commit message of the new, larger squashed commit:# This is a combination of 3 commits
# This is the 1st commit message:

Accessibility fix for frontpage bug

# This is the commit message for #1:

Updated screenreader attributes

# This is the commit message for #2:

Added comments & updated README

# Please enter the commit message for your changes. Lines starting
# with '#' will be ignored, and an empty message aborts the commit
...Simply saving this file without making changes will result in a single commit with a commit message that is a concatination of all 3 messages. If you‚Äôd rather rename your new commit entirely, comment out each commit‚Äôs message, and write you‚Äôre own. Once you‚Äôve done, save and exit:
Afterthought
I think the merge approach is a more systematic approach to logging and developing commit messages in large codebases. Nevertheless, the importance of viewing raw history can't be overlooked as well. That's some food for thought. Until next time.....
Smart and Dumb Components,An examination of a new method of structuring your component driven applications in a systematic manner,There‚Äôs a simple pattern I find immensely useful when writing React applications. If you‚Äôve been doing React for a while, you have probably already discovered it. You‚Äôll find your components much easier to reuse and reason about if you divide them into two categories. I call them Container and Presentational components but I also heard Fat and Skinny, Smart and Dumb, Stateful and Pure, Screens and Components, etc. These all are not exactly the same, but the core idea is similar.


Presentational (Smart)
Containers (Dumb)




Are concerned with how things look.
Are concerned with how things work.


May contain both presentational and container components inside. Usually have some DOM markup and styles of their own.
May contain both presentational and container components inside. Usually don‚Äôt have any DOM markup of their own except for some wrapping divs, and never have any styles.


Often allow containment via this.props.children
Provide the data and behavior to presentational or other container components.


Have no dependencies on the rest of the app, such as Flux actions or stores.
Call Flux actions and provide these as callbacks to the presentational components.


Don‚Äôt specify how the data is loaded or mutated. Receive data and callbacks exclusively via props. Rarely have their own state (when they do, it‚Äôs UI state rather than data)
Are often stateful, as they tend to serve as data sources


Are written as functional components unless they need state, lifecycle hooks, or performance optimizations.
Are usually generated using higher order components such as connect() from React Redux, createContainer() from Relay, or Container.create() from Flux Utils, rather than written by hand.


Examples: Page, Sidebar, Story, UserInfo, List
Examples:UserPage, FollowersSidebar, StoryContainer, FollowedUserList.



Benefits of This ApproachBetter separation of concerns. You understand your app and your UI better by writing components this way.Better reusability. You can use the same presentational component with completely different state sources, and turn those into separate container components that can be further reused.Presentational components are essentially your app‚Äôs ‚Äúpalette‚Äù. You can put them on a single page and let the designer tweak all their variations without touching the app‚Äôs logic. You can run screenshot regression tests on that page.Less duplication of code. This forces you to extract ‚Äúlayout components‚Äù such as Sidebar, Page, ContextMenu and use this.props.children instead of duplicating the same markup and layout in several container components.AfterthoughtThus, structuring your React components in a proper manner will go a long way in building extensible, reusable and scalable software. Also, there's the aspect of reducing the complexity of a large codebase into more of a microservice-like-architecture for better maintenance and developer experience because, at the end of the day, a project is only as good as the energy of the team of developers working on it !!
React-Firebase auth,A tutorial on authenticating your frontent React apps with Google Firebase Auth service,What is authentication ?Authentication is the means of verifying that a person is who they claim to be based on 3 things :Something they know (eg : a password)Something they have (eg : a key)Something they are (eg : fingerprint)
Integrating authentication with frontend applications can be quite cumbersome and verbose using conventional methods like JSON Web Tokens as all the logic handling for encrypting and decrypting has to be written by the developer This is where Google Firebase's Authentication comes as a relief, helping the developer setup authentication using not just custom email and password, but even Single Sign On features with over 15 options including Facebook, GitHub, Apple etc,Ok, but what does Single Sign On (SSO) Mean ?SSO is the ability to sign into a third party website instead of creating a new login account specifically for that website.
Let's get started now !
All the code for this project can be accessed from hereThis project is consisting of 2 main parts :Creating a React Context for authentication and its related functions like sign-up, login, forgot-password etc.Using this context to secure the routes we wish toRun these commands to install the necessary packagesnpx create-react-app reactfire-auth
npm i firebase react-router-domCreating the authentication context
First, we setup firebase in our project :Create an account on Google Firebase and create a project (it's completely FREE).Then, just add an application to this project and get all the connection info about this projectThis is simply involving just exporting the necessary stuff out of Firebase API in one file to handle all Firebase API related stuff, to be used in components and logic elsewhere.import firebase from "firebase/app"
import "firebase/auth"

const app = firebase.initializeApp({
  apiKey: ,
  authDomain:,
  databaseURL: ,
  projectId: ,
  storageBucket: ,
  messagingSenderId: ,
  appId: 
})

export const auth = app.auth()
export default app
Next, we write the context code keeping these things in mind : 


Requirement
Solution
Implementation




Authenticate and store user object somewhere
Manage the state of the currently signed in user
useState to store this user's info


Secure multiple components and routes with this auth info
Share this user's auth info with everyone
Context API to share it with all components


Reverify if users sign out or do something else that changes their auth info
Reauthenticate each time the user's auth state changes
useEffect will be used


Too cumbersome to setup a context in every component
Create a custom hook to fetch user auth info from context
useContext will be used


import React, { useContext, useState, useEffect } from "react"
import { auth } from "../firebase"

const AuthContext = React.createContext()

export function useAuth() {
  return useContext(AuthContext)
}

export function AuthProvider({ children }) {
  const [currentUser, setCurrentUser] = useState()
  const [loading, setLoading] = useState(true)

  function signup(email, password) {
    return auth.createUserWithEmailAndPassword(email, password)
  }

  function login(email, password) {
    return auth.signInWithEmailAndPassword(email, password)
  }

  function logout() {
    return auth.signOut()
  }

  function resetPassword(email) {
    return auth.sendPasswordResetEmail(email)
  }

  function updateEmail(email) {
    return currentUser.updateEmail(email)
  }

  function updatePassword(password) {
    return currentUser.updatePassword(password)
  }

  useEffect(() => {
    const unsubscribe = auth.onAuthStateChanged(user => {
      setCurrentUser(user)
      setLoading(false)
    })

    return unsubscribe
  }, [])

  const value = {
    currentUser,
    login,
    signup,
    logout,
    resetPassword,
    updateEmail,
    updatePassword
  }

  return (
    <AuthContext.Provider value={value}>
      {!loading && children}
    </AuthContext.Provider>
  )
}Thus, we have done 5 things here :
Setup a state called user to store all the currently signed in information about the user
Setup functions that can be called to implement authentication features by interacting with Firebase's API. These functions help us setup :

Sign up
Login
Logout
Forgot Password
Update Email
Update Password


Next, we shared all this function and information with all app level components by setting up a Provider, a custom hook and React's context API.
Also,we have setup reauthentication to take place each time the users do something that change their authentication information like updating email ID, changing password etc.

Now, we can setup Private Routes to safeguard certain routes on our frontend appPrivate Routes
import React from "react"
import { Route, Redirect } from "react-router-dom"
import { useAuth } from "../contexts/AuthContext"

export default function PrivateRoute({ component: Component, ...rest }) {
  const { currentUser } = useAuth()

  return (
    <Route
      {...rest}
      render={props => {
        return currentUser ? <Component {...props} /> : <Redirect to="/login" />
      }}
    ></Route>
  )
}Private Route setup as a separate component to reuse it in several routes containing several components and pagesimport React from "react"
import { AuthProvider } from "../contexts/AuthContext"
import { BrowserRouter as Router, Switch, Route } from "react-router-dom"
import Dashboard from "./Dashboard"

function App() {
  return (
      <div className="w-100" style={{ maxWidth: "400px" }}>
        <Router>
          <AuthProvider>
            <Switch>
              <PrivateRoute exact path="/" component={Dashboard} />
            </Switch>
          </AuthProvider>
        </Router>
      </div>
  )
}

export default AppThe main App.js file is given the Provider and the PrivateRoute component to establish authentication logic accross the entire appThus, by simple implementation of features from React Router, we have simply just inherited the shared information (on current user's authentication) from our context by using our custom hook, and setup redirection in case the information is faulty Conclusion
This is how we can easily integrate authentication into our frontend React app using Firebase auth.

If you're more interested in this, I would highly recommend you to explore the Authorization and Role Based Access Control features that Firebase has to offer.
Also, check out this amazing YouTube tutorials to learn more about this !!
React Authentication Crash Course With Firebase And Routingüö® IMPORTANT:Learn React Today Course: https://courses.webdevsimplified.com/learn-react-todayAuthentication is crucial for nearly every application. It can a...YouTubeFirebase Auth TutorialShare your videos with friends, family, and the worldYouTube
React Query,A refresher on managing, monitoring and optimizing async API calls using the React Query library,React query is a really powerful library to work with asynchronous data in React. It is termed as the missing data-fetching library in ReactWhile most traditional state management libraries are amazing at client side state management,  the complex problems of the server state need to be dealt with.This is because server state is totally different. For starters, server state:Is persisted remotely in a location you do not control or ownRequires asynchronous APIs for fetching and updatingImplies shared ownership and can be changed by other people without your knowledgeCan potentially become "out of date" in your applications if you're not careful
Once you grasp the nature of server state in your application, even more challenges will arise as you go, for example:CachingDeduping multiple requests for the same data into a single requestUpdating out of date data in the backgroundKnowing when data is "out of date"Reflecting updates to data as quickly as possiblePerformance optimizations like pagination and lazy loading dataManaging memory and garbage collection of server stateMemoizing query results with structural sharingMuch of theory though, let's get rolling by creating a simple react component using react query to fetch some data from a server import { QueryClient, QueryClientProvider, useQuery } from 'react-query'
 
 const queryClient = new QueryClient()
 
 export default function ParentComponent() {
   return (
     <QueryClientProvider client={queryClient}>
       <ChildComponent />
     </QueryClientProvider>
   )
 }
 
const fetchData = async() => {
 const API_ENDPOINT ='https://api.github.com/repos/tannerlinsley/react-query'
 try{
       const res = await fetch(API_ENDPOINT)
       return res.json
 }catch(error => console.log(error))
}
 
 function ChildComponent() {
   const { isLoading, error, data } = useQuery('repoData', fetchData)
 
   if (isLoading) return 'Loading...'
 
   if (error) return 'An error has occurred: ' + error.message
 
   return (
     <div>
       <h1>{data.name}</h1>
       <p>{data.description}</p>
       <strong>üëÄ {data.subscribers_count}</strong>{' '}
       <strong>‚ú® {data.stargazers_count}</strong>{' '}
       <strong>üç¥ {data.forks_count}</strong>
     </div>
   )
 }
scrreact query simple applicationBasically we just need to do 3 things :Setup a query client : This is done at the parent component to ensure all the children can easily use the useQuery hook available and track and optimize their async API calls from the frontend.	Easily implemented in 2 steps: i) Setup a client using the QueryClient constructor import { QueryClient, QueryClientProvider, useQuery } from 'react-query'
 
 const queryClient = new QueryClient()ii) Pass it down as context to all child components using a QueryClientProvider export default function ParentComponent() {
   return (
     <QueryClientProvider client={queryClient}>
       <ChildComponent />
     </QueryClientProvider>
   )
 }2. Define a function that makes this callconst fetchData = async(key) => {
 const [type, greeting, page] = key.queryKey;
 console.log(greeting)
 const API_ENDPOINT =`https://swapi.dev/api/people/?page=${page}`
 try{
       const res = await fetch(API_ENDPOINT)
       return res.json
 }catch(error => console.log(error))
}2. useQuery hook : Use this react hook to fetch, configure and optimize everything about your async API calls.const [page, setPage] = useState(1)
    const { data, status } = useQuery(['people', 'hello friends', page], fetchPeople, {
        staleTime: 5000,
        // cacheTime: 10,
        onSuccess: () => console.log('data fetched with no problemo')
    })Basically in steps 2 and 3, we have also carefully passed parameters to the useQuery hook and used them in our API call function. This is yet another cool feature that the useQuery hook offersNow, combining it all in a ChildComponent we can show this on the UIimport React,{useState} from 'react'
import {useQuery} from 'react-query'
import Person from './Person'

const fetchPeople = async (key) =>
{
    const [type, greeting, page] = key.queryKey;
    console.log(greeting)
    const res = await fetch(`https://swapi.dev/api/people/?page=${page}`)
    return res.json()
}

const People = () =>
{
    const [page, setPage] = useState(1)
    const { data, status } = useQuery(['people', 'hello bois', page], fetchPeople, {
        staleTime: 5000,
        // cacheTime: 10,
        onSuccess: () => console.log('data fetched with no problemo')
    })
    switch(status)
        {
            case 'error':
            {
                return (<p>Error fetching from API</p>)
                break
            }
            case 'loading':
                {
                    return ( <p>Loading....</p> )
                    break
                }
            case 'success':
            {
                return (
                    <>
                    <button onClick ={()=>setPage(1)} >Page 1</button>
                    <button onClick ={()=>setPage(2)} >Page 2</button>
                    <button onClick ={()=>setPage(3)} >Page 3</button>
                    {data.results.map(person => (
                    <div className="container">
                        <h3>{person.name} </h3>
                        <p>Gender : {person.gender} </p>
                        <p>Birth year : {person.birth_year} </p>
                    </div>
                    ) )}
                    </>
                )
            }
        }
}

export default People
React Query Dev Tools
A collection of highly powerful and insightful tools that help visualize all of the inner workings of React Query and will likely save you hours of debugging if you find yourself in a pinch!
You can easily integrate them into your project with just 2 lines of code in the parent file !!
import {QueryClient, QueryClientProvider, useQuery} from 'react-query'
import Planets from "@components/Planets"
import People from '@components/People'
import {ReactQueryDevtools} from 'react-query-devtools'

const queryClient = new QueryClient()

export default function Home({page})
{
  return (
    <>
    <QueryClientProvider client={queryClient}>
    <div>
    {page == 'planets' ? <Planets/> : <People/> }
    </div>
    </QueryClientProvider>
    <ReactQueryDevtools/>
    </>
  )
}

Conclusion
This is how async API call management can be effectively handled from a React JS frontend. I hope you have learnt something of value and wish you the very best of luck on your developer journey.
Additional Resources
For better insight on this topic you can follow this amazing tutorial series (on a deprecated version of this package, but nevertheless brilliantly well-taught) :React Query TutorialShare your videos with friends, family, and the worldYouTube2. You are feel free to browse through the code of my integration of React Query in a Next JS frontendcrew-guy/react-queryA repo to study and understand using react query to handle, optimize and monitor async API calls from a React JS frontend - crew-guy/react-queryGitHubcrew-guy
Setting up Postgres,A guide on getting started with Database Management System with PostgreSQL,What is a database ?A database is simply a place that does these 3 things Store dataManipulate dataRetrieve dataThat's it, no more, no less. This data is usually stored on a computer server. You put data on the server and now can perform Create, Read, Update, Delete (CRUD) operations on it using the database's accompanying engine
What are tables ?Tables are like collections of entries in a database, where each collection specifies a particular set of entries.This is done to effectively manage separately defined data but integrate it all under a common roof for effective communication between the data types.Eg : Amazon.com, among other tables has separately defined tables for Users and Products. In this manner, although these tables are separately defined and managed they can communicate with each other (say, when a user from Users table places an order for a product in the Products table )
What is a relational database ?It is a database that will always have a specific set of properties, that need to be compulsarily defined for every entry stored in it.Eg : Amazon.com will not store a product in its relational database if important specs like the product price or details are not properly specified.This set of compulsary conditions that every entry stored in it is called a database's schema. The databases that do not adhere to such specific set of properties for each entry are called non-relational databases. Eg : MongoDB, Google Firebase, Apache Cassandra
What is SQLIn the most simple terms, it is a language that allows us to talk to the database, tell it to store, manipulate or retrieve our data. It is primarily used with relational databases.
What is Postgres ?Postgres is simply a database engine. It simply takes the SQL code that we have written and tells the database to perform whichever CRUD command we have specified.Now, lets get the ball rolling by installing Postgres and the GUI client on our system. We shall be choosing the GUI tool as follows :Postico => For MacOS userspgAdmin4 => For Windows and Linux usersFor windows
Simply download the postgreSQL .exe file from here and run it to get started
PostgreSQL: Windows installersInstall the GUI client => pgAdmin4, from this link and just run the .exe/.msi file to let Windows Installer handle the installation for youPostgreSQL: File BrowserFor Linux users
How To Install and Use PostgreSQL on Ubuntu 18.04 | DigitalOceanThis tutorial provides instructions on how to install the PostgreSQL database software on an Ubuntu 18.04 server. It also includes instructions for basic database management.DigitalOceanJustin Ellingwood## Installing postgres
sudo apt install postgresql postgresql-contrib
curl https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo apt-key add

## Installing the GUI client => pgAdmin4
# Create the repository configuration file:
sudo sh -c 'echo "deb https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/$(lsb_release -cs) pgadmin4 main" > /etc/apt/sources.list.d/pgadmin4.list && apt update'

#
# Install pgAdmin
#

# Install for both desktop and web modes:
sudo apt install pgadmin4

# Install for desktop mode only:
sudo apt install pgadmin4-desktop

# Install for web mode only: 
sudo apt install pgadmin4-web 

# Configure the webserver, if you installed pgadmin4-web:
sudo /usr/pgadmin4/bin/setup-web.shFor MacOS users
brew install postgresql

Install the GUI client : Postico, from this link and follow the setup instructions to install and configure it on your systemPostico ‚Äì a modern PostgreSQL client for the MacThings you have just installed




TYPE




Postgresql Server (psql)
SQL server => The database engine


pgAdmin4 / Postico
GUI => For click, drag and drop management as well as better visualization and analysis of SQL queries and the data they fetch


CLI
psql => CLI tool to handle entire database management from the command line



Creating your first database
PostgreSQL, by default, assigns postgres as a default user using which you can access your databaseIf you are a Linux user, you first need to enter the following command to switch over to the postgres account on your server. You can also create your own custom account but the postgres account is something you get by default on installing postgres.sudo -i -u postgresSimply this command to login as user and start interacting with Postgres' database enginepsql -U postgres
NOTE : You will be asked to set a password for the default postgres user the first time around. In case you forget it, you can refer to this link to get it sorted I forgot the password I entered during postgres installationI either forgot or mistyped (during the installation) the password to the default user of Postgres. I can‚Äôt seem to be able to run it and I get the following error: psql: FATAL: password authenti...Stack OverflowhilarlEnter the following command to create your first table :CREATE DATABASE my_first_db;Connecting to this database :This is done by typing the command specifying the following things :Database name : Any one of the created databasesUsername : Username to connect to database. For remote services like AWS, Heroku, Google Cloud Platform, that offer these databases as a managed service, they themselves supply all these details. By default, on your local machine, it is postgresHostname : The IP address which is used to connect the SQL server (i.e. the database engine) to your local machine. By default it is localhost i.e. http://127.0.0.1Port : The port on which this particular host is extending this database engine as a service. By default, this port is 5432 for Postgres\c my_first_db postgres localhost 5432Helpful commands :To get list of all available databases\d2. To delete a database : Exercise this command with EXTREME CAUTION as it will erase all your data DROP DATABASE your_database_nameOnce you have connected to your database you can insert your first table in it. I would recommend an online tool like Mockaroo to play with random sample data while you're still in the learning phaseCREATE TABLE people [
	person_id INT NOT NULL PRIMARY KEY,
    	person_name VARCHAR(100)
];Thus, we have created a table ie collection of a people and given a schema that each person should have an person_id : ¬†an id which is an integer, cannot be null and is a primary identifier of any entry on the tableperson_name : a string of maximum 100 charactersHelpful Commands To get list of available tables\dt
2. To get list of description of schema of a particular table\d peopleNow, lets populate this table with entriesINSERT INTO people (person_id, person_name)VALUES(1, 'John Doe');
INSERT INTO people (person_id, person_name)VALUES(2, 'Jen Doe');If you have a pre-written .sql file on your machine, that you wish to feed queries from into the table, use this command :\i /home/user_name/Desktop/person.sqlWhere the person.sql file could be basically everything we've done in this database, after connecting to it.CREATE TABLE people [
	person_id INT NOT NULL PRIMARY KEY,
    	person_name VARCHAR(100)
];

INSERT INTO people (person_id, person_name)VALUES(1, 'John Doe');
INSERT INTO people (person_id, person_name)VALUES(2, 'Jen Doe');
Finally, you can top this mini-tutorial by executing the simplest SQL query ever => to fetch all the entries from this table SELECT * FROM peopleConclusion
That's all for now. Welcome to the one of the most brilliant problems in computer and data science - database management. There's a lot to learn and know from here and trust me, this is one domain which requires a lot of braincrunching but feels very fulfilling.
I would highly recommend you to watch this amazing course on PostgreSQL for further digging in into the concept
Learn PostgreSQL Tutorial - Full Course for BeginnersLearn how to use PostgreSQL in this full course. PostgreSQL is a general purpose and object-relational database management system. It is the most advanced op...YouTubeI hope you learnt something from here and wish you the very best of luck on your developer journey ! üöÄ
Redux,An introduction to the state management with Redux and a mini project to further strenghten the concept.,Redux is an open-source JavaScript library for managing application state. Basically, different components of the UI have their state dependent on a central place, where, if you change data, you start seeing changes in the UI components.It is the most popular solution for state management in all JavaScript frontend applications whether they be in React, Angular, Vue, Svelte or just plain'ol Javascript
It works on the fundamentals of functional programming where usage of pure functions is done to prevent any mutation of data to Have a single source of truthPerform parallel reads/writes to the store for increased speedRedux dev tools => A chrome extension which offers amazing insights on state changes and the resulting UI updates in your application.
More on functional programming and object immutability can be found brilliantly here :Pure functions, immutability and other software superpowersPure functions are without a doubt the most important technique for writing software that I have learnt thus far in my career. This is because they help us simplify our code and make the effects of‚Ä¶DailyJSSimon SchwartzRedux Tutorial - Learn Redux from ScratchRedux tutorial - Learn redux from scratch. üöÄGet the full Redux course: https://codewithmosh.com/p/ultimate-reduxüëçSubscribe for more tutorials like this: ht...YouTubeThere are just 4 basic interacting concepts in Redux

STORE =>


A centralised place where all the reactive data is stored and updated.
A state in Redux simply refers to a snapshot of the store at a particular instance



ACTION =>


This is basically just an object describing what needs to be changed in the store based on some interaction of the user with the UI.
These are dispatched through a common line and given a payload and a type to tell the reducer or underlying middlewares (such as logging) what needs to be done.



REDUCER => This is just a function that defines the stores updates, based on the action object received




4 Steps to follow while designing a Redux store
Design the store ¬†‚áí Decide what you wanna keep in the storeDesign the actions ‚áí What are the actions that the user can perform in this appCreate one or more reducers ‚áí Take in action, update stateSet up the store ‚áí Update the store, based on the reducerMini ProjectHere, we will be creating a small bug tracking application to understand how a redux store is designed and implemented based on the requirements. actionTypes.js => This file will simply help you keep track of what actions are available in your store. Also, by storing actions options as variables instead of strings, we reduce the chance of spelling errors and mismatches while writing code later // ACTION TYPES
export const BUG_ADDED = "bugAdded"
export const BUG_REMOVED = "bugRemoved"
export const BUG_RESOLVED = "bugResolved"
a2. actionCreators.js => This file basically defines functions to create action objects. The functions are something that can be called from the UI ¬†interactions and the action objects created basically involve 2 main properties
type : One of the action types chosen from the options listed above
payload : Some data that needs to be passed along to help the functions processing in the background (reducer and middleware functions) to process the action

import * as actions from './actionTypes'

export const bugAdded = (desc) =>
{
    type: actions.BUG_ADDED,
    payload:{
        desc
    }
}

export const bugRemoved = (id) => ({
    type: actions.BUG_REMOVED,
    payload: {
        id:id
    }
})

export const bugResolved = (id =>
{
    type: actions.BUG_ADDED,
    payload:{
        id,
        desc:`Bug ${id} was resolvedd `
    }
})
2. reducer.js => This file defines what mutations must be performed on the store when an action of a certain type is dispatched.let lastId = 0;
const reducer = (state=[], action => {
	switch(action.type){
		case actions.BUG_ADDED:
            return [
                ...state,
                {
                    id: ++lastId,
                    desc: action.payload.desc,
                    resolved:false
                }
            ]
        case actions.BUG_REMOVED:
            return state.filter(state.id !== action.payload.id )
        case actions.BUG_RESOLVED:
            return state.map((bug) => 
                bug.id !== action.payload.id
                    ? bug
                    : {
                        ...bug,
                        desc: action.payload.desc,
                        resolved:true
                    }
            )
    
    }
} )
3. ¬†store.js => This file involves creation of a store that follows the norms set forth by the reducer and the action files.The window.__REDUX_DEVTOOLS_EXTENSION__ && window.__REDUX_DEVTOOLS_EXTENSION() parameter is just passed to the createStore method to sync it with the chrome extentsion - Redux Dev Tools
import { createStore } from 'redux'
import reducer from './reducer'

// Install the Google Chrome Extension -> redux dev tools
const createStore(reducer, window.__REDUX_DEVTOOLS_EXTENSION__ && window.__REDUX_DEVTOOLS_EXTENSION())

export default store
5. index.js => Just an arbitrary example of how any file can subscribe to the store updates and pass actions to it.import store from './store'
import { bugAdded, bugRemoved } from './actionCreators.js'

const unsubscribe = store.subscribe(() =>
{
    console.log('State was just changed !', store.getState())
})

// Adding a bug
store.dispatch(bugAdded('Bug 1 encountered'))

// Removing a bug
store.dispatch(bugRemoved(1))

// Resolving a specific bug
store.dispatch(bugResolved(1))

unsubscribe()
NOTE : Combining reducersSometimes you may wish for > 1 reducers to operate on the store. This is where you come in and combine them both into one and push it to the store.import 'reducer1' from '...'
import 'reducer2' from '...'

const allReducers1 = combineReducers({
	red1:reducer1,
	red2:reducer2
})
Conclusion
Congrats ! You have everything you need now to become a next level frontend engineer. Redux may be painfully verbose but it's undeniably the most powerful tool for state management in production level applications
Crawling, Indexing and Deindexing,A brief explanation about the fundamentals of SEO and how the search engine basically ranks webpages,CrawlingCrawling a site means following a path. A site crawler (aka spider) following your links and crawling around every inch of your websiteCrawlers can validate HTML code or hyperlinks. They can also go extract data from certain websites, which is called web scraping.When Google‚Äôs bots come to your website to crawl around, they follow other linked pages that are also on your site.The bots then use this information to provide up-to-date data to searchers about your pages. They also use it to create ranking algorithms.This is one of the reasons why sitemaps are so important. Sitemaps contain all of the links on your site so that Google‚Äôs bots can easily take a deeper look at your pages.Googlebot (Google‚Äôs search engine bot) has a ‚Äúcrawl budget.‚Äù It is made up of 2 parts ‚áí crawl rate limit and ¬†crawl demand

IndexingIndexing, refers to the process of adding certain web pages into the index of all pages that are searchable on Google.If a web page is indexed, Google will be able to crawl and index that page. Once you deindex a page, Google will no longer be able to index it.eg : By default, every WordPress post and page is indexed.It‚Äôs good to have relevant pages indexed because the exposure on Google can help you earn more clicks and bring in more traffic, which translates into more money and brand exposure.But, if you let parts of your blog or website that aren‚Äôt vital be indexed, you could be doing more harm than good.DeindexingThere are many different occasions where you may need (or want) to exclude a web page (or at least a portion of it) from search engine indexing and crawling.Pages you wanna deindexDUPLICATE PAGES : Basically pages containing duplicate contentDuplicate content refers to there being more than one version of one of your web pages. For example, one might be a printer-friendly version while the other is not.
THANK YOU PAGE : the page that visitors land on after taking a desired action such as downloading your software.Here is a set of guidelines to get deindexed really effectively.16 Ways to Get Deindexed by GoogleHas your website, or a client‚Äôs website, vanished from search results? It‚Äôs likely that you have been deindexed from Google. At least one of these 16 schemes is likely the cause.Search Engine JournalMaddy Osman
Atomic Design,An explainer on how atomic design redefines and restructures building large interfaces by testing UI components on the most elemental levels.,Atoms
Understanding Atoms in Front-end Development.The smallest indivisible particle of matter. Today, we all know that it's not like that, but we'll get there.Atoms is like our inputs, buttons, labels etc. And on their own are almost useless.Molecules
Understanding Molecules in Front-end Development.Group of Atoms. Following the previous example, joining our inputs (e-mail and password),labels ("e-mail" and "password") and button, we build a molecule-form.<formclass="molecule-form">
  <labelclass="atom-label">E-mail</label>
  <input type="email"class="atom-input">
  <labelclass="atom-label">Password</label>
  <input type="password"class="atom-input">
  <buttonclass="atom-button">Login</button>
</form>

Organisms

Understanding Organisms in Front-end Development.Group of molecules and possibly atoms. Seeing the code is much easier for this part:<headerclass="organic-header">
  <img src="" class="atom-logo"><ulclass="molecule-menu">
    <liclass="atom-item"></li>
    <liclass="atom-item"></li>
    <liclass="atom-item"></li>
  </ul>
</header>
And now I'll explain about Bosons, elements that doesn't exist in the Atomic Design Methodology, but was created by Suissa. Here things starts getting more interesting. ‚ò∫TemplatesAt the template stage, we break our chemistry analogy to get into language that makes more sense to our clients and our final output.Templates consist mostly of groups of organisms stitched together to form pages. It‚Äôs here where we start to see the design coming together and start seeing things like layout in action.Templates are very concrete and provide context to all these relatively abstract molecules and organisms. Templates are also where clients start seeing the final design in place.Pages
Pages are specific instances of templates ‚áí accurate depiction of what a user will ultimately see.Pages are the highest level of fidelity and because they‚Äôre the most tangible, it‚Äôs typically where most people in the process spend most of their time and what most reviews revolve around.The page stage is essential as it‚Äôs where we test the effectiveness of the design system. Viewing everything in context allows us to loop back to modify our molecules, organisms, and templates to better address the real context of the design.Pages are also the place to test variations in templates. For example, you might want to articulate what a headline containing 40 characters looks like, but also demonstrate what 340 characters looks like.BosonsYou already have heard about Bosons, right? It is smaller than Atoms. Quantum physics explains how particles gain mass, by Higgs Field. So the boson are particles that gives mass to others. Which we can use as placeholders in Sass. Looking at the photo below, we can see that %boson-button and the %bosson-button-success are giving mass to .atom-button.Understanding Bosons in Front-end Development.The greater is the interaction between the particle and the Higgs Field, the more mass it gets. Remembering the placeholders doesn't exist until you @extend them.In the above example we notice that .atom-button has a interaction with bosons. And in the end, it'll be:.atom-button {
  padding: 6px 12px;
  display: inline-block;
  border: 1px solid transparent;
  border-radius: 3px;
  background: green;
  border-color: dark-green;
  color: white;
}
After the interaction with bosons, our atom gained mass.I separeted in boson-button (structure) and boson-button-success (UI). So, whenever I want to build a button, I just @extend my bosons..atom-button-save {
  @extend %boson-button;
  @extend %boson-button-success;
}.atom-button-cancel {
  @extend %boson-button;
  @extend %boson-button-danger;
}
Why atomic design ?Atomic design gives us the ability to traverse from abstract to concrete.Because of this, we can create systems that promote consistency and scalability while simultaneously showing things in their final context. And by assembling rather than deconstructing, we‚Äôre crafting a system right out of the gate instead of cherry picking patterns after the fact.
Component Driven Development,An overview of component driven development and atomic design principles and how they are going to craft the future of frontend development,Component-Driven Development (CDD) is a development methodology that anchors the build process around components.It is a process that builds UIs from the ‚Äúbottom up‚Äù by starting at the level of components and ending at the level of pages or screens.
BenefitsSo what is the big deal about working one component at a time in a fixed series of states? Let me count the ways:Focus development: Working on a single component by manipulating an entire app into a certain state is painful and laborious. Certain states can be difficult or impossible to achieve within the full app context in development (think certain loading or error states).Increase UI coverage: Enumerating all relevant states means you can be confident you‚Äôve not missed anything and the component works in all possible scenarios.Target feedback: Looking it up in an explorer is a much easier way for a colleague to review a new or changed component; Focusing on one component at a time allows communication (especially between design and development) to happen with much higher precision.Build a component library: Supercharge component reuse within your app and organization.Parallelize development: Working one component at a time allows you to share tasks between different team members in a way that is just not possible at the level of ‚Äúscreens‚Äù.Test visually: Component explorers allow for a class of ‚Äúvisual‚Äù tests, analogous to traditional automated tests, in an area (UIs) that has often defied automated testing. In particular, they allow a form of ‚ÄúVisual TDD‚Äù that has the same benefits as TDD, but in the UI arena.Our ideas are faulty, we have bugs~ Fred BrooksSince ideas are prone to bugs, apps must be easily adaptable to accommodate new learning. Dissecting UIs into interchangeable components enables rapid reconfiguration of the pieces as business needs change.The component model encourages interchangeability by isolating component state from business logic. A component explorer makes this separation evident by providing a sandbox to develop the UI component in isolation of the app.The model also allows for parallel production where a team of people can work on different pieces of the UI simultaneously without distraction or state pollution from other parts of the app.Your team gets things done fasterProcessBuild components in isolationRegister the component with the explorerSpecify state and stub data from within your component‚Äôs directory (e.g., a developer might use a factory to generate randomized data for your exampleListItem component or hardcode states they specifically want to test)Test frontend behavior by specifying ‚Äústates‚Äù a user can find themselves in. A progreHow to be Component DrivenBuild one component at a timeBuild each component in isolation and define its relevant states. Start small.
Combine componentsCompose small components together to unlock new features while gradually increasing complexity.
Assemble pagesBuild pages by combining composite components. Use mock data to simulate pages in hard-to-reach states and edge cases.
Integrate pages into your projectAdd pages to your app by connecting data and hooking up business logic. This is when your UI meets your backend APIs and services.
Storybooks
Storybook is a tool for CDD UI development.
It makes development faster and easier by isolating components. This allows you to work on one component at a time.
You can develop entire UIs without needing to start up a complex dev stack, force certain data into your database, or navigate around your application.

Introduction to StorybookStorybook is an open source tool for developing UI components in isolation for React, Vue, and AngularStorybook
Bidirectional API with gRPC,Creating a staggered, bidirectional, secure connection between client and server using the gRPC framework,It is really easy to setup language agnostic open, staggered client-server interactions in a bidirectional, streaming way using the gRPC frameworkThis is done due to the fact that gRPC runs on HTTP2, so resource multiplexing comes into the picture and helps client and server both keep pushing data and listen and respond accordinglyResources
I have committed all the protofiles in detailed branches at this github repository for your perusal
crew-guy/gRPCA repository that explains setting up modern, fast, language agnostic APIs using the gRPC framework - crew-guy/gRPCGitHubcrew-guyGitHub repo linkThis is the file structure that I have followed while designing this API.




root_folder/
|-- client/
|   |-- client.js
|-- node_modules  
|-- protos/
|   |-- max.proto
|-- server/
     |-- index.js
     |-- protos/
        |-- max_pb.js
        |-- max_grpc_pb.js
0. Sample Problem
In this exercise, we will be building an API to return the maximum of numbers streamed from client  (client streaming) every couple of seconds (server streaming)

The function takes a stream of Request messages that have one integer each, and returns a stream of Responses that represents the maximum of all the nummbers streamed to it upto that point.

This interaction is :

Bidirectional - as client and server both keep streaming data to each other.
Staggered - as client and server streams have different speeds

1. Writing the protofile
This is pretty basic and will involve




TYPE
HTML




gRPC Request
STREAM OF INTEGERS
Simple message object


gRPC Response
STREAM INTEGER
Simple message object


Service
FUNCTION
rpc CalciMax(stream MaxRequest) returns (stream MaxResponse ){}



syntax="proto3";

package max;

service MaximumService{
    rpc CalciMax(stream MaxRequest) returns (stream MaxResponse ){}
}

message MaxResponse{
    int32 result=1;
}

message MaxRequest{
    int32 num=1;
}Here, the stream keyword next to AvgRequest and AvgResponse is of key importance as it tells gRPC  to generate code accordingly such that the function called from the stream of client request will return stream of responses from server.2. Generate the protofilesPackages required : protoc, grpc-tools1. grpc-toolsnpm i grpc-tools
We get MaxServiceService and MaxServiceClient generated from the "grpc-tools" module that will act on our proto to create the client and service (server)Output file ‚áí PACKAGENAME_grpc_pb.js2. protocWhereas, the protoc will simply generate the Request and Response objects and give us getters and setters on themOutput filename ‚áí PACKAGENAME_pb.jssudo protoc -I=. ./protos/max.proto \                                        
  --js_out=import_style=commonjs,binary:./server \  
  --grpc_out=./server \
  --plugin=protoc-gen-grpc=`which grpc_tools_node_protoc_plugin`
Here the inside the main project directory, we have a client, server and protos subdirectory. Currently, we are generating the API from proto file in the protos directory into the protos subdirectory inside the server directorySIDE NOTE
I shall be using the chalk library here to setup beautiful text outputs in our console so we can clearly see how each request and response are communicated
npm i chalk3. Setup a gRPC servernpm i grpc google-protobuf
Import the necessary dependencies. Here,max_pb ‚áí ¬†Contains all the details about the request and response object.max_grpc_pb ‚áí ¬†Will help the grpc server create an API from the proto generated code.const grpc = require("grpc")
const max = require('../server/protos/max_pb')
const maxService = require('../server/protos/max_grpc_pb')

Importing the required packages and generated proto files// HELPER FUNCTION
const sleep = async (interval) =>
{
    return new Promise(resolve =>
    {
        setTimeout(()=>resolve(),interval)
    })
}

// MAIN FUNCTION
const calciMax = async(call, callback) =>
{
    const noOfTimesYouWannaComputeMaximum = 15
    const numArray = []
    call.on('data', request =>
    {
        const num = request.getNum()
        console.log(chalk.green(`Client just pushed this number to me : ${num}`))
        numArray.push(num)
    })

    call.on('status', status => console.log(chalk.magenta(status)))

    call.on('end', ()=>{console.log(chalk.cyanBright('Client has stopped streaming from the server'))})
    

    for (let i = 0; i < noOfTimesYouWannaComputeMaximum; i++)
    {
        const maxResponse = new max.MaxResponse()
        const maximum = _.max(numArray)
        console.log(maximum)
        maxResponse.setResult(maximum)
        call.write(maxResponse)
        await sleep(3000)
    }
    call.end()
}Function that processes request to return response¬†Define the function that will handle the request and return a response. This function is what we defined in the proto file and this is also the function that will be callled from the clientHere the call.on() methods allow us to basically monitor the stream sent from the client for proper response and error handling. Also, by setting up different sleep intervals  for client and server we can establish a staggered communication between them.4. Setup the Client SideImport the necessary dependencies.max_grpc_pb ‚áí Contains proto generated code that will help us setup the client and link it to the server functions we have definedmax_pb ‚áí Contains code to help us access and work with the request objectconst grpc = require('grpc');
const max = require('../server/protos/max_pb')
const maxService = require('../server/protos/max_grpc_pb')Importing the necessary packages and generated filesSetup a client using the proto generated codeconst URL_ENDPOINT = `localhost:50051` 

//* Client setup
    const maxClient = new maxService.MaximumServiceClient(
        URL_ENDPOINT,
        grpc.credentials.createInsecure()
    )Setup the client from the service hosted at the URL_ENDPOINT//* Setup the response handling
    const maxReq = new max.MaxRequest()
    const call = maxClient.calciMax(maxReq, () => { })
    call.on('data', (response) =>
    {
        const max = response.getResult()
        console.log(chalk.yellow(`Server says that uptil now, this is the max num : ${max}`))
    })
    
    call.on('error', err => console.log(chalk.yellow(err)))
    call.on('status', status => console.log(chalk.magenta(status)))
    call.on('end', ()=>{console.log(chalk.cyanBright('Server has stopped streaming from the client'))})
Handling the responseconst numArray = [1, 5, 3, 6, 2, 20, 1,23, 54, 78, 34, 545,23,54, 4,544,5,23,12,12,,35,4,55,6345214,11231231]
    for (let i = 0; i < numArray.length; i++)
    {
        maxReq.setNum(numArray[i])
        call.write(maxReq)
        await sleep(1000)
    }
    call.end()Sending the client requests in a streamAdd service to server and start it
//* Setup a server
const server = new grpc.Server()

server.addService(maxService.MaximumServiceService, { calciMax })

//? START THE SERVER
const URL_ENDPOINT = "127.0.0.1:50051"
server.bind(URL_ENDPOINT, grpc.ServerCredentials.createInsecure())
server.start()

console.log(chalk.green(`Server running on ${URL_ENDPOINT}`))
 Conclusion 

 Congratulations ! You are now equipped with the futuristic skills of bidirectional streaming type API designing. Thanks a lot for reading this blog. If you like more content like this, subscribe to my mailing list
Client Push API with gRPC,Understanding the client push API setup using gRPC,gRPC allows a one-word implementation to write an API where a client sends a continuous stream of data to the serverThis is done due to the fact that gRPC runs on HTTP2, so resource multiplexing comes into the picture and helps client keep pushing data while the server listens for it and responds as per requirement.Resources
I have committed all the protofiles in detailed branches at this github repository for your perusal
crew-guy/gRPCA repository that explains setting up modern, fast, language agnostic APIs using the gRPC framework - crew-guy/gRPCGitHubcrew-guyThis is the file structure that I have followed while designing this API.




root_folder/
|-- client/
|   |-- client.js
|-- node_modules  
|-- protos/
|   |-- avg.proto
|-- server/
     |-- index.js
     |-- protos/
        |-- avg_pb.js
        |-- avg_grpc_pb.js
0. Sample Problem
In this exercise, we will be building a Calculate the Average of streamed numbers using client streaming

The function takes a stream of Request messages that have one integer each, and returns a single Response that represents the average of all the nummbers streamed to it

1. Writing the protofile
This is pretty basic and will involve




TYPE
HTML




gRPC Request
STREAM OF INTEGERS
Simple message object


gRPC Response
INTEGER
Simple message object


Service
FUNCTION
rpc ComputeAvg(stream AvgRequest) returns (AvgResponse){}



syntax="proto3";

package avg;

service AvgService{
    rpc ComputeAvg(stream AvgRequest) returns (AvgResponse){}
}

message AvgRequest{
    int32 num = 1;
}

message AvgResponse{
    double average = 1;
}
Here, the stream keyword next to AvgRequest is of key importance as it tells gRPC  to generate code accordingly such that the function called from the stream of client request will return single response.2. Generate the protofilesPackages required : protoc, grpc-tools1. grpc-toolsnpm i grpc-tools
We get AvgServiceService and AvgServiceClient generated from the "grpc-tools" module that will act on our proto to create the client and service (server)Output file ‚áí PACKAGENAME_grpc_pb.js2. protocWhereas, the protoc will simply generate the Request and Response objects and give us getters and setters on themOutput filename ‚áí PACKAGENAME_pb.jssudo protoc -I=. ./protos/avg.proto \                                        
  --js_out=import_style=commonjs,binary:./server \  
  --grpc_out=./server \
  --plugin=protoc-gen-grpc=`which grpc_tools_node_protoc_plugin`
Here the inside the main project directory, we have a client, server and protos subdirectory. Currently, we are generating the API from proto file in the protos directory into the protos subdirectory inside the server directory3. Setup a gRPC servernpm i grpc google-protobuf
Import the necessary dependencies. Here,avg_pb ‚áí ¬†Contains all the details about the request and response object.avg_grpc_pb ‚áí ¬†Will help the grpc server create an API from the proto generated code.const grpc = require("grpc")
const avg = require('./protos/avg_pb')
const avgService = require('./protos/avg_grpc_pb')

Importing the required packages and generated proto filesconst computeAvg = (call, callback) =>
{
    const numArray = []
    call.on('error', (error =>
    {
        console.log(error)
    }))

    call.on('end', () =>
    {
        console.log('Average has been computed !')
        const result = new avg.AvgResponse()
        const calc = (numArray.reduce((i, acc)=> i + acc, 0))/numArray.length
        result.setAverage(calc)

        console.log(chalk.blue.bold(`Average : ${result.getAverage()}`))
    })

    call.on('data', request =>
    {
        const num = request.getNum()
        console.log(chalk.blueBright(num))
        numArray.push(num)
    })
}Function that processes request to return response¬†Define the function that will handle the request and return a response. This function is what we defined in the proto file and this is also the function that will be callled from the clientHere the call.on() methods allow us to basically monitor the stream sent from the client for proper response and error handling. 4. Setup the Client SideImport the necessary dependencies.avg_grpc_pb ‚áí Contains proto generated code that will help us setup the client and link it to the server functions we have definedavg_pb ‚áí Contains code to help us access and work with the request objectconst grpc = require('grpc');
const avg = require('../server/protos/avg_pb')
const avgService = require('../server/protos/avg_grpc_pb')
Importing the necessary packages and generated filesSetup a client using the proto generated codeconst URL_ENDPOINT = `localhost:50051` 

const avgClient = new avgService.AvgServiceClient(
    URL_ENDPOINT,
    grpc.credentials.createInsecure()
)Setup the client from the service hosted at the URL_ENDPOINTconst avgReq = new avg.AvgRequest()
const call = avgClient.computeAvg(avgReq, (error, response) =>
{
    if (!error)
    {
        console.log(`Responded average is ${response}`)
    } else
    {
        console.log(error)
    }
})
Handling the responseconst numArray = [1,2,3,4,5,6,7,8,9,10]
let count = 0, intervalID = setInterval(() =>
{
    console.log(chalk.magentaBright(`Sending request num ${count + 1}`))
    avgReq.setNum(numArray[count])
    call.write(avgReq)
    if (++count == numArray.length)
    {
        clearInterval(intervalID)
        call.end()
    }
},1000)Sending the client requests in a streamAdd service to server and start it
//* Setup a server
const server = new grpc.Server()

server.addService(avgService.AvgServiceService, { computeAvg })

//? START THE SERVER
const URL_ENDPOINT = "127.0.0.1:50051"
server.bind(URL_ENDPOINT, grpc.ServerCredentials.createInsecure())
server.start()

console.log(chalk.green(`Server running on ${URL_ENDPOINT}`))
 Conclusion 

 Congratulations ! You are now equipped with the futuristic skills of client-push-server-stream type API designing. Thanks a lot for reading this blog. If you like more content like this, subscribe to my mailing list
Server Push API with gRPC,Using gRPC to create Publish Subscribe model involving server-push-client-listen,gRPC allows a one-word implementation to write an API where a client requests for data once but the server returns a stream of data

This is done due to the fact that gRPC runs on HTTP2, so resource multiplexing comes into the picture and helps server keep pushing data while the client listens for itResources
I have committed all the protofiles in detailed branches at this github repository for your perusal
crew-guy/gRPCA repository that explains setting up modern, fast, language agnostic APIs using the gRPC framework - crew-guy/gRPCGitHubcrew-guyGitHub repo linkThis is the file structure that I have followed while designing this API.




root_folder/
|-- client/
|   |-- client.js
|-- node_modules  
|-- protos/
|   |-- primenum.proto
|-- server/
     |-- index.js
     |-- protos/
        |-- primenum_pb.js
        |-- primenum_grpc_pb.js
0. Sample Problem
In this exercise, we will be building a Prime Number Decomposition API using server streaming

The function takes a Request message that has one integer, and returns a stream of Responses that represent the prime number decomposition of that number

Pseudo code
k = 2
N = 210
while N > 1:
    if N % k == 0:   // if k evenly divides into N
        print k      // this is a factor
        N = N / k    // divide N by k so that we have the rest of the number left.
    else:
        k = k + 11. Writing the protofile
This is pretty basic and will involve




TYPE
HTML




gRPC Request
INTEGER
Simple message object


gRPC Response
STREAM OF INTEGERS
Simple message object


Service
FUNCTION
rpc  CalcPn(PnRequest) returns (stream  PnResponse){}



syntax="proto3";

package primenum;

service PnService {
    rpc CalcPn(PnRequest) returns (stream PnResponse){}
}

message PnRequest{
    int32 pn = 1;
}

message PnResponse{
    int32 result = 1;
}Here, the stream keyword next to PnResponse is of key importance as it tells gRPC  to generate code accordingly such that the function called from the client will return a stream of responses2. Generate the protofilesPackages required : protoc, grpc-tools1. grpc-toolsnpm i grpc-tools
We get PnServiceService and PnServiceClient generated from the "grpc-tools" module that will act on our proto to create the client and service (server)Output file ‚áí PACKAGENAME_grpc_pb.js2. protocWhereas, the protoc will simply generate the Request and Response objects and give us getters and setters on themOutput filename ‚áí PACKAGENAME_pb.jssudo protoc -I=. ./protos/primenum.proto \                                        
  --js_out=import_style=commonjs,binary:./server \  
  --grpc_out=./server \
  --plugin=protoc-gen-grpc=`which grpc_tools_node_protoc_plugin`
Here the inside the main project directory, we have a client, server and protos subdirectory. Currently, we are generating the API from proto file in the protos directory into the protos subdirectory inside the server directory3. Setup a gRPC servernpm i grpc google-protobuf
Import the necessary dependencies. Here,primenum_pb ‚áí ¬†Contains all the details about the request and response object primenum_grpc_pb ‚áí ¬†Will help the grpc server create an API from the proto generated code.const grpc = require("grpc")
const pn = require('./protos/primenum_pb')
const pnService = require('./protos/primenum_grpc_pb')
const grpc = require("grpc")
const chalk = require('chalk')


// Define the function (camelcased version of the function described in the protofile) that will do the calculation

const calcPn = (call, callback) =>
{
    let pnReq = call.request.getPn()
    console.log(chalk.greenBright(pnReq))
    let pnTemp = pnReq
    for(let i = 2; i < pnReq; i++)
    {
        while (pnTemp % i === 0)
        {
            pnTemp = pnTemp / i
            const pnRes = new pn.PnResponse()
            pnRes.setResult(i)
            call.write(pnRes)
        }
    }
    call.end()
}

// Add this service request to the server

server.addService(pnService.PnServiceService, { calcPn })

// START THE SERVER
const URL_ENDPOINT = "127.0.0.1:50051"
server.bind(URL_ENDPOINT, grpc.ServerCredentials.createInsecure())
server.start()

console.log(chalk.yellow(`Server running on ${URL_ENDPOINT}`))


Define the function that will handle the request and return a response. This function is what we defined in the proto file and this is also the function that will be callled from the client4. Setup the Client SideImport the necessary dependencies.primenum_grpc_pb ‚áí Contains proto generated code that will help us setup the client and link it to the server functions we have definedprimenum_pb ‚áí Contains code to help us access and work with the request objectconst grpc = require('grpc');
const pn = require('../server/protos/primenum_pb')
const pnService = require('../server/protos/primenum_grpc_pb')
Setup a client using the proto generated codeconst pnServiceClient = new pnService.PnServiceClient(
    URL_ENDPOINT,
    grpc.credentials.createInsecure()
)
Configure and send the request objectconst pnServiceClient = new pnService.PnServiceClient(
    URL_ENDPOINT,
    grpc.credentials.createInsecure()
)

const pnServiceReq = new pn.PnRequest()
pnServiceReq.setPn(20)
const call = pnServiceClient.calcPn(pnServiceReq, () => {})
Handle the response received on the client sidecall.on('status', (status) =>
{
    console.log(status)
})

call.on('data', (response) =>
{
    console.log(`Factor : ${response}`)
})

call.on('error', (error) =>
{
    console.log(error)
})

call.on('end', () =>
{
    console.log('Streaming has ended ........')
})
Add service to server and start it
//* Setup a server
const server = new grpc.Server()

 server.addService(pnService.PnServiceService, { calcPn })

//? START THE SERVER
const URL_ENDPOINT = "127.0.0.1:50051"
server.bind(URL_ENDPOINT, grpc.ServerCredentials.createInsecure())
server.start()

console.log(chalk.green(`Server running on ${URL_ENDPOINT}`))
 Conclusion 

 Congratulations ! You are now equipped with the futuristic skills of server-push-client-stream type API designing. Thanks a lot for reading this blog. If you like more content like this, subscribe to my mailing list
Less CSS,Using a smarter approach to structuring and writing CSS,OverviewLess (which stands for Leaner Style Sheets) is a backwards-compatible language extension for CSS. This is the official documentation for Less, the language and Less.js, the JavaScript tool that converts your Less styles to CSS styles.Because Less looks just like CSS, learning it is a breeze. Less only makes a few convenient additions to the CSS language, which is one of the reasons it can be learned so quickly.For detailed documentation on Less language features, see FeaturesFor a list of Less Built-in functions, see FunctionsFor detailed usage instructions, see Using Less.jsFor third-party tools for Less, see ToolsWhat does Less add to CSS? Here's a quick overview of features.VariablesThese are pretty self-explanatory:@width: 10px;
@height: @width + 10px;

#header {
  width: @width;
  height: @height;
}

Outputs:#header {
  width: 10px;
  height: 20px;
}

MixinsMixins are a way of including ("mixing in") a bunch of properties from one rule-set into another rule-set. So say we have the following class:.bordered {
  border-top: dotted 1px black;
  border-bottom: solid 2px black;
}

And we want to use these properties inside other rule-sets. Well, we just have to drop in the name of the class where we want the properties, like so:#menu a {
  color: #111;
  .bordered();
}

.post a {
  color: red;
  .bordered();
}

The properties of the .bordered class will now appear in both #menu a and .post a. (Note that you can also use #ids as mixins.)NestingLess gives you the ability to use nesting instead of, or in combination with cascading. Let's say we have the following CSS:#header {
  color: black;
}
#header .navigation {
  font-size: 12px;
}
#header .logo {
  width: 300px;
}

In Less, we can also write it this way:#header {
  color: black;
  .navigation {
    font-size: 12px;
  }
  .logo {
    width: 300px;
  }
}

The resulting code is more concise, and mimics the structure of your HTML.You can also bundle pseudo-selectors with your mixins using this method. Here's the classic clearfix hack, rewritten as a mixin (& represents the current selector parent):.clearfix {
  display: block;
  zoom: 1;

  &:after {
    content: " ";
    display: block;
    font-size: 0;
    height: 0;
    clear: both;
    visibility: hidden;
  }
}

Nested At-Rules and BubblingAt-rules such as @media or @supports can be nested in the same way as selectors. The at-rule is placed on top and relative order against other elements inside the same ruleset remains unchanged. This is called bubbling..component {
  width: 300px;
  @media (min-width: 768px) {
    width: 600px;
    @media  (min-resolution: 192dpi) {
      background-image: url(/img/retina2x.png);
    }
  }
  @media (min-width: 1280px) {
    width: 800px;
  }
}

outputs:.component {
  width: 300px;
}
@media (min-width: 768px) {
  .component {
    width: 600px;
  }
}
@media (min-width: 768px) and (min-resolution: 192dpi) {
  .component {
    background-image: url(/img/retina2x.png);
  }
}
@media (min-width: 1280px) {
  .component {
    width: 800px;
  }
}

OperationsArithmetical operations +, -, *, / can operate on any number, color or variable. If it is possible, mathematical operations take units into account and convert numbers before adding, subtracting or comparing them. The result has leftmost explicitly stated unit type. If the conversion is impossible or not meaningful, units are ignored. Example of impossible conversion: px to cm or rad to %.// numbers are converted into the same units@conversion-1: 5cm + 10mm;// result is 6cm@conversion-2: 2 - 3cm - 5mm;// result is -1.5cm// conversion is impossible@incompatible-units: 2 + 5px - 3cm;// result is 4px// example with variables@base: 5%;
@filler: @base * 2;// result is 10%@other: @base + @filler;// result is 15%
Multiplication and division do not convert numbers. It would not be meaningful in most cases - a length multiplied by a length gives an area and css does not support specifying areas. Less will operate on numbers as they are and assign explicitly stated unit type to the result.@base: 2cm * 3mm;// result is 6cm
You can also do arithmetic on colors:@color: #224488 / 2;//results in #112244background-color: #112244 + #111;// result is #223355
However, you may find Less's Color Functions more useful.calc() exceptionFor CSS compatibility, calc() does not evaluate math expressions, but will evaluate variables and math in nested functions.@var: 50vh/2;
width: calc(50% + (@var - 20px));// result is calc(50% + (25vh - 20px))
EscapingEscaping allows you to use any arbitrary string as property or variable value. Anything inside ~"anything" or ~'anything' is used as is with no changes except interpolation.@min768: ~"(min-width: 768px)";
.element {
  @media @min768 {
    font-size: 1.2rem;
  }
}

results in:@media (min-width: 768px) {
  .element {
    font-size: 1.2rem;
  }
}

Note, as of Less 3.5, you can simply write:@min768: (min-width: 768px);
.element {
  @media @min768 {
    font-size: 1.2rem;
  }
}

In 3.5+, many cases previously requiring "quote-escaping" are not needed.FunctionsLess provides a variety of functions which transform colors, manipulate strings and do maths. They are documented fully in the function reference.Using them is pretty straightforward. The following example uses percentage to convert 0.5 to 50%, increases the saturation of a base color by 5% and then sets the background color to one that is lightened by 25% and spun by 8 degrees:@base: #f04615;
@width: 0.5;

.class {
  width: percentage(@width);// returns `50%`color: saturate(@base, 5%);
  background-color: spin(lighten(@base, 25%), 8);
}

See: Function ReferenceNamespaces and Accessors(Not to be confused with CSS @namespace or namespace selectors).Sometimes, you may want to group your mixins, for organizational purposes, or just to offer some encapsulation. You can do this pretty intuitively in Less. Say you want to bundle some mixins and variables under #bundle, for later reuse or distributing:#bundle() {
  .button {
    display: block;
    border: 1px solid black;
    background-color: grey;
    &:hover {
      background-color: white;
    }
  }
  .tab { ... }
  .citation { ... }
}

Now if we want to mixin the .button class in our #header a, we can do:#header a {
  color: orange;
  #bundle.button();// can also be written as #bundle > .button
}

Note: append () to your namespace (e.g. #bundle()) if you don't want it to appear in your CSS output i.e. #bundle .tab.MapsAs of Less 3.5, you can also use mixins and rulesets as maps of values.#colors() {
  primary: blue;
  secondary: green;
}

.button {
  color: #colors[primary];
  border: 1px solid #colors[secondary];
}

This outputs, as expected:.button {
  color: blue;
  border: 1px solid green;
}

See also: MapsScopeScope in Less is very similar to that of CSS. Variables and mixins are first looked for locally, and if they aren't found, it's inherited from the "parent" scope.@var: red;

#page {
  @var: white;
  #header {
    color: @var;// white
  }
}

Like CSS custom properties, mixin and variable definitions do not have to be placed before a line where they are referenced. So the following Less code is identical to the previous example:@var: red;

#page {
  #header {
    color: @var;// white
  }
  @var: white;
}

See also: Lazy LoadingCommentsBoth block-style and inline comments may be used:/* One heck of a block
 * style comment! */@var: red;

// Get in line!@var: white;

ImportingImporting works pretty much as expected. You can import a .less file, and all the variables in it will be available. The extension is optionally specified for .less files.@import "library";// library.less@import "typo.css";
Unary API with gRPC,A fresh introduction to the modern gRPC framework in API designing,What is gRPC ?

 gRPC stands for google Remote Procedure Call and it is the technology that will invetably lead the future of API designing and communication

What does RPC mean

RPC stands for  Remote Procedure Call  and it simply defines the *procedure for calling a function on the  server  from the  client .

Why should you care

Well, it's basically 5 things that gRPC offers out of the box

 Speed => as it is transmitted through multiplexing/ over HTTP2 ([Link to blog](http://))
 Security  => As HTTP2 compulsarily implements SSL
 Developer Ease  => gRPC is language agnostic so you could literally talk to a python server from a react client, have a C++ proxy in the middle and handle yor database in Java without the heavy lifting of serializing communication objects


 There are several more of such features that gRPC hands out of the box for API managementResources
I have committed all the protofiles in detailed branches at this github repository for your perusal
crew-guy/gRPCA repository that explains setting up modern, fast, language agnostic APIs using the gRPC framework - crew-guy/gRPCGitHubcrew-guyThis is the file structure that I have followed while designing this API.




root_folder/
|-- client/
|   |-- client.js
|-- node_modules  
|-- protos/
|   |-- max.proto
|-- server/
     |-- index.js
     |-- protos/
        |-- max_pb.js
        |-- max_grpc_pb.js
0. Sample Problem
In this exercise, we will be building a Calculate the sum of numbers sent from client to server.

The function takes a Request message that has 2 integers, and returns a single Response that represents the sum of these numbers.

1. Make the protofileThis just involves creation of 3 thingsRequestResponseService (The function that will take in request and return a response)syntax="proto3";

package PACKAGENAME;

message SumMessage{
    int32 first_num = 1;
    int32 second_num = 2;
}

message SumRequest{
    SumMessage sum_message = 1; 
}

message SumResponse{
    int32 result = 1;
}

service SumService{
    rpc Sum(SumRequest) returns (SumResponse){}
}
Here, SumRequest and SumResponse are the request response objects and SumMessage just helps describe the request object2. Generate the protofilesPackages required : protoc, grpc-tools1. grpc-toolsnpm i grpc-tools
We get SumServiceService and SumServiceClient generated from the "grpc-tools" module that will act on our proto to create the client and service (server)Output file ‚áí PACKAGENAME_grpc_pb.js2. protocWhereas, the protoc will simply generate the Request and Response objects and give us getters and setters on themOutput filename ‚áí PACKAGENAME_pb.jssudo protoc -I=. ./protos/sum.proto \                                        
  --js_out=import_style=commonjs,binary:./server \  
  --grpc_out=./server \
  --plugin=protoc-gen-grpc=`which grpc_tools_node_protoc_plugin`
Here the inside the main project directory, we have a client, server and protos subdirectory. Currently, we are generating the API from proto file in the protos directory into the protos subdirectory inside the server directory3. Setup a gRPC servernpm i grpc google-protobuf
Import the necessary dependencies. Here,sum_pb ‚áí ¬†Contains all the details about the request and response object whereas thesum_grpc_pb ‚áí ¬†Will help the grpc server create an API from the proto generated code.const grpc = require("grpc")
const sumService = require('./protos/sum_grpc_pb')
const sums = require('./protos/sum_pb')
Define the function that will handle the request and return a response. This function is what we defined in the proto file and this is also the function that will be callled from the clientconst sum = (call, callback) =>

// The name is chosen as "sum" as in the proto we have said, in PascalCasing that the "Sum" function will take in a request and return a response

{
    const summing = new sums.SumResponse()
    summing.setResult(
					call.request.getSumMessage().getFirstNum() 
				+ call.request.getSumMessage().getSecondNum()
		)		
    callback(null,summing)
}
Start up the gRPC server and link it to our function and proto generated codeconst server = new grpc.Server()
 
server.addService(sumService.SumServiceService,{sum})
const url = "127.0.0.1:50051"
server.bind(url, grpc.ServerCredentials.createInsecure())
server.start()
4. Setup the Client SideImport the necessary dependencies.sum_grpc_pb ‚áí Contains proto generated code that will help us setup the client and link it to the server functions we have definedsum_pb ‚áí Contains code to help us access and work with the request objectconst grpc = require('grpc');
const sumService = require('../server/protos/sum_grpc_pb')
const sums = require('../server/protos/sum_pb')
Setup a client using the proto generated codeconst sumClient = new sumService.SumServiceClient(
    'localhost:50051',
    grpc.credentials.createInsecure()
)
Configure and send the request objectconst sumRequest = new sums.SumRequest()
const sumObject = new sums.SumMessage()
sumObject.setFirstNum(3)
sumObject.setSecondNum(10)
sumRequest.setSumMessage(sumObject)
Handle the response received on the client sidesumClient.sum(sumRequest, (error, response) =>
{
    if (!error)
    {
        console.log(`Sum result : ${response.getResult()}`)
    } else
    {
        console.log(error)
    }
})
Add service to server and start it
//* Setup a server
const server = new grpc.Server()

server.addService(sumService.SumServiceService, { sum })

//? START THE SERVER
const URL_ENDPOINT = "127.0.0.1:50051"
server.bind(URL_ENDPOINT, grpc.ServerCredentials.createInsecure())
server.start()

console.log(chalk.green(`Server running on ${URL_ENDPOINT}`))
 Conclusion 

 Congratulations ! You are now equipped with the futuristic skills of bidirectional streaming type API designing. Thanks a lot for reading this blog. If you like more content like this, subscribe to my mailing list 
 Conclusion 

 Congratulations ! You are now equipped with the futuristic skills of API designing. Thanks a lot for reading this blog. If you like more content like this, subscribe to my mailing list